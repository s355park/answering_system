Jointly Distributed Random Variables 6.1 Joint Distribution Functions So far, we have only concerned ourselves with probability distributions for single random variables. In this chapter, we deal with probability statements concerning two or more random variables. In order to deal with such probabilities, we deﬁne, for any two random variables X and Y , the joint cumulative probability distribution function of X and Y by F ( a, b ) = P { X ≤ a, Y ≤ b } −∞ < a, b < ∞ The distribution of X can be obtained from the joint distribution of X and Y as follows: F X ( a ) = P { X ≤ a } = P { X ≤ a, Y < ∞} = F ( a, ∞ ) and similarly F Y ( b ) = F ( ∞ , b ) The distribution functions F X and F Y are sometimes referred to as the marginal distributions of X and Y . In the case that both X and Y are both discrete random variables, it is convenient to deﬁne the joint probability mass function of X and Y by p ( x, y ) = P { X = x, Y = y } The probability mass function of X can be obtained from p ( x, y ) by p X ( x ) = P { X = x } = X y p ( x, y ) and similarly p Y ( y ) = P { Y = y } = X x p ( x, y ) ©A. K. Khandani, E&CE307—Fall 2024 95 Jointly Continuous Random Variables We say that X and Y are jointly continuous if there exists a function f ( x, y ) deﬁned for all real x and y , having the property that for every set C of pairs of real numbers (that is, C is a set in the two dimensional plane) P { ( X, Y ) ∈ C } = ZZ ( x,y ) ∈ C f ( x, y ) dx dy The function f ( x, y ) is called the joint probability density function of X and Y . If A and B are any sets of real numbers, then by deﬁning C = { x, y ) : x ∈ A, y ∈ B } , we see that P { X ∈ A, Y ∈ B } = Z B Z A f ( x, y ) dx dy Because F ( a, b ) = P { X ∈ ( −∞ , a ] , Y ∈ ( −∞ , b ] } = Z b −∞ Z a −∞ f ( x, y ) dx dy after diﬀerentiating f ( a, b ) = ∂ 2 ∂a ∂b F ( a, b ) wherever the partial derivations are deﬁned. ©A. K. Khandani, E&CE307—Fall 2024 96 Another interpretation of the joint density function is as follows: P { a < X < a + da, b < Y < b + db } = Z b + db b Z a + da a f ( x, y ) dxdy ≈ f ( a, b ) dadb when da and db are small and f ( x, y ) is continuous at a, b . Hence f ( a, b ) is a measure of how likely it is that the random vector ( X, Y ) will be near ( a, b ) . Similar to discrete case we have f X ( x ) = Z ∞ −∞ f ( x, y ) dy and f Y ( y ) = Z ∞ −∞ f ( x, y ) dx We can also deﬁne joint probability distributions for n random variables in exactly the same manner as we did for n = 2 . The joint cumulative probability distribution function F ( a 1 , a 2 , . . . , a n ) of the n random variables X 1 , X 2 , . . . , X n is deﬁned by F ( a 1 , a 2 , . . . , a n ) = P { X 1 ≤ a 1 , X 2 ≤ a 2 , . . . , X n ≤ a n } Further, the n random variables are said to be jointly continuous if there exists a function f ( x 1 , x 2 , . . . , x n ) , called the joint probability density function, such that for any set C in n -space P { ( X 1 , X 2 , . . . , X n ) ∈ C } = ZZ · · · Z ( x 1 ,x 2 ,...,x n ) ∈ C f ( x 1 , . . . , x n ) dx 1 dx 2 dx n ©A. K. Khandani, E&CE307—Fall 2024 97 6.2 Independent Random Variables The random variables X and Y are said to be independent if for any two sets of real numbers A and B , P { X ∈ A, Y ∈ B } = P { X ∈ A } P { Y ∈ B } In terms of the joint distribution function F of X and Y , we have that X and Y are independent if F ( a, b ) = F X ( a ) F Y ( b ) for all a, b When X and Y are discrete random variables, the condition of independence is equivalent to p ( x, y ) = p X ( x ) p Y ( y ) for all x, y and for continuous case, f ( x, y ) = f X ( x ) f Y ( y ) for all x, y Proposition : The continuous (discrete) random variables X and Y are independent if and only if their joint probability density (mass) function can be expressed as f X,Y ( x, y ) = f X ( x ) f Y ( y ) −∞ < x < ∞ , −∞ < y < ∞ Remark : For set of random variables X 1 , . . . , X n we can show that theses random variables are independent by showing that X 2 is independent of X 1 X 3 is independent of X 1 , X 2 X 4 is independent of X 1 , X 2 , X 3 . . . . . . X n is independent of X 1 , . . . , X n − 1 ©A. K. Khandani, E&CE307—Fall 2024 98 6.3 Sums of Independent Random Variables Suppose that X and Y are independent, continuous random variables having probability distribution functions f x and f y . The cumulative distribution function of X + Y is obtained as follows: F X + Y ( a ) = P { X + Y ≤ a } = ZZ x + y ≤ a f X ( x ) f Y ( y ) dx dy = Z ∞ −∞ Z a − y −∞ f X ( x ) f Y ( y ) dx dy = Z ∞ −∞ F X ( a − y ) f Y ( y ) dy By diﬀerentiating, we obtain that the probability density function f X + Y of X + Y is given by f X + Y ( a ) = d da Z ∞ −∞ F X ( a − y ) f Y ( y ) dx dy = Z ∞ −∞ f X ( a − y ) f Y ( y ) dy ©A. K. Khandani, E&CE307—Fall 2024 99 Proposition, Sum of Gamma Random Variables : If X and Y are independent gamma random variables with respective parameters ( s, λ ) and ( t, λ ) , then X + Y is a gamma random variable with parameters ( s + t, λ ) . Proof: f X + Y ( a ) = 1 Γ( s )Γ( t ) Z a 0 λe − λ ( a − y ) [ λ ( a − y )] s − 1 λe − λy ( λy ) t − 1 dy = Ke − λa Z a o ( a − y ) s − 1 y t − 1 dy = Ke − λa a s + t − 1 Z 1 o (1 − x ) s − 1 x t − 1 dx by letting x =  y a = Ce − λa a s + t − 1 where C is a constant that does not depend on a . But as the above is a density function and thus must integrate to 1 , the value of C is determined, and we have f X + Y ( a ) =  λe − λa ( λa ) s + t − 1 Γ( s + t ) ©A. K. Khandani, E&CE307—Fall 2024 100 Sum of Square of Standard Normal Random Variables : If Z 1 , Z 2 , . . . , Z n are independent standard nor- mal random variables, then Y ≡ n X i =1 Z 2 i  is said to have the chi-squared (sometimes seen as χ 2 ) distribution with n degrees of freedom. When n = 1 , Y = Z 2 1 , we can see that its probability density function is given by f Z 2 ( y ) = 1 2 √ y [ f Z ( √ y ) + f Z ( −√ y )] = 1 2 √ y 2 √ 2 π e − y/ 2 = 1 2 e − y/ 2 ( y/ 2) 1 / 2 − 1 √ π Noting that Γ( 1 2 ) = √ π , this is the gamma distribution with parameters (1 2 , 1 2 ) . From the above proposition we obtain that the χ 2  distribution with n degree of freedom is just the gamma distribution with parameters ( n 2  , 1 2 ) and hence has the probability density function as f χ 2 ( y ) = 1 2 e − y 2 / 2 ( y 2 ) n/ 2 − 1 Γ( n 2 ) y > 0 = e − y 2 / 2 y n/ 2 − 1 2 n/ 2 Γ( n 2 ) y > 0 When n is an even integer, Γ( n 2 ) = [( n/ 2) − 1]! , and when n is odd, Γ( n 2 ) can be obtained from iterating the relationship Γ( t ) = ( t − 1)Γ( t − 1) and then using Γ( 1 2 ) = √ π . ©A. K. Khandani, E&CE307—Fall 2024 101 Proposition, Sum of Normal Random Variables : If X i , i = 1 , . . . , n are independent random variables that are normally distributed with respective parameters µ i , σ 2 i  , i = 1 , . . . , n , then n X i =1 X i is normally distributed with parameters n X i =1 µ i and n X i =1 σ 2 i  . 6.4 Conditional Distributions: Discrete Case For any two events E and F , the conditional probability of E given F is deﬁned, provided that P ( F ) > 0 , by P ( E | F ) =  P ( EF ) P ( F ) Hence, If X and Y are discrete random variables, it is natural to deﬁne the conditional probability mass function of X given by Y = y , by p X | Y ( x | y ) = P { X = x | Y = y } = P { X = x, Y = y } P { Y = y } = p ( x, y ) p Y ( y ) for all values of y such that p Y ( y ) > 0 . Similarly, the conditional probability distribution function of X given that Y = y is deﬁned, for all y such that p Y ( y ) > 0 , by F X | Y ( x | y ) = P { X ≤ x | Y = y } = X a ≤ x p X | Y ( a | y ) If X and Y are independent, then p X | Y ( x | y ) = P { X = x } ©A. K. Khandani, E&CE307—Fall 2024 102 6.5 Conditional Distributions: Continuous Case If X and Y have a joint density function f ( x, y ) , then the conditional probability density function of X , given that Y = y , is deﬁned for all values of y such that f Y ( y ) > 0 , by f X | Y ( x | y ) =  f ( x, y ) f Y ( y ) 6.6 Order Statistics Not covered in this course. 6.7 Joint Probability Distribution Functions of Random Variables Let X 1 and X 2 be jointly continuous random variables with joint probability density function f X 1 ,X 2 . It is sometimes necessary to obtain the joint distribution of the random variables Y 1 and Y 2 , which arise as functions of X 1 and X 2 . Speciﬁcally, suppose that Y 1 = g 1 ( X 1 , X 2 ) and Y 2 = g 2 ( X 1 , X 2 ) . Assume that functions g 1 and g 2 , satisfy the following conditions: 1. The equations y 1 = g 1 ( x 1 , x 2 ) and y 2 = g 2 ( x 1 , x 2 ) can be uniquely solved for x 1 and x 2 in terms of y 1 and y 2 with solutions given by, x 1 = h 1 ( y 1 , y 2 ) and x 2 = h 2 ( y 1 , y 2 ) . 2. The functions g 1 and g 2 have the continuous partial derivatives at all points ( x 1 , x 2 0 that are such that the following 2 × 2 determinant J ( x 1 , x 2 ) =  ∂g 1 ∂x 1 ∂g 1 ∂x 2 ∂g 2 ∂x 1 ∂g 2 ∂x 2  =  ∂g 1 ∂x 1 ∂g 2 ∂x 2 − ∂g 1 ∂x 2 ∂g 2 ∂x 1 ̸ = 0 at all points ( x 1 , x 2 ) . Under these two conditions it can be shown that the random variables Y 1 and Y 2 are jointly continuous with joint density function given by f Y 1 ,Y 2 ( y 1 , y 2 ) = f X 1 ,X 2 ( x 1 , x 2 ) | J ( x 1 , x 2 ) | − 1 where x 1 = h 1 ( y 1 , y 2 ) and x 2 = h 2 ( y 1 , y 2 ) . ©A. K. Khandani, E&CE307—Fall 2024 103 When the joint density function of n random variables X 1 , X 2 , . . . , X n is given and we want to compute the joint density function of Y 1 , Y 2 , . . . , Y n , where Y 1 = g 1 ( X 1 , . . . , X n ) , Y 2 = g 2 ( X 1 , . . . , X n ) , . . . , Y n = g n ( X 1 , . . . , X n ) the approach is the same. Namely, we assume that the functions g i have continuous partial derivatives and that the Jacobian determinant J ( x 1 , . . . , x n ) ̸ = 0 at all points ( x 1 , . . . , x n ) , where J ( x 1 , . . . , x n ) =  ∂g 1 ∂x 1 ∂g 1 ∂x 2 · · · ∂g 1 ∂x n ∂g 2 ∂x 1 ∂g 2 ∂x 2 · · · ∂g 2 ∂x n ... ... ... ∂g n ∂x 1 ∂g n ∂x 2 · · · ∂g n ∂x n  ©A. K. Khandani, E&CE307—Fall 2024 104 6.8 Some Solved Problems 1. Let R 1 and R 2 be independent, each with density f ( x ) = e − x , x ≥ 0; f ( x ) = 0 , x < 0 . Let R 3 =max( R 1 , R 2 ). Compute E( R 3 ). Solution: E ( R 3 ) = E [ g ( R 1 , R 2 )] = Z ∞ −∞ Z ∞ −∞ g ( x, y ) f 12 ( x, y ) dx dy = Z ∞ 0 Z ∞ 0 max ( x, y ) e − x e − y  dx dy Now max ( x, y ) = x if x ≥ y ; max ( x, y ) = y if x ≤ y . Thus E ( R 3 ) = Z Z x ≥ y xe − x e − y  dx dy + Z Z y ≥ x ye − x e − y  dx dy = Z ∞ x =0 xe − x Z x y =0 e − y  dy dx + Z ∞ y =0 ye − y Z y x =0 e − x  dx dy The two integrals are equal, since one may be obtained from the other by interchanging x and y . Thus E ( R 3 ) = 2 Z ∞ 0 xe − x Z x 0 e − y  dy dx = 2 Z ∞ 0 xe − x (1 − e − x ) dx = 2 Z ∞ 0 xe − x  dx − 2 Z ∞ 0 z 2 e − z dz 2 = 3 2 Γ(2) = 3 2 2. We arrive at a bus stop at time t = 0 . Two buses A and B are in operation. The arrival time R 1 of bus A is uniformly distributed between 0 and t A minutes, and the arrival time R 2 of bus B is uniformly distributed between 0 and t B minutes, with t A ≤ t B . The arrival times are independent. Find the probability that bus A will arrive ﬁrst. Solution: We are looking for the probability that R 1 < R 2 . Since R 1 and R 2 are independent, the conditional density of R 2 given R 1 is f ( x, y ) f 1 ( x ) = f 2 ( y ) = 1 t B , 0 ≤ y ≤ t B If bus A arrives at x, 0 ≤ x ≤ t A , it will be ﬁrst provided that bus B arrives between x and t B . This happens with probability ( t B − x ) /t B . Thus P { R 1 < R 2 | R 1 = x } = 1 − x t B , 0 ≤ x ≤ t A Hence, P { R 1 < R 2 } = Z ∞ −∞ P { R 1 < R 2 | R 1 = x } f 1 ( x ) dx = Z t A 0  1 − x t B  1 t A dx = 1 − t A 2 t B Alternatively, we may simply use the joint density: P { R 1 < R 2 } = Z Z x<y f ( x, y ) dx dy = the shaded area in ﬁgure below, divided by total area t A t B = 1 − t 2 A / 2 t A t B = 1 − t A 2 t B as before. ©A. K. Khandani, E&CE307—Fall 2024 105 mup t A mup t B mup x = y mup x mup y 3. Suppose that X and Y have joint density f ( x, y ) = (3 x 2  + 4 xy ) / 2 when 0 < x, y < 1 . Find the marginal density of X and the conditional density of Y given X = x . Solution: f X ( x ) = Z 1 0 (3 x 2  + 4 xy ) / 2 dy = 3 2 x 2 + x , for 0 < x < 1 and f Y ( y | X = x ) = 3 x 2 + 4 xy 2 / ( 3 2 x 2 + x ) = 3 x + 4 y 3 x + 2  , for 0 < y < 1 . 4. Suppose the joint pdf for the random variables X and Y is given by f X,Y ( x, y ) =  c, 0 ≤ x ≤ y ≤ 1 0 , otherwise. Find the following: (a) The constant c , (b) The marginal pdfs, f X ( x ) and f Y ( y ) , and (c) The probability that X + Y < 1 . Solution: (a) We know that the area under the pdf should be equal to one, and in this case, Z ∞ −∞ Z ∞ −∞ f X,Y ( x, y ) dx dy = Z 1 0 Z 1 x c dy dx = 1 Thus c/ 2 = 1 = ⇒ c = 2 (b) f X ( x ) = Z ∞ −∞ f X,Y ( x, y ) dy so, f X ( x ) = Z 1 y = x 2 dy =  2(1 − x ) , 0 ≤ x ≤ 1 0 , otherwise Similarly, ©A. K. Khandani, E&CE307—Fall 2024 106 f Y ( y ) = Z ∞ −∞ f X,Y ( x, y ) dx = Z y x =0 2 dx =  2 y, 0 ≤ y ≤ 1 0 , otherwise (c) By noticing the following ﬁgure, it is clear that: P ( X + Y < 1) = 1 2                                                                    mup1 mup0 mup y mup x + y = 1 mup1 mup x 5. Consider a joint pdf for random variables X and Y deﬁned as f X,Y ( x, y ) =  cxy, 0 ≤ x, y ≤ 1 0 , otherwise. Deﬁne the event A as Y > X . (i) Compute the contact c . (ii) Find the conditional pdf of X and Y , given A . (ii) Find the conditional pdf of Y given A . Solution: We ﬁrst must ﬁnd the value for c in f X,Y ( x, y ) . We have, Z 1 0 Z 1 0 cxy dx dy = 1 And so, c Z 1 0 y (  x 2 2  1 0 ) dy = c Z 1 0 y 2  dy = 1 Which results in c = 4 . To ﬁnd f X,Y | A ( x, y | A ) , we ﬁrst ﬁnd p ( A ) . P ( A ) = P ( Y > X ) = 4 Z 1 0 Z y 0 xy dx dy = 1 2 The conditional density, given A, is ©A. K. Khandani, E&CE307—Fall 2024 107 f X,Y | A ( x, y | A ) = ( f X,Y ( x,y ) 1 / 2 , ( x, y ) ∈ A ≡ 0 ≤ x < y ≤ 1 0 , otherwise. =  8 xy, 0 ≤ x < y ≤ 1 0 , otherwise. The marginal pdf of Y given A , is obtained by integrating f X,Y | A ( x, y | A ) with respect to x. f Y | A ( y | A ) = Z y 0 8 xy dx = 8 y x 2 2  y 0 =  4 y 3 , 0 ≤ y ≤ 1 0 , otherwise. 6. Let X and Y be independent and uniformly distributed over the interval (0 , a ) . The density functions are given by f X ( x ) = 1 /a (0 ≤ x ≤ a ) f Y ( y ) = 1 /a (0 ≤ y ≤ a ) and zero otherwise. Find the density function of Z = X + Y . Solution: We know that the PDF of Z is given by f Z ( z ) = Z ∞ −∞ f X ( x ) f Y ( z − x ) dx Hence, the integrand is 1 /a 2  if 0 ≤ x ≤ a, 0 ≤ z − x ≤ a and zero otherwise. Two cases depending on the value of z : (a) For 0 ≤ z ≤ a , we ﬁnd, f Z ( z ) = Z z 0 (1 /a ) 2  dx = z/a 2 (b) For a ≤ z ≤ 2 a , we ﬁnd, f Z ( z ) = Z a z − a (1 /a ) 2  dx = (2 a − z ) /a 2 7. At a crossing there is a traﬃc light showing alternately green and red light for a seconds. A car driver who arrives at random has to wait for a time period Z . Find the distribution of Z . Solution: If the driver arrives during a green period, his waiting time is zero. Since the green and the red lights have the same durations, we have P ( Z = 0) = 1 / 2 . If the driver arrives during the ﬁrst a − z seconds of a red period, his waiting time is greater than z . Hence for 0 < z < a we have P ( Z > z ) =  a − z 2 a and P ( Z ≤ z ) = 1 − a − z 2 a = 1 2 + z 2 a Thus, the cdf function is: F Z ( z ) =        0 if z < 0 1 / 2 if z = 0 1 2  + z 2 a if 0 < z < a 1 if z ≥ a ©A. K. Khandani, E&CE307—Fall 2024 108 8. Let ( X, Y, Z ) be a random point uniformly selected in the unit sphere. That is, their joint density is 3 / 4 π when x 2  + y 2  + z 2  ≤ 1 , and 0 otherwise. Find the marginal densities of (i) ( X, Y ) and (ii) Z . Solution: To ﬁnd the f XY ( x, y ) , we integrate f XY Z with respect to z in the range [ − p 1 − x 2  − y 2 , p 1 − x 2  − y 2 ] . This results in, f X,Y ( x, y ) = 3 4 π  · 2 q 1 − x 2  − y 2 , for x 2  + y 2  ≤ 1 . To compute f Z ( z ) , from the previous result we know that, f X,Z ( x, z ) = 3 4 π  · 2 p 1 − x 2  − z 2 , for x 2  + z 2  ≤ 1 . We integrate f X,Z ( x, z ) with respect to x in the range, [ − √ 1 − z 2 , √ 1 − z 2 ] . One way to do this is to apply the change of variable x √ 1 − z 2 = cos ϕ This results in, f Z ( z ) = 3 4  · (1 − z 2 ) . 9. Suppose X 1 , . . . , X n are independent and all have the cdf F X ( x ) . Find the cdf of Y = max { X 1 , . . . , X N } , and Z = min { X 1 , . . . , X N } . Solution: (a) F Y ( y ) = P ( X 1 ≤ y, · · · , X n ≤ y ) = F X ( y ) n . (b) P ( Z > z ) = P ( X 1 > z, · · · , X n > z ) = (1 − F X ( z )) n , so F Z ( z ) = 1 − (1 − F X ( z )) n . 10. Suppose X 1 , . . . , X 5 are independent and all have the same distribution which is continuous. Show that P ( X 3 < X 5 < X 1 < X 4 < X 2 ) = 1 / 5! . Solution: Let X 1 , . . . , X 5 be independent with distribution f . We use the fact that f is continuous to conclude that with probability one all the X ’s are distinct. This means that for a given set of X 1 , . . . , X 5 values, we can have 5! diﬀerent permutation to rearrange the X values. Noting the independence of the X ’s, we conclude that these 5! diﬀerent permutations all have the same probability. As these 5! diﬀerent permutations are disjoint and cover all the possibilities for the ordering of X ’s, we conclude that each happen with probability 1 / 5! . 11. Suppose X and Y are independent, X is uniform on (0 , 1) and Y has the cdf F ( y ) . Show that Z = X + Y has the density function F ( z ) − F ( z − 1) . Solution: Using the result of problem 4 with f X ( x ) = 1 , we obtain, f X + Y ( z ) = Z 1 0 f Y ( z − x ) dx = F ( z ) − F ( z − 1) 12. Fill in the rest of the joint distribution given in the following table knowing that (i) P ( Y = 2 | X = 0) = 1 / 4 , and (ii) X and Y are independent. Y X = 0 3 6 1 ? ? ? 2 . 1 . 05 ? (i) P ( Y = 2 | X = 0) = 1 / 4 and P ( X = 0 , Y = 2) = 0 . 1 implies P ( X = 0) = 0 . 4 and P ( X = 0 , Y = 1) = 0 . 3 . Using the fact that X and Y are independent, we conclude now from P ( X = 0 , Y = 2) = P ( X = 0) × P ( Y = ©A. K. Khandani, E&CE307—Fall 2024 109 2) = 0 . 1 that P ( Y = 2) = 0 . 25 , and P ( X = 3 , Y = 2) = P ( X = 3) × P ( Y = 2) = 0 . 05 gives that P ( X = 3) = 0 . 2 and hence, P ( X = 6) = 1 − 0 . 4 − 0 . 2 . Thus the entire distribution is given by, Y X = 0 3 6 1 . 3 . 15 . 3 2 . 1 . 05 . 1 13. Suppose X and Y have joint density f ( x, y ) . Are X and Y independent if ( a ) f ( x, y ) = xe − x (1+ y )  for x, y ≥ 0 ( b ) f ( x, y ) = 6 xy 2  when x, y ≥ 0 and x + y ≤ 1 ( c ) f ( x, y ) = 2 xy + x when 0 < x < 1 and 0 < y < 1 ( d ) f ( x, y ) = ( x + y ) 2  − ( x − y ) 2  when 0 < x < 1 and 0 < y < 1 In each case, f ( x, y ) = 0 otherwise. Solution: (a) No. Since f X ( x ) = R ∞ 0  xe − x (1+ y ) dy = e − x for x > 0 . f Y ( y ) = Z ∞ 0 xe − x (1+ y )  dx = − xe − x (1+ y ) 1 + y  ∞ 0 + Z ∞ 0 e − x (1+ y ) 1 + y dx = 1 (1 + y ) 2 for y > 0 , and f ( x, y ) ̸ = f X ( x ) f Y ( y ) . We could simply reach this conclusion by noticing that f X,Y ( x, y ) cannot be factored as the product of two functions, one only function of x and the other one only function of y . (b) No. Since { ( x, y ) : f ( x, y ) > 0 } is not a rectangle. Note that f X,Y ( x, y ) can be factored as the product of two functions, one only function of x and the other one only function of y . However, this is not suﬃcient condition for independence. Theorem: If f ( x, y ) can be written as g ( x ) h ( y ) then there is a constant c so that f X ( x ) = cg ( x ) and f Y ( y ) = h ( y ) /c . It follows that f ( x, y ) = f X ( x ) f Y ( y ) and hence X and Y are independent. Note that this requires the range of ( x, y ) to be a rectangle with one side corresponding to the range of x and the other side corresponding to the range of y . (c) Yes by the above theorem since f ( x, y ) = x (2 y + 1) . (d) Yes by the above theorem since f ( x, y ) = 2 x · 2 y . 14. Suppose X 1 , . . . , X n are independent and have distribution function F ( x ) . Find the joint density function of Y = max { X 1 , . . . , X n } and Z = min { X 1 , . . . , X n } . Find the joint density of Y and Z . Solution: We have, P ( Y ≤ y, Z ≥ z ) = P ( z ≤ X i ≤ y for all i ≤ n ) = ( Z y z f ( x ) dx ) n  = [ F ( y ) − F ( z )] n . and, P ( Y ≤ y, Z ≥ z ) + P ( Y ≤ y, Z ≤ z ) = P ( Y ≤ y ) This is used to compute the joint distribution function F ( y, z ) as follows: F ( y, z ) = P ( Y ≤ y, Z ≤ z ) = P ( Y ≤ y ) − P ( Y ≤ y, Z ≥ z ) = F ( y ) n  − [ F ( y ) − F ( z )] n The corresponding joint density function is equal to, f ( y, z ) =  ∂ 2 F ∂y∂z  = n ( n − 1) f ( y ) f ( z ) Z y z f ( x ) dx  n − 2 ©A. K. Khandani, E&CE307—Fall 2024 110 15. Suppose X 1 , . . . , X n are independent and uniform on (0,1). Let X ( k )  be the k th smallest of the X j . Show that the density of X ( k )  is given by n  n − 1 k − 1 ! x k − 1 (1 − x ) n − k Solution: This formula is easy to understand: There are n values of index j that we can pick such that the corresponding X j is the k th smallest of the X ’s. The rest of the formula then gives the probability that exactly k − 1 of the remaining n − 1 variables are smaller than x . Suppose X 1 , . . . , X n are independent and have density function f . Let X (1)  be the smallest of the X j , X (2) be the second smallest, and so on until X ( n )  is the largest. It can be shown that their joint density is given by f ( x 1 , . . . , x n ) =  n ! f ( x 1 ) · · · f ( x n ) if x 1  < x 2  · · · < x n ( ∗ ) 0 otherwise Hence, the density function of X ( k )  is obtained by integrating the above joint density with respect to the other variables: n ! Z x k 0 Z x k x 1 · · · Z x k x k − 2 Z 1 x k · · · Z 1 x n − 1 dx n · · · dx k +1 dx k − 1 · · · dx 2 dx 1 = n ! 1 ( k − 1)! x k − 1 k 1 ( n − k )! (1 − x k ) n − k = n  n − 1 k − 1 ! x k − 1 k (1 − x k ) n − k To generalize the above result we show that if X 1 , . . . , X n are independent and have density f then the density of X ( k )  is given by nf ( x )  n − 1 k − 1 ! F ( x ) k − 1 (1 − F ( x )) n − k The density function of X ( k )  is obtained by integrating the (*) equation: n ! · Z x k −∞ · · · Z x k x k − 2 Z ∞ x k · · · Z ∞ x n − 1 Π n i =1 f ( x i ) dx n  · · · dx k +1  dx k − 1  · · · dx 1 To evaluate this we prove by induction that Z b a · · · Z b x k + i − 1 Π i j =1 f ( x k + j ) dx k + i  · · · dx k +1  = 1 i ! ( F ( b ) − F ( a )) i ( ∗∗ ) This is clear if i = 1 . If the formula is true for i then the integral for i + 1 is R b a  f ( x k +1 ) 1 i ! ( F ( b ) − F ( x k +1 )) i dx k +1 = − ( F ( b ) − F ( x k +1 )) i +1 ( i + 1)!  b a = 1 ( i +1)! ( F ( b ) − F ( a )) i +1 and we have proved (**). Using (**) with a = −∞ , b = x k , i = k − 1 and then a = x k , b = ∞ , i = n − k we have g ( x k ) = n ! f ( x k ) 1 ( k − 1)! F ( x k ) k − 1 · 1 ( n − k )! (1 − F ( x k )) n − k = nf ( x k )  n − 1 k − 1 ! F ( x k ) k − 1 (1 − F ( x k )) n − k ©A. K. Khandani, E&CE307—Fall 2024 111 16. Suppose we take a die with 3 on three sides, 2 on two sides, and 1 on one side, roll it n times, and let X i be the number of times side i appeared. Find the conditional distribution P ( X 2 = k | X 3 = m ) . Solution: P ( X 3 = m ) =  n m !  3 6  m  1 − 3 6  n − m P ( X 2 = k, X 3 = m ) = n ! ( n − k − m )! k ! m ! (3 / 6) m (2 / 6) k (1 / 6) n − m − k P ( X 2 = k | X 3 = m ) =  n − m k ! (2 / 3) k (1 / 3) n − m − k that is, binomial ( n − m, 2 / 3) . 17. Suppose X 1 , . . . , X m are independent and have a geometric distribution with parameter p . Find P ( X 1 = k | X 1 + · · · X m = n ) . Solution: One can show that the sum of i.i.d random variables with geometric distribution has a Pascal distribution. Using this fact we have, P ( X 1 + · · · + X m = n ) =  n − 1 m − 1 ! p m (1 − p ) n − m , so: P ( X 1 = k | X 1 + · · · + X m = n ) = P ( X 1 = k ) P ( X 2 + · · · + X m = n − k ) P ( X 1 + · · · + X m = n ) =  n − 1 − k m − 2 ! /  n − 1 m − 1 ! This is quite intuitive since among  n − 1 m − 1 ! choices for the ﬁrst m − 1 successes there are  n − 1 − k m − 2 ! with the ﬁrst success occurring at the k th trial. 18. Suppose X and Y have joint density f ( x, y ) = (1 / 2) e − y  when y ≥ 0 and − y ≤ x ≤ y . Compute P ( X ≤ 1 | Y = 3) . Solution: f Y ( y ) = Z y − y 1 2 e − y dx = ye − y for y ≥ 0 f X ( x | Y = y ) = (1 / 2) e − y /ye − y  = 1 / 2 y for − y ≤ x ≤ y P ( X ≤ 1 | Y = 3) = Z 1 − 3 1 / 6 dx = 2 / 3 19. Jobs 1 and 2 must be completed before job 3 is begun. If the amount of time each task takes is independent and uniform on (2,4), ﬁnd the density function for the amount of time T it takes to complete all three jobs. Solution: Let T i be the time required for job i , S = max { T 1 , T 2 } , and T = S + T 3 be the total time. P ( S ≤ s ) = ( s − 2) 2 / 4 when 2 ≤ s ≤ 4 so S has density function ( s − 2) / 2 , when 2 ≤ s ≤ 4 . Since S and T 3 are independent, the pdf of T is the convolution of pdf’s for S and T 3 . So, we get: f T ( t ) = ( ( u − 4) 2 / 8 if 4 < u < 6 1 2 h 1 − ( u − 6) 2 4 i if 6 < u < 8 ©A. K. Khandani, E&CE307—Fall 2024 112 20. Suppose X has density function x/ 2 for 0 < x < 2 and 0 otherwise. Find the density function of Y = X (2 − X ) . Solution: P ( Y ≥ y ) = P ( X 2  − 2 X + 1 ≤ 1 − y ) = P ( | X − 1 | ≤ p 1 − y ) = F (1 + p 1 − y ) − F (1 − p 1 − y ) Diﬀerentiating we see that the density function of Y is (for 0 ≤ y ≤ 1 ) f (1 − √ 1 − y ) + f (1 +  √ 1 − y ) 2 √ 1 − y = 1 / 2 p 1 − y 21. Suppose X 1 and X 2 have joint density f ( x 1 , x 2 ) =  1 for 0 < x 1 , x 2 < 1 0 otherwise Find the density of Y 1 = X 1 /X 2 and Y 2 = X 1 X 2 . Solution: The map r ( x 1 , x 2 ) = ( x 1 /x 2 , x 1 x 2 ) has the inverse s ( y 1 , y 2 ) = h ( y 1 y 2 ) 1 / 2 , ( y 2 /y 1 ) 1 / 2 i The elements of the Jacobian are equal to: D 11 =  1 2 y − 1 / 2 1 y 1 / 2 2 , D 12 =  1 2 y 1 / 2 1 y − 1 / 2 2 , D 21 = − 1 2 y − 3 / 2 1 y 1 / 2 2 , D 22 = 1 2 y − 1 / 2 1 y − 1 / 2 2 and the Jacobian is D =  1 2 y − 1 1  . Thus f Y 1 ,Y 2 ( y 1 , y 2 ) = 1 / 2 y 1 for 0 < y 1 y 2 < 1 , 0 < y 2 < y 1 22. Let f XY ( x, y ) = 1 /π if 0 ≤ x 2  + y 2  ≤ 1 = 0 elsewhere Find the pdf of r 2  = x 2  + y 2  and θ = tan − 1 y/x . Solve the same problem for the following joint pdf: f XY ( x, y ) = Ae − ( x 2 + y 2 ) if 0 ≤ x 2  + y 2  ≤∞ = 0 elsewhere We solve the second part of the problem which is more general. Consider, f X,Y ( x, y ) = f X ( x ) f Y ( y ) = 1 2 πσ 2 exp  − x 2 + y 2 2 σ 2 ! (4) where σ 2  = 1 / 2 and A = 1 /π . For the polar coordinates we have, R = √ X 2  + Y  2 Θ = arctan( Y/X ) (5) This results in, J ( x, y ) = " x √ x 2 + y 2 y √ x 2 + y 2 − y x 2 + y 2 x x 2 + y 2 # (6) ©A. K. Khandani, E&CE307—Fall 2024 113 We have, | det [ J ( x, y )] | = 1 p x 2  + y 2 = 1 r (7) The set of equations,  r = p x 2  + y 2 θ = arctan( y/x ) (8) has only one solution given by,  x = r cos θ y = r sin θ (9) Substituting these results, we obtain, f R, Θ ( r, θ ) = rf X,Y ( r cos θ, r sin θ ) = r 2 πσ 2 exp  − r 2 2 σ 2 ! (10) The marginal pdf’s for R , Θ are equal to, f Θ ( θ ) = Z ∞ 0 f R, Θ ( r, θ ) dr = 1 2 π Z ∞ 0 r σ 2 exp  − r 2 2 σ 2 ! dr = 1 2 π " exp  − r 2 2 σ 2 !# ∞ 0 = 1 2 π = ⇒ Θ has a uniform distribution in [0 , 2 π ] (11) and f R ( r ) = Z 2 π 0 f R, Θ ( r, θ ) dθ =  r σ 2 exp  − r 2 2 σ 2 ! , r ≥ 0 (12) Then, f R ( r ) =      r σ 2 exp  − r 2 2 σ 2 ! , r ≥ 0 0 r < 0 (13) This is known as the Raleigh probability density function . We also note that f R, Θ ( r, θ ) = f R ( r ) f Θ ( θ ) . (14) This means that R and Θ are independent of each other. 23. Let f XY ( x, y ) = e − ( x + y ) , x, y ≥ 0 = 0 elsewhere Find the pdf of Z = X + Y . Solution: Noticing that X and Y are independent, the pdf of Z is the convolution of pdf’s for X and Y . This results: f Z ( z ) = ze − z , z ≥ 0 = 0 elsewhere ©A. K. Khandani, E&CE307—Fall 2024 114 24. The discrete random variables X and Y are independent and each has a geometric probability distribution P ( X = k ) = q k − 1 p , P ( Y = k ) = q k − 1 p , k = 1 , 2 , 3 , · · · Find the conditional probability distribution P ( X = k | X + Y = n ) . Solution: P ( X + k | X + Y = n ) =  P ( X = k, Y = n − k ) P ( X + Y = n ) and P ( X + Y = n ) =  P n l =0  P ( X = l ) P ( Y = n − l ) = P n l =0  q l − 1 pq n − l − 1 p = p 2 P n l =0  q n − 2 = p 2 q n − 2 ( n + 1) and P ( X = k | X + Y = n ) =  q k − 1 pq n − k − 1 p p 2 q n − 2 ( n + 1) = 1 n + 1 , k = 0 , 1 , 2 , · · · , n. 25. Suppose a point ( X, Y ) is chosen at random in the unit circle x 2 + y 2  ≤ 1 . Find the marginal density function of X . Solution: The bivariate density function is uniform over the unit circle and hence has height 1 /π . The marginal density function for X is then f X ( x ) = Z  √ 1 − x 2 − √ 1 − x 2 1 π dy = 2 π p 1 − x 2  , − 1 < x < 1 . 26. Suppose X and Y have the joint density function f ( x, y ) . Are X and Y independent if: (i) f ( x, y ) = xe − x (1+ y )  , x, y > 0 . (ii) f ( x, y ) = 6 xy 2  when x, y ≥ 0 and x + y < 1 . (iii) f ( x, y ) = ( x + y ) 2  − ( x − y ) 2  when 0 ≤ x, y ≤ 1 . Solution: (i) f X ( x ) = Z ∞ 0 xe − x e − xy dy = e − x  , x ≥ 0 and f Y ( y ) = Z ∞ 0 xe − x (1+ y ) dx = 1 (1 + y ) 2 , y ≥ 0 and since f ( x, y ) ̸ = f X ( x ) f Y ( y ) X and Y are not independent. (ii) f X ( x ) = Z 1 − x 0 6 xy 2 dy = 2 x (1 − x ) 3  , 0 ≤ x ≤ 1 and f Y ( y ) = Z 1 − y 0 6 xy 2 dx = 3 y 2 (1 − y ) 2  , 0 ≤ y ≤ 1 and clearly X and Y are not independent. (iii) Notice that f ( x, y ) = 4 xy and so f X ( x ) = 2 x , 0 ≤ x ≤ 1 and f Y ( y ) = 2 y , 0 ≤ y ≤ 1 and X and Y are independent. ©A. K. Khandani, E&CE307—Fall 2024 115 27. The bivariate random variable ( X, Y ) has the joint pdf: f ( x, y ) =  4 x (1 − y ) 0 ≤ x, y ≤ 1 0 elsewhere (i) ﬁnd the marginal density functions of X and Y and determine whether or not they are independent. (ii) Determine the probability that Y > X 2 . Solution: (i) By straight forward computation the marginals are determined as f X ( x ) = Z 1 0 4 x (1 − y ) dy = 2 x , 0 ≤ x ≤ 1 f Y ( y ) = Z 1 0 4 x (1 − y ) dx = 2(1 − y ) , 0 ≤ y ≤ 1 and X and Y are independent. (ii) P ( Y > X 2 ) = Z 1 0 Z 1 x 2 4 x (1 − y ) dy  dx = 1 / 3 . 28. ( X, Y ) is a bivariate random variable which is uniformly distributed over the triangle x, y ≥ 0 , x + y ≤ 1 . (i) Find the marginal pdf, mean and variance of X . (ii) Find the correlation coeﬃcient ρ XY . Solution: (i) Notice that the problem is symmetric in X and Y . The marginal of X is f X ( x ) = 2(1 − x ) , 0 ≤ x ≤ 1 . The mean and variance of X are, respectively: E ( X ) = Z 1 0 x · 2(1 − x ) dx = 1 / 3 , E ( X 2 ) = Z 1 0 x 2  · 2(1 − x ) dx = 1 / 6 and V ( X ) = 1 / 18 . (ii) To compute the correlation coeﬃcient of X and Y we need: E ( XY ) = 2 Z 1 0 x Z 1 − x 0 ydy  dx = 1 / 12 and thus ρ XY = 1 12  − 1 3  · 1 3 r 1 18  · 1 18 = − 1 2  . 29. Suppose that X and Y have the joint probability density function f XY =  (2 m + 6) x m y 0 < y < x < 1 0 otherwise . Show that this is a probability density function and ﬁnd E ( Y | X ) and V ( Y | X ) . Solution: The marginal density of X is f X ( x ) = Z x 0 (2 m + 6) x m ydy = ( m + 3) x m +2  , 0 ≤ x ≤ 1 and notice that Z 1 0 ( m + 3) x m +2 dx = 1 ©A. K. Khandani, E&CE307—Fall 2024 116 and hence is a pdf. The conditional pdf is f Y | X ( y | x ) = 2( m + 3) x m y ( m + 3) x m +2 = 2 y x 2 , 0 ≤ y ≤ x and E ( Y | X ) = Z x 0 y ·  2 y x 2 dy = 2 3 x , E ( Y  2 | X ) = Z x 0 y 2  ·  2 y x 2 dy = 1 2 x 2 and V ( Y | X ) = E ( Y  2 | X ) − E 2 ( Y | X ) = x 2 / 18 . 30. If X is a binomial random variable with parameters n and p and Y is a binomial random variable with parameters m and p , X and Y independent, and Z = X + Y , ﬁnd E ( X | Z ) . Solution: P ( X = j | Z = X + Y = l ) =  P ( X = j and Y = l − j ) P ( X + Y = l ) =  P ( X = j ) P ( Y = l − j ) P ( X + Y = l ) = ( n j )( m l − j ) ( n + m l  ) which is a hypergeometric distribution. The mean of this distribution is in the text and is E ( X | Z ) = nl/ ( n + m ) . 31. If ( X, Y ) is a bivariate normal random variable with pdf f XY ( x, y ) = 1 π √ 3 exp {− 2 3 ( x 2 − xy + y 2 ) } (i) Find the probability that P ( Y > 3 | X = 2) and compare this to P ( Y > 3) . (ii) Find E ( Y | X ) . Solution: (i) As with the previous problem, the ﬁve parameters of the bivariate density are found as µ X = µ Y = 0 σ X = σ Y = 1 ρ XY = 1 / 2 . Consequently the conditional density function f Y | X ( y | x ) ∼ N (  1 2 x, 3 4 ) . Then P ( Y > 3 | X = 2) = Z ∞ 3 e − ( y − 1) 2 / (3 / 2) √ 2 π p 3 / 4  dy = 1 − Φ( 4 √ 3 ) . Since Y ∼ N (0 , 1) , P ( Y > 3) = Z ∞ 3 e − y 2 / 2 √ 2 π dy = 1 − Φ(3) . (ii) E ( Y | X ) = X/ 2 . 32. The time in minutes taken by person A to complete a certain task is assumed to be a random variable with an exponential pdf with parameter α 1 . The time taken by person B is independent of that for person A and has an exponential pdf with parameter α 2 i.e. f X ( x ) = α 1 e − α 1 x  , x ≥ 0 , f Y ( y ) = α 2 e − α 2 y  , y ≥ 0 (i) What is the probability it takes A longer to complete the task than B? (ii) If α 1 = α 2 what is the probability the tasks are ﬁnished within two minutes of each other? ©A. K. Khandani, E&CE307—Fall 2024 117 Solution: (i) Let X be the time taken by person A and Y that taken by person B. P ( X > Y ) = Z ∞ 0 Z x 0 α 2 e − α 2 y dy  α 1 e − α 1 x dx which can be computed to be P ( X > Y ) = α 2 α 1 + α 2 . (ii) It is desired to compute P ( | X − Y | < 2) . Let α 1 = α 2 = α . Consider P ( Y < X − 2) = Z ∞ 2 Z x − 2 0 αe − α dy  αe − αx dx which is readily evaluated to e − 2 α / 2 . The required probability is then P ( | X − Y | < 2) = 1 − 2( e − 2 α / 2) = 1 − e − 2 α . 33. Let X and Y be independent random variables, each with an exponential density function with parameter α (i.e. f X ( x ) = αe − αx  , x ≥ 0 ). Find the density function of Z = X/ ( X + Y ) . Solution: F Z ( z ) = P ( X X + Y  ≤ z ) = P ( X ≤ z ( X + Y )) = P ( X ≤ z 1 − z Y ) = Z ∞ 0 (Z ∞ (1 − z ) x/z αe − αy dy ) αe − αx dx = Z ∞ 0 e − α (1 − z ) x/z αe − αx dx = Z ∞ 0 αe − αx/z dx = z and hence f Z ( z ) = 1 , 0 ≤ z ≤ 1 . 34. Let X, Y and Z be independent and identically distributed random variables, each with an exponential pdf with parameter α . Find the probability density function of W = X + Y + Z . Solution: Let U + X + Y then f U ( u ) = Z u 0 f X ( u − y ) f Y ( y ) dy = Z z 0 αe − α ( z − y ) αe − α ( y ) dy = α 2 ze − αz  , 0 ≤ z < ∞ The process is repeated to ﬁnd the density of W : f W ( w ) = Z w 0 αe − α ( w − v ) α 2 ve − α ( v ) dv =  α 3 2  w 2 e − αw , w ≥ 0 35. Let X and Y be independent random variables with density functions: f X ( x ) = αe − αx  , x ≥ 0 f Y ( y ) = α n y n − 1 ( n − 1)! e − αy , y ≥ 0 for n a positive integer. (i) Find the pdf of U = X + Y . (ii) Find the pdf of W = X/ ( X + Y ) . Solution: We use the same technique as the previous two problems: f U ( u ) = Z u 0 αe − α ( u − y ) α n ( n − 1)! y n − 1 e − α ( y ) dy = α n +1 n !  u n e − αu . Notice that this implies that if Z = X 1 + · · · X n , where the X i are i.i.d., each with a density α exp − αx , then the pdf of Z is: f Z ( z ) = α n z n − 1 ( n − 1)! e − αz z ≥ 0 . ©A. K. Khandani, E&CE307—Fall 2024 118 36. Let X and Y be independent random variables with the pdf’s: f X ( x ) = e − x  , x ≥ 0 , f Y ( y ) = e − y  , y ≥ 0 . Find the pdf of the random variable Z = ( X − Y ) / ( X + Y ) and specify clearly its region of deﬁnition. Solution: Notice that Z takes values in the range [ − 1 , 1] . F Z ( z ) = P (  X − Y X + Y  ≤ z ) = P (( X − Y ) ≤ z ( X + Y )) = P (( 1 − z 1+ z ) X ≤ Y ) = R ∞ 0 R ∞ 1 − z 1+ z  x e − y  e − x dx = R ∞ 0  e − 1 − z 1+ z  x  · e − x dx =  1+ z 2 , − 1 ≤ z ≤ 1 . Hence f Z ( z ) = 1 / 2 , − 1 ≤ z ≤ 1 and zero elsewhere. 37. The random variables X and Y have the joint probability density function f XY ( x, y ) = xe − x (1+ y )  , 0 ≤ x, y < ∞ . Find the pdf of Z = XY . Solution: F Z ( z ) = P ( Z ≤ z ) = P ( XY ≤ z ) = R ∞ 0 nR z/x 0 xe − x (1+ y ) dy o dx R ∞ 0  xe − x n − 1 x e − xy | z/x 0 o dx = 1 − e − z and consequently f Z ( z ) = dF Z ( z ) /dz = e − z  , z ≥ 0 . 38. Let Y be a random variable with pdf f X ( x ) = e − x  , x ≥ 0 . Find the pdf of the random variable Y = 1 − e − X  , X ≥ 0 . Show that, in general, if X has the pdf f X ( x ) and cdf F X ( x ) then Y = F X ( X ) is uniformly distributed on (0 , 1) . Solution: Notice that Y takes values in the range [0 , 1] and − log(1 − y ) > 0 . F Y ( y ) = P ( Y ≤ y ) = P (1 − e − X  ≤ y ) = P ( X ≤− log(1 − y )) = R − log(1 − y ) 0 e − x dx = 1 − e log(1 − y )  = y and consequently f Y ( y ) = dF Y ( y ) /dy = 1 , 0 ≤ y ≤ 1 . In the general case, note that we are using the cdf as a montonic function, and that again Y takes values in [0 , 1] : F Y ( y ) = P ( Y ≤ y ) = P ( F X ( X ) ≤ y ) = P ( X ≤ F  − 1 X  ( y )) = F X ( F  − 1 X  ( y )) = y , 0 ≤ y ≤ 1 where F  − 1 X  ( · ) is the inverse function of F X ( · ) . The result that Y is a uniformly distributed random variable on [0 , 1] follows. 39. Let X, Y be independent normal random variables, each with zero mean and variance σ 2 . Show that Z = X + Y has a normal density N (0 , 2 σ 2 ) . ©A. K. Khandani, E&CE307—Fall 2024 119 Solution: As in previous problems: F Z ( z ) = Z ∞ −∞ 1 √ 2 πσ e − ( z − x ) 2 / (2 σ 2 ) · 1 √ 2 πσ e − x 2 / (2 σ 2 ) dx = Z ∞ −∞ 1 2 πσ 2 e − 1 2 σ 2 (2 x 2 − 2 zx + z 2 ) dx = 1 2 πσ 2 Z ∞ −∞ e − 2 2 σ 2 { ( x − z/ 2) 2 + z 2 / (4 σ 2 ) dx = 1 √ 2 π 2 σ e − z 2 / 4 σ 2 ∼ N (0 , 2 σ 2 ) . ©A. K. Khandani, E&CE307—Fall 2024 120