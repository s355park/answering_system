Random Variables 4.1 Random Variables It is frequently the case when an experiment is performed that we are mainly interested in some function of the outcome as opposed to the actual outcome itself. For instance, in tossing dice we are often interested in the sum of the two dice and are not really concerned about the separate values of each die. These quantities of interest, or more formally, these real-valued functions deﬁned on the sample space, are known as random variables. Random Variable : A random variable is a number which selects value in an unpredictable manner. In other words, a random variable is the result of a random experiment whose outcome is a number. A more formal deﬁnition of a random variable is as follows. Let S be the sample space associated with some experiment E . A random variable X is a function that assigns a real number X ( s ) to each elementary event s ∈ S . In this case, as the outcome of the experiment, i.e., the speciﬁc s , is not predetermined, then the value of X ( s ) is not ﬁxed. This means that the value of the random variable is determined by the speciﬁc outcome of the experiment. The domain of a random variable X is the sample space S and the range space R X is a subset of the real line, i.e., R X ⊆ℜ . There are two types of random variables: discrete and continuous. It is conventional to use capital letters such as X, Y, S, T, . . . , to denote a random variable and the corresponding lower case letter, x, y, s, t, . . . , to denote particular values taken by the random variable. ©A. K. Khandani, E&CE307—Fall 2024 50 4.2 Discrete Random Variables A random variable that can take on at most a countable number of possible values is said to be discrete. Probability mass function : For a discrete random variable X , we deﬁne the probability mass function p ( a ) of X by p ( a ) = P { X = a } Since X must take on one of the values x i , we have X i p ( x i ) = X i p { x = x i } = 1 Cumulative Distribution Function (CDF) : For the random variable X , we deﬁne the function F ( x ) by the equation, F ( x ) = P ( X ≤ x ) . And for any a F ( a ) = X all x ≤ a p ( x ) 4.3 Expected Value The expected value of the discrete random variable X , which we denote by E ( X ) , is given by E ( X ) = X x xp ( x ) ©A. K. Khandani, E&CE307—Fall 2024 51 4.4 Expectation of a Function of a Random Variable A function of a discrete random variable X , say Y = g ( X ) , is also a discrete random variable. We have, E ( Y ) = E [ g ( X )] = X i g ( x i ) p ( x i ) If a and b are constants, then E [ aX + b ] = aE [ X ] + b 4.5 Variance The variance of the random variable X , which we denote by Var( X ) , is given by, Var( X ) = E { [ X − µ ] 2 } = E [ X 2 ] − µ 2 where µ = E [ X ] For any two constants a and b Var( aX + b ) = a 2 Var( X ) Standard deviation is deﬁned as the square root of the variance. That is, SD( X ) = q Var( X ) ©A. K. Khandani, E&CE307—Fall 2024 52 4.6 The Bernoulli and Binomial Random Variables Bernoulli random variable : The Bernoulli random variable corresponds to an event E which have two possible outcomes, success ( X = 1 ) and failure ( X = 0 ) and its probability mass function is p (0) = P { X = 0 } = 1 − p p (1) = P { X = 1 } = p for 0 ≤ p ≤ 1 . Binomial random variable : We repeat the Bernoulli experiment n times. The n trials are independent. Such an independent repetition of the same experiment under identical conditions is called a Bernoulli Trial . Let random variable X denotes the total number of times that E has occurred (success). We have, p ( i ) =  n i ! p i (1 − p ) n − i , i = 0 , 1 , . . . , n The binomial coeﬃcient enters here to account for the total number of possible ways that the i times of success can be located among the n times of performing the experiment. We let 0! = 1 by deﬁnition. ©A. K. Khandani, E&CE307—Fall 2024 53 4.6.1 Properties of Binomial Random Variables The mean of the distribution is: E [ X ] = n X i =0 iP ( i ) = n X i =1 i  n i  p i (1 − p ) n − i = n X i =1 i n ! i !( n − i )! p i (1 − p ) n − i = n X i =1 n ( n − 1)! ( i − 1)!( n − i )! p i (1 − p ) n − i = n n X i =1 ( n − 1)! ( i − 1)!( n − i )! p i (1 − p ) n − i substituting j = i − 1 , we obtain, = n n − 1 X j =0 ( n − 1)! j !( n − 1 − j )! p j +1 (1 − p ) n − 1 − j = np n − 1 X j =0 ( n − 1)! j !( n − 1 − j )! p j +1 (1 − p ) n − 1 − j = np n − 1 X j =0  n − 1 j  p j (1 − p ) n − 1 − j  = np ( p + 1 − p ) n − 1  = np (1) n − 1  = np The variance of the distribution can be computed similarly. The result is Var( X ) = np (1 − p ) If X is a binomial random variable with parameters ( n, p ) , as k goes from 0 to n , P { X = k } ﬁrst increases monotonically and then decreases monotonically, reaching its largest value when k is the largest integer less than or equal to ( n + 1) p . ©A. K. Khandani, E&CE307—Fall 2024 54 4.6.2 Computing the Binomial Distribution Function Suppose that X is binomial with parameters ( n, p ) . The key to computing its distribution function P { X ≤ i } = i X k =0  n k ! p k (1 − p ) n − k i = 0 , 1 , . . . , n is to utilize the following relationship between P { X = k + 1 } and P { X = k } , P { X = k + 1 } = p 1 − p n − k k + 1  P { X = k } 4.7 The Poisson Random Variable A random variable X , taking one of the values 0 , 1 , 2 , . . . is said to be a Poisson random variable with parameter λ if for some λ > 0 , p ( i ) = P { X = i } = e − λ λ i i ! The Poisson random variable is used as an approximation of a binomial random variable with parameter ( n, p ) when n is large and p is small so that np is in moderate size. The parameter of the approximated Poisson random variable is λ = np . That is, P { X = i } = n ! ( n − i )! i ! p i (1 − p ) n − i = n ! ( n − i )! i !  λ n  i  1 − λ n  n − i = n ( n − 1) · · · ( n − i + 1) n i λ i i ! (1 − λ/n ) n (1 − λ/n ) i Now, for n large and λ moderate,  1 − λ n  n ≈ e − λ n ( n − 1) · · · ( n − i + 1) n i ≈ 1  1 − λ n  i ≈ 1 ©A. K. Khandani, E&CE307—Fall 2024 55 Hence, for n large and λ moderate, P { X = i } = e − λ λ i i ! The mean of the distribution is: E [ X ] = ∞ X i =0 ie − λ λ i i ! = λ ∞ X i =1 ie − λ λ i − 1 ( i − 1)! = λe − λ ∞ X j =0 λ j j ! by substituting j = i − 1 , = λ since ∞ X j =0 λ j j ! = e λ The variance of the distribution can be computed similarly. The result is Var( X ) = λ The expected value and variance of a Poisson random variable are both equal to its parameter λ . ©A. K. Khandani, E&CE307—Fall 2024 56 4.7.1 Computing the Poisson Distribution Function If X is Poisson with parameter λ , then P { X = i + 1 } P { X = i } =  e − λ λ i +1 / ( i + 1)! e − λ λ i / ( i )! = λ ( i + 1) Starting with P { X = i + 1 } = e − λ , we can use the above equation to compute successively P { X = 1 } = λP { X = 0 } P { X = 2 } = λ 2  P { X = 1 } ... P { X = i + 1 } = λ i + 1 P { X = i } 4.8 Other Discrete Probability Distributions Geometric Distribution : Suppose we repeat an experiment, independently, until the event E (success) occurs. What is the probability that the ﬁrst success occurs on the n th try if, on any one try, we have P ( E ) = p ? P { X = n } = (1 − p ) n − 1 p, n = 1 , 2 , . . . This is referred to as the Geometric Distribution . We can show that, E [ X ] = 1 p , Var( X ) = 1 − p p 2 ©A. K. Khandani, E&CE307—Fall 2024 57 The negative binomial random variable : Suppose we repeat an experiment. Let X be the number of trials to obtain the ﬁrst r success. We can show that, P ( X = n ) =  n − 1 r − 1 ! p r (1 − p ) n − r , n = r, r + 1 , . . . This is called the negative binomial Distribution . The binomial coeﬃcient enters here to account for the total number of possible ways that the r − 1 times of success can be located among the n − 1 times of performing the experiment. Note that the last experiment has deﬁnitely resulted in a success, and consequently, we only consider the number of ways that the ﬁrst r − 1 success are located among the ﬁrst n − 1 trials. We have, E [ X ] =  r p and Var( X ) = r (1 − p ) p 2 The hypergeometric random variable : Assume that N items are in an urn, numbered 1 , 2 , . . . , N . Suppose that items 1 , 2 , . . . , m are white and the remaining N − m items are black. Assume further that n items from the N are chosen at random. The probability that, in a draw of n items, we obtain exactly i white items is equal to: P ( X = i ) =  m i ! N − m n − i !  N n ! , i = 0 , 1 , 2 , . . . , min ( n, m ) This is referred to as the Hypergeometric Distribution . The mean and variance of the Hypergeometric Distribution are equal to: E [ X ] =  nm N , Var( X ) =  N − n N − 1  np (1 − p ) ©A. K. Khandani, E&CE307—Fall 2024 58 The Zeta (or zipf) distribution : A random variable is said to have a zeta distribution if its probability mass function is given by P { X = k } = C k α +1 for some value of α > 0 and C = " ∞ X k =1  1 k  α +1 # − 1 The zeta distribution owes its name to the fact that the function ζ ( s ) = 1 +  1 2  s +  1 3  s + · · ·  1 k  s + · · · is known in mathematical disciplines as the Riemann zeta function (after the German mathematician G. B. F. Riemann). 4.9 Properties of Cumulative Distribution Function Some Properties of the cumulative distribution function (c.d.f.) F are 1. F is a nondecreasing function; that is, if a < b , then F ( a ) ≤ F ( b ) . 2. lim b →∞ F ( b ) = 1 . 3. lim b →−∞ F ( b ) = 0 . 4. F is right continuous. That is, for any b and any decreasing sequence b n , n ≥ 1 , that converges to b , lim n →∞ F ( b n ) = F ( b ) . 5. P { a < X ≤ b } = F ( b ) − F ( a ) ©A. K. Khandani, E&CE307—Fall 2024 59 4.10 Some Solved Problems 1. A large lot of items is known to contain a fraction θ defective. Let X denote the random variable for the number of items to be inspected to obtain the second defective item. Find the probability distribution and mean of X . Solution: If the second defective item is drawn on the i th draw, then the ﬁrst defective can be in one of the ﬁrst ( i − 1) positions. The probability that X = i is then P ( X = i ) = ( i − 1)(1 − θ ) i − 2 θ 2  , i = 2 , 3 , · · · . To see that this is a probability distribution, note the following formulae, obtained by diﬀerentiating the ﬁrst one two times: ∞ X i =0 x i  = 1 1 − x , ∞ X i =1 ix i − 1  = 1 (1 − x ) 2 , ∞ X i =2 i ( i − 1) x i − 2  = 2 (1 − x ) 3 It follows readily from these relationships that: ∞ X j =2 ( i − 1) θ 2 (1 − θ ) i − 2  = 1 and E ( X ) = ∞ X i =2 iP ( X = i ) = ∞ X i =2 i ( i − 1) θ 2 (1 − θ ) i − 2  = 2 θ 2. Find the mean and variance of the Poisson random variable Y if it is three times as likely that Y = 4 than Y = 2 . Solution : Since, by the problem statement, P ( Y = 4) = e − λ λ 4 4! = 3 e − λ λ 2 2! → λ = 6 Then E ( Y ) = Var( Y ) = λ = 6 (property of the Poisson distribution). 3. The discrete random variable U has a geometric distribution of the form P ( U = j ) = ap j  , j = 0 , 1 , 2 , · · · If P ( U ≥ 4) = 1 / 256 , ﬁnd P ( U ≥ 2) . Solution: Since the given distribution must sum to unity we have ∞ X j =0 ap j  = 1 = a/ (1 − p ) which implies that a = 1 − p . Now P ( U ≥ 4) = ∞ X j =4 ap j  = 1 / 256 =  ap 4 1 − p  = 1 256 ⇒ p = 1 4 Then P ( U ≥ 2) = (1 − p ) p 2 / (1 − p ) = 1 / 16 . 4. Z is a discrete random variable with probability distribution P ( Z = j ) = aj 2 − ( j − 1)  , j = 1 , 2 , · · · For what value of a is this a distribution? ©A. K. Khandani, E&CE307—Fall 2024 60 Solution: Notice that for | x | < 1 , ∞ X j =0 x j  = 1 / (1 − x ) . Diﬀerentiating both sides of this relationship wrt x gives ∞ X j =1 jx j − 1  = 1 / (1 − x ) 2 . Applying this to the problem gives: ∞ X j =1 aj 2 − ( j − 1)  = a  1 − 1 2  2 = 1 which gives 4 a = 1 or a = 1 / 4 . 5. The number of trials, X , to obtain the ﬁrst defective from a large lot of items, has a geometric distribution with variance 2 . What is the probability it takes more than 4 draws to obtain the ﬁrst defective? Solution: The mean of a geometric distribution is 1 /θ and the variance is (1 − θ ) /θ 2 . If (1 − θ ) /θ 2  = 2 then θ = 1 / 2 . The probability X > 4 is then P ( X > 4) = ∞ X 5 θ i − 1 (1 − θ ) = θ 5 1 − θ  = 1 16  . 6. The number of emissions from a radioactive source, Z , in a one hour period, is known to be a Poisson random variable. If it is known that the probability there are 10 emissions in the one hour period, is exactly the same as the probability there are 12 emissions, what is the probability there are no emissions in the one hour period? Solution: If P ( X = 10) = exp( − λ ) λ 10 / 10! = exp( − λ ) λ 12 / 12! then λ 2  = 132 . it follows that P ( X = 0) = e − λ  = e − √ 132 . 7. In Lotto 649, a player chooses 6 distinct numbers from 1 to 49. This set is compared to 6 other numbers chosen at random by the house and prizes are awarded according to the number matched. (a) What is the probability that k of the numbers match, k = 3 , 4 , 5 , 6 . Calculate the answer to 8 signiﬁcant digits. (b) What is the probability that all six numbers chosen by the house are from 1 to 9 ? (c) What is the probability that 2 of the numbers chosen by the house are from 1 to 9 and one each is from 10 to 19 , 20 to 29 , 30 to 39 and 40 to 49 respectively? Solution: (a) Let X be the number of the player’s 6 numbers that match the house’s 6 numbers. Then X has a hypergeometric distribution and P ( X = k ) =  6 k ! 43 6 − k !  49 6 ! which is evaluated as: P ( X = 3) = . 017650404 P ( X = 4) = . 00096861972 P ( X = 5) = . 000018449899 P ( X = 6) = 7 . 1511238 × 10 − 8 ©A. K. Khandani, E&CE307—Fall 2024 61 (b) The probability all six numbers chosen by the house are from 1 to 9 is:  9 6 !  49 6 ! = 6 . 00694403 × 10 − 6  . (c) The required probability is:  9 2 ! 10 1 ! 4  49 6 ! = 0 . 025744046 . 8. In the game of Keno, a player chooses 5 distinct numbers from 1 to 100 , inclusive. A machine then chooses 10 numbers, again from 1 to 100 . Let X be the number of the player’s numbers chosen by the machine. Find the probability distribution of X . Solution: This is a simple hypergeometric distribution: P ( X = k ) =  10 k ! 90 5 − k !  100 5 ! . 9. Consider a random experiment in which the probability of certain outcome, say A , is equal to p . The experiment is performed for n times. Let X denote the total number of times that A has happened. Compute the average and the standard deviation of X . Solution: Consider an experiment having only two possible outcomes, A and  ¯ A , which are mutually exclu- sive. Let the probabilities be P ( A ) = p and P ( ¯ A ) = 1 − p = q . The experiment is repeated n times and the probability of A occurring i times is P i =  n i ! p i q n − i where  n i ! is the binomial coeﬃcient,  n i ! = n ! i !( n − i )! The binomial coeﬃcient enters here to account for the total number of possible ways to combine n items taken i at a time, with all possible permutations permitted. We let 0! = 1 by deﬁnition. This is called a Binomial Random Variable. It is straightforward to show that the mean and the variance of a Binomial Random Variable are equal to, mean = np & σ 2  = np (1 − p ) = m (1 − p ) ©A. K. Khandani, E&CE307—Fall 2024 62 To compute the mean, we have, mean = n X i =0 iP i = n X i =1 i  n i ! p i q n − i = n X i =1 i n ! i !( n − i )! p i q n − i = n X i =1 n ( n − 1)! ( i − 1)!( n − i )! p i q n − i = n n X i =1 ( n − 1)! ( i − 1)!( n − i )! p i q n − i substituting j = i − 1 , we obtain, = n n − 1 X j =0 ( n − 1)! j !( n − 1 − j )! p j +1 q n − 1 − j = np n − 1 X j =0 ( n − 1)! j !( n − 1 − j )! p j q n − 1 − j = np n − 1 X j =0  n − 1 j ! p j q n − 1 − j  = np ( p + q ) n − 1  = np (1) n − 1  = np The standard deviation can be computed similarly. 10. A person repeatedly shoots at a target and stops as soon as he hits it. The probability of hitting the target is 2/3 each time. The shots are ﬁred independently, and hence may be regarded as a sequence of repeated trials. Determine the probability that the target is hit on the k th attempt. Solution: The event in which we are interested happens if the target is missed k − 1 times and then is hit the k th time. The probability of missing the target with the ﬁrst shot is 1 − 2 / 3 = 1 / 3 , and likewise in each of the following shots. Because of the independence, the probability we seek is (1 / 3) k − 1 (2 / 3) . 11. Russian roulette is played with a revolver equipped with a rotatable magazine of six shots. The revolver is loaded with one shot. The ﬁrst duelist, A , rotates the magazine at random, points the revolver at his head and presses the trigger. If, afterwards, he is still alive, he hands the revolver to the other duelist, B , who acts in the same way as A . The players shoot alternately in this manner, until a shot goes oﬀ. Determine the probability that A is killed. Solution: Let H i be the event that “a shot goes oﬀat the i th trial”. The events H i are mutually exclusive. The event H i occurs if there are i − 1 “failures” and then one “success”. Hence, we get, P ( H i ) =  5 6  i − 1 1 6 The probability we want is given by P = P ( H 1 ∪ H 3 ∪ H 5 ∪· · · ) = 1 6 " 1 +  5 6  2 +  5 6  4 + · · · # = 1 6  · 1 1 −  5 6  2 = 6 11 Hence the probability that B loses his life is 1 − 6 / 11 = 5 / 11 ; that is, the second player has a somewhat greater chance of surviving, as might be expected. ©A. K. Khandani, E&CE307—Fall 2024 63 12. In a food package there is a small prize which can be of N diﬀerent types. All types are distributed into the packages at random with the same probability. A person buys n packages in a shop. What is the probability that he obtains a complete collection of presents? Solution: Denote the event in question by H and consider its complement  ¯ H , which is the event that at least one type is missing. We then have P ( ¯ H ) = P  N [ 1 A i ! , where A i =“type no. i is missing”. We now utilize the same addition formula for the union of some events. It is evident that P ( A i 1 A i 2 ...A i r ) =  1 − r N  n for the probability that a given package does not contain types nos. i 1 , i 2 , ..., i r is 1 − r/N . Hence we ﬁnd 1 − P ( H ) = P ( H c ) = N  1 − 1 N  n −  N 2 !  1 − 2 N  n + · · · +( − 1) N − 1  N N !  1 − N N  n 13. Suppose we roll a die repeatedly until a 6 occurs, and let N be the number of times we roll the die. Compute the average value and the variance of N . Solution: Using × to denote “not a 6”, we have P ( N = 1) = P (6) = 1 6 P ( N = 2) = P ( × 6) = 5 6  · 1 6 P ( N = 3) = P ( × × 6) = 5 6  · 5 6  · 1 6 From the ﬁrst three terms it is easy to see that for k ≥ 1 P ( N = k ) = P ( × on k-1 rolls then 6 ) =  5 6  k − 1 1 6 Generalizing, we see that if we are waiting for an event of probability p , the number of trials needed, N , has the following distribution, P ( N = k ) = (1 − p ) k − 1 p for k = 1 , 2 , . . . since { N = k } occurs exactly when we have k − 1 failures followed by a success. This is called a Geometric Distribution and as we will see in the lecture, it has, E ( N ) = 1 /p & V ar ( N ) = (1 − p ) /p 2 These values can be easily obtained using the second and the third formulas given in your sheet of “Some useful relationships”. 14. Suppose that an airplane engine will fail, when in ﬂight, with probability 1 − p independently from engine to engine. Suppose that the airplane will make a successful ﬂight if at least 50 per cent of its engines remain operative. For what values of p is a 4-engine plane preferable to a 2-engine plane? ©A. K. Khandani, E&CE307—Fall 2024 64 Solution: As each engine is assumed to fail or function independently of what happens with the other engines, it follows that the number of engines remaining operative is a binomial random variable. Hence the probability that a 4-engine plane makes a successful ﬂight is  4 2 ! p 2 (1 − p ) 2  +  4 3 ! p 3 (1 − p ) +  4 4 ! p 4 (1 − p ) 0  = 6 p 2 (1 − p ) 2  + 4 p 3 (1 − p ) + p 4 whereas the corresponding probability for a 2-engine plane is  2 1 ! p (1 − p ) +  2 2 ! p 2  = 2 p (1 − p ) + p 2 Hence the 4-engine plane is safer if 6 p (1 − p ) 2  + 4 p 2 (1 − p ) + p 3  ≥ 2 − p which simpliﬁes to 3 p 3  − 8 p 2  + 7 p − 2 ≥ 0 or ( p − 1) 2 (3 p − 2) ≥ 0 which is equivalent to 3 p − 2 ≥ 0 or p ≥ 2 3 Hence the 4-engine plane is safer when the engine success probability is at least as large as  2 3 , whereas the 2-engine plane is safer when this probability falls below  2 3 . 15. Ten hunters are waiting for ducks to ﬂy by. When a ﬂock of ducks ﬂies overhead, the hunters ﬁre at the same time, but each chooses his target at random, independently of the others. If each hunter independently hits his target with probability p , compute the expected number of ducks that escape unhurt when a ﬂock of size 10 ﬂies overhead. Solution: Let X i equal 1 if the i th duck escapes unhurt and 0 otherwise, i = 1 , 2 , . . . , 10 . The expected number of ducks to escape can be expressed as E [ X 1 + · · · + X 10 ] = E [ X 1 ] + · · · + E [ X 10 ] To compute E [ X i ] = P ( X i = 1) , we note that each of the hunters will, independently, hit the i th duck with probability p/ 10 and so P ( X i = 1) =  1 − p 10  10 Hence E [ X ] = 10  1 − p 10  10 16. A fair coin is to be tossed until a head comes up for the ﬁrst time. What is the chance of that happening on an odd-numbered toss? Solution: Suppose we let P ( k ) denote the probability that the ﬁrst head appears on the k th toss. Since the coin was presumed to be fair, P (1) = 1 2 . Furthermore, we would expect half of the coins that showed a tail on the ﬁrst toss to come up heads on the second, so, intuitively, P (2) = 1 4 . In general, P ( k ) =  1 2  k , ©A. K. Khandani, E&CE307—Fall 2024 65 k = 1 , 2 , . . . . Let E be the event “ﬁrst head appears on an odd-numbered toss.” Then P ( E ) = P (1) + P (3) + P (5) + · · · = ∞ X i =0  1 2  2 i +1 = 1 2 ∞ X i =0  1 4  i Recall the formula for the sum of a geometric series: If 0 < x < 1, ∞ X k =0 x k  = 1 1 − x Applying that result to P ( E ) gives P ( E ) = 1 2  ·     1 1 − 1 4     = 2 3 A similar computation would show that the probability of the ﬁrst head appearing on an even-numbered toss is  1 3 . 17. Two gamblers, A and B, each choose an integer from 1 to m (inclusive) at random. What is the probability that the two numbers they pick do not diﬀer by more than n ? Solution: It will be easier if we approach this problem via its complement. Let x and y denote the numbers selected by A and B, respectively. The complement has two cases, depending on whether x < y or x > y . Let us ﬁrst suppose that x < y . Then, for a given x , the values of y such that y − x > n are y = x + n + 1 , y = x + n + 2 , . . . , and y = m , altogether, a range of m − n − x choices. Summing over x , we ﬁnd that the total number of ( x , y )-pairs such that y − x > n reduces to the sum of the ﬁrst m − n − 1 integers: m − n − 1 X x =1 ( m − n − x ) = m − n − 1 X i =1 i = ( m − n − 1)( m − n ) 2 By symmetry, the same number of ( x , y )-pairs satisﬁes the second case: x > y and x − y > n . Thus the total number of ( x , y )-selections such that | x − y | > n is ( m − n − 1)( m − n ) . The sample space S contains m 2  outcomes, all equally likely by assumption. It follows that P ( | x − y | ≤ n ) = 1 − ( m − n − 1)( m − n ) m 2 18. A secretary is upset about having to stuﬀenvelopes. Handed a box of n letters and n envelopes, he vents his frustration by putting the letters into the envelopes at random . How many people, on the average, will receive the correct mail? Solution: If X denotes the number of envelopes properly stuﬀed, what we are seeking is E ( X ) . Let X i denote a random variable equal to the number of correct letters put into the i th envelope, i = 1 , 2 , . . . , n . Then X i equals 0 or 1, and f X i ( k ) = P ( X i = k ) =      1 n for k =1 n − 1 n for k =0 ©A. K. Khandani, E&CE307—Fall 2024 66 But X = X 1 + X 2 + · · · + X n and E ( X ) = E ( X 1 ) + E ( X 2 ) + · · · + E ( X n ) . Furthermore, each of the X i ’s has the same expected value, 1 /n : E ( X i ) = 1 X k =0 k · P ( X i = k ) = 0 ·  n − 1 n + 1 ·  1 n  = 1 n It follows that E ( X ) = n X i =1 E ( X i ) = n  1 n  = 1 showing that, regardless of n , the expected number of properly stuﬀed envelopes is 1. 19. The honor count in a (13-card) bridge hand can vary from 0 to 37 according to the formula Honor count = 4 · number of aces + 3 · number of kings + 2 · number of queens + 1 · number of jacks What is the expected honor count of North’s hand? Solution: If X i , i =1,2,3,4, denotes the honor count for North, South, East, and West, respectively, and if X denotes the analogous sum for the entire deck, we can write X = X 1 + X 2 + X 3 + X 4 But X = E ( X ) = 4 · 4 + 3 · 4 + 2 · 4 + 1 · 4 = 40 By symmetry, E ( X i ) = E ( X j ) , i ̸ = j , so it follows that 40 = 4 · E ( X 1 ) , which implies that 10 is the expected honor count of North’s hand. 20. Let X 1 , X 2 , . . . , X n denote a set of n independent observations made on a random variable X having pdf f X ( x ) . Let σ 2  = E [( X − µ ) 2 ] denote the variance of X . The sample variance of the X i ’s, denoted by S 2 , is deﬁned as S 2  = 1 n − 1 n X i =1 ( X i − X ) 2 where X = 1 n n X i =1 X i . Show that E ( S 2 ) = σ 2 . Solution: We know that if X is a random variable having mean µ and E ( X 2 ) ﬁnite, then V ar ( X ) = E ( X 2 ) − µ 2 Hence, we rewrite S 2  in a form that enables us to apply the above equation: E ( S 2 ) = E " 1 n − 1 n X i =1 ( X i − X ) 2 # = E " 1 n − 1 n X i =1 ( X 2 i  − 2 X i X + X 2 ) # = E " 1 n − 1  n X i =1 X 2 i  − 2 X n X i =1 X i + nX 2 !# = 1 n − 1 " n X i =1 E ( X 2 i  ) − 2 E  X n X i =1 X i ! + nE ( X 2 ) # ©A. K. Khandani, E&CE307—Fall 2024 67 But, we have: E ( X 2 i  ) = σ 2 + µ 2 and, E  X n X i =1 X i ! = 1 n n X i =1 n X j =1 E ( X i X j ) = 1 n h ( n 2  − n ) µ 2  + n ( σ 2  + µ 2 ) i = nµ 2  + σ 2 and, E ( X 2 ) = 1 n 2 n X i =1 n X j =1 E ( X i X j ) = 1 n 2 h ( n 2  − n ) µ 2  + n ( σ 2  + µ 2 ) i = 1 n ( nµ 2 + σ 2 ) = µ 2 + σ 2 n Therefore: E ( S 2 ) = 1 n − 1 h n ( σ 2  + µ 2 ) − 2 nµ 2  − 2 σ 2  + nµ 2  + σ 2 i = 1 n − 1 ( n − 1) σ 2 = σ 2 21. A chip is to be drawn at random from each of k urns, each holding n chips numbered 1 through n . What is the probability all k chips will bear the same number? Solution: If X 1 , X 2 , . . . , X k denote the numbers on the 1st, 2nd,..., k th chips, respectively, we have, P ( X i = α ) = 1 n , ∀ X i ∈{ X 1 , . . . , X k } , ∀ α ∈ [1 , n ] We are looking for the probability that X 1 = X 2 = · · · = X k . Considering the independence of the events, we obtain, P ( X 1 = X 2 = · · · = X k = α ) =  1 n  k , ∀ α ∈ [1 , n ] Adding up these values for diﬀerent values of α , we obtain, P ( X 1 = X 2 = · · · = X k ) = 1 n k − 1 22. Mean of a Hypergeometric Random Variable: If n balls are randomly selected from an urn containing N white and M black balls, ﬁnd the expected number of white balls selected. Solution: Let X denote the number of white balls selected. It follows that P ( X = k ) =  N k ! M n − k !  N + M n ! Hence, assuming n ≤ N : E [ X ] = n X k =0 k  N k ! M n − k !  N + M n ! However, we can obtain a simpler expression for E [ X ] by using the representation X = X 1 + · · · + X n ©A. K. Khandani, E&CE307—Fall 2024 68 where X i =  1 if the i th ball selected is white 0 otherwise Since the i th ball selected is equally likely to be any of the N + M , we have E [ X i ] = N M + N and thus E [ X ] = E [ X 1 ] + · · · + E [ X N ] = nN M + N 23. Six balls are tossed independently into three boxes A , B , C . For each ball the probability of going into a speciﬁc box is 1/3. Find the probability that box A will contain (a) exactly four balls, (b) at least two balls, (c) at least ﬁve balls. Solution: Here we have six Bernoulli trials, with success corresponding to a ball in box A , failure to a ball in box B or C . Recall, a sequence of n Bernoulli trials is a sequence of n independent observations each of which may result in exactly one of two possible situations, called “success” or “failure”. At each observation the probability of success is p and the probability of failure is q = 1 − p . Thus, referring the our problem we have that n = 6, p = 1/3, q = 2/3, and so the required probabilities are ( a ) p (4) =  6 4 ! 1 3  4  2 3  2 ( b ) 1 − p (0) − p (1) = 1 −  2 3  6 −  6 1 !  1 3   2 3  5 ( c ) p (5) + p (6) =  6 5 ! 1 3  5  2 3  +  1 3  6 24. A bubble gum company is printing a special series of baseball cards featuring r of the greatest base-stealers of the past decade. Each player appears on the same number of cards, and the cards are randomly distributed to retail outlets. If a collector buys n ( ≥ r ) packets of gum (each containing one card), what is the probability she gets a complete set of the special series? Solution: Let A i be the event the collector has no card for player i , i = 1 , 2 , . . . , r , and deﬁne A to be the union, A = ∪ r i =1 A i . Then P ( collector has at least one card of each player ) = 1 − P ( A ) To begin the derivation of P ( A ) , notice that for any value of k , k = 1 , 2 , . . . , r , P ( A 1 A 2 · · · A k ) =  1 − k r  n Therefore, from the general Addition Law, P  r [ i =1 A i ! = P ( A ) = r X i =1  1 − 1 r  n − X i<j  1 − 2 r  n + X i<j<k  1 − 3 r  n −· · · + ( − 1) r +1  · 0 =  r 1 !  1 − 1 r  n −  r 2 !  1 − 2 r  n +  r 3 !  1 − 3 r  n −· · · + ( − 1) r +1  · 0 ©A. K. Khandani, E&CE307—Fall 2024 69 Or more concisely, P ( A ) = r X k =1 ( − 1) k +1  r k !  1 − k r  n 25. An urn contains nine chips, ﬁve red and four white. Three are drawn out at random without replacement. Let X denote the number of red chips in the sample. Find E ( X ) , the expected number of red chips selected. Solution: We recognize X to be a Hypergeometric random variable, where P ( X = x ) =  5 x ! 4 3 − x !  9 3 ! , x = 0 , 1 , 2 , 3 Hence, E ( X ) = X all x x · P ( X = x ) = 3 X i =0 x ·  5 x ! 4 3 − x !  9 3 ! = (0)  4 84  + (1)  30 84  + (2)  40 84  + (3)  10 84  = 5 3 26. The following problem was posed and solved in the eighteenth century by Daniel Bernoulli. Suppose that a jar contains 2 N cards, two of them marked 1, two marked 2, two marked 3, and so on. Draw out m cards at random. What is the expected number of pairs that still remain in the jar? Solution: Deﬁne, i = 1 , 2 , . . . , N X i =  1 if the i th pair remains in the jar 0 otherwise Now, E [ X i ] = P ( X i = 1) =  2 N − 2 m !  2 N m ! = (2 N − 2)! m !(2 N − 2 − m )! (2 N )! m !(2 N − m )! = (2 N − m )(2 N − m − 1) (2 N )(2 N − 1) Hence the desired result is E [ X 1 + X 2 + · · · + X N ] = E [ X 1 ] + · · · + E [ X N ] = (2 N − m )(2 N − m − 1) 2(2 N − 1) ©A. K. Khandani, E&CE307—Fall 2024 70 27. Consider a jury trial in which it takes 8 of the 12 jurors to convict; that is, in order for the defendant to be convicted, at least 8 of the jurors must vote him guilty. If we assume that jurors act independently and each makes the right decision with probability θ , what is the probability that the jury renders a correct decision? Solution: The problem, as stated, is incapable of solution, for there is not yet enough information. For instance, if the defendant is innocent, the probability of the jury’s rendering a correct decision is 12 X i =5  12 i ! θ i (1 − θ ) 12 − i whereas, if he is guilty, the probability of a correct decision is 12 X i =8  12 i ! θ i (1 − θ ) 12 − i Therefore, if α represents the probability that the defendant is guilty, then, by conditioning on whether or not he is guilty, we obtain that the probability that the jury renders a correct decision is α 12 X i =8  12 i ! θ i (1 − θ ) 12 − i  + (1 − α ) 12 X i =5  12 i ! θ i (1 − θ ) 12 − i 28. A single unbiased die is tossed independently n times. Let R 1 be the number of 1’s obtained, and R 2 the number of 2’s. Find E ( R 1 R 2 ) . Solution: The indicator of an event A is a random variable I A and is deﬁned as follows. I A ( ω ) =  1 if ω ∈ A 0 if ω ̸∈ A If A i is the event that the i th toss results in a 1, and B i the event that the i th toss results in a 2, then R 1 = I A 1 + · · · + I A n R 2 = I B 1 + · · · + I B n Hence E ( R 1 R 2 ) = n X i,j =1 E ( I A i I B j ) Now if i ̸ = j , I A i and I B j are independent; hence E ( I A i I B j ) = E ( I A i ) E ( I B j ) = P ( A i ) P ( B j ) = 1 36 If i = j , A i and B j are disjoint, since the i th toss cannot simultaneously result in a 1 and a 2. Thus I A i I B i = I A i ∩ B i = 0. Thus E ( R 1 R 2 ) =  n ( n − 1) 36 since there are n ( n − 1) ordered pairs ( i , j ) of integers ∈{ 1 , 2 , . . . , n } such that i ̸ = j . 29. A miner is trapped in a mine containing 3 doors. The ﬁrst door leads to a tunnel that will take him to safety after 3 hours of travel. The second door leads to a tunnel that will return him to the mine after 5 hours of travel. The third door leads to a tunnel that will return him to the mine after 7 hours. If we assume that ©A. K. Khandani, E&CE307—Fall 2024 71 the miner is at all times equally likely to choose any one of the doors, what is the expected length of time until he reaches safety? Solution: Let X denote the amount of time (in hours) until the miner reaches safety, and let Y denote the door he initially chooses. Now E [ X ] = E [ X | Y = 1] P ( Y = 1) + E [ X | Y = 2] P ( Y = 2) + E [ X | Y = 3] P ( Y = 3) = 1 3 ( E [ X | Y = 1] + E [ X | Y = 2] + E [ X | Y = 3]) However, E [ X | Y = 1] = 3 E [ X | Y = 2] = 5 + E [ X ] E [ X | Y = 3] = 7 + E [ X ] To understand why the above equation is correct, consider, for instance, E [ X | Y = 2] and reason as follows. If the miner chooses the second door, he spends 5 hours in the tunnel and then returns to his cell. But once he returns to his cell the problem is as before; thus his expected additional time until safety is just E[X]. Hence E [ X | Y = 2] = 5 + E [ X ] . The argument behind the other equalities in the above equation is similar. Hence E [ X ] = 1 3 (3 + 5 + E [ X ] + 7 + E [ X ]) or E [ X ] = 15 ©A. K. Khandani, E&CE307—Fall 2024 72