Properties of Expectation 7.1 Introduction In this chapter we develop and exploit additional properties of expected values. 7.2 Expectation of Sums of Random Variables Suppose that X and Y are random variables and g is a function of two variables. Then we have the following result. Proposition: If X and Y have a joint probability mass function p ( x, y ) , then E [ g ( X, Y )] = X y X x g ( x, y ) p ( x, y ) If X and Y have a joint probability density function f ( x, y ) , then E [ g ( X, Y )] = Z ∞ −∞ Z ∞ −∞ g ( x, y ) f ( x, y ) dxdy For an important application of this proposition, suppose that E [ X ] and E [ Y ] are both ﬁnite and let g ( X, Y ) = X + Y . Then, in the continuous case, E [ X + Y ] = Z ∞ −∞ Z ∞ −∞ ( x + y ) f ( x, y ) dxdy = Z ∞ −∞ Z ∞ −∞ xf ( x, y ) dxdy + Z ∞ −∞ Z ∞ −∞ yf ( x, y ) dxdy = Z ∞ −∞ xf X ( x ) dx + Z ∞ −∞ yf Y ( y ) dy = E [ X ] + E [ Y ] The same result holds in general. Using this result and induction shows that, if E [ X i ] is ﬁnite for all i = 1 , . . . , n , then E [ X 1 + · · · + X n ] = E [ X 1 ] + · · · + E [ X n ] ©A. K. Khandani, E&CE307—Fall 2024 121 7.3 Covariance, Variance of Sums, and Correlations Proposition: If X and Y are independent, then for any functions h and g , E [ g ( X ) h ( Y )] = E [ g ( X )] E [ h ( Y )] Deﬁnition: The covariance between X and Y , denoted by Cov( X, Y ) , is deﬁned by Cov( X, Y ) = E [( X − E [ X ])( Y − E [ Y ])] Just as the expected value and the variance of a single random variable give us information about this random variable, so does the covariance between two random variables give us information about the relationship between the random variables. Upon Expanding the preceding deﬁnition, we see that Cov( X, Y ) = E [ XY ] − E [ X ] E [ Y ] Note that if X and Y are independent then using the preceding proposition, it follows that Cov( X, Y ) = 0 . However, the converse is not true. Proposition: (i) Cov( X, Y ) = Cov( Y, X ) (ii) Cov( X, X ) = Var( X ) (iii) Cov( aX, Y ) = a Cov( X, Y ) (iv) Cov   n X i =1 X i , m X j =1 Y j   = n X i =1 m X j =1 Cov( X i , Y j ) ©A. K. Khandani, E&CE307—Fall 2024 122 It follows from parts (ii) and (iv) of the preceding proposition, upon taking Y j = X j , j = 1 , . . . , n , that Var  n X i =1 X i ! = Cov  n X i =1 X i , n X i =1 X i ! = n X i =1 n X j =1 Cov( X i , X j ) = n X i =1 Var( X i ) + n X i =1 X j ̸ = i Cov( X i , X j ) Since each pair of indices, i, j, i ̸ = j appears twice in the double summation, the above is equivalent to the following: Var  n X i =1 X i ! = n X i =1 Var( X i ) + 2 n X i =1 X j>i Cov( X i , X j ) If X 1 , . . . , X n are pairwise independent, the preceding equation reduces to Var  n X i =1 X i ! = n X i =1 Var( X i ) ©A. K. Khandani, E&CE307—Fall 2024 123 Deﬁnition: The correlation of two random variables X and Y , denoted by ρ ( X, Y ) , is deﬁned, as long as Var( X )Var( Y ) is positive, by ρ ( X, Y ) = Cov( X, Y ) p Var( X )Var( Y ) It can be shown that − 1 ≤ ρ ( X, Y ) ≤ 1 In fact, ρ ( X, Y ) = 1 implies that Y = a + bX , where b = σ X /σ Y > 0 and ρ ( X, Y ) = − 1 implies that Y = a + bX , where b = − σ X /σ Y < 0 . The reverse is also true, if Y = a + bX , then ρ ( X, Y ) is either +1 or -1, depending on the sign of b . The correlation coeﬃcient is a measure of the degree of linearity between X and Y . A value of ρ ( X, Y ) near +1 or -1 indicates a high degree of linearity between X and Y , whereas a value near 0 indicates a lack of such linearity. A positive value of ρ ( X, Y ) indicates that Y tends to increase when X does, whereas a negative value indicates that Y tends to decrease when X increases. If ρ ( X, Y ) = 0 , then X and Y are said to be uncorrelated . ©A. K. Khandani, E&CE307—Fall 2024 124 7.4 Conditional Expectation 7.4.1 deﬁnitions We saw that for two jointly discrete random variables X and Y , given that Y = y , the conditional probability mass function is deﬁned by p X | Y ( x | y ) = P { X = x | Y = y } =  p ( x, y ) p Y ( y ) So, the conditional expectation of X , given that Y = y , for all values of y such that p Y ( y ) > 0 by E [ X | Y = y ] = X x xp { X = x | Y = y } = X x xp X | Y ( x | y ) Similarly for the case of continuous random variables, we have E [ X | Y = y ] = Z ∞ −∞ xf X | Y ( x | y ) dx where f X | Y ( x | y ) =  f ( x, y ) f Y ( y ) for all y such that f Y ( y ) > 0 . Similar to ordinary expectations we have E [ g ( X ) | Y = y ] =        X x g ( x ) p X | Y ( x | y ) in the discrete case Z ∞ −∞ g ( x ) f X | Y ( x | y ) dx in the continuous case and E " n X i =1 X i | Y = y # = n X i =1 E [ X i | Y = y ] ©A. K. Khandani, E&CE307—Fall 2024 125 7.4.2 Computing Expectations by Conditioning Let us denote by E [ X | Y ] that function of a random variable Y whose value at Y = y is E [ X | Y = y ] . Note that E [ X | Y ] is itself a random variable. An extremely important property of conditioning property of conditional expectation is given by the following proposition. Proposition: E [ X ] = E [ E [ X | Y ]] One way to understand this equation is to interpret it as follows: To calculate E [ X ] , we may take a weighted average of the conditional expected value of X , given that Y = y , each of the terms E [ X | Y = y ] being weighted by the probability of the event on which it is conditioned. 7.4.3 Computing probabilities by Conditioning We can also use conditioning to ﬁnd probabilities. To see this, let E denote an arbitrary event and deﬁne the indicator random variable X by X =  1 if E occurs 0 if E does not occur It follows from the deﬁnition of X that E [ X ] = P ( E ) E [ X | Y = y ] = P ( E | Y = y ) for any random variable Y Therefore, P ( E ) = X y P ( E | Y = y ) P ( Y = y ) if Y is discrete = Z ∞ −∞ P ( E | Y = y ) f Y ( y ) dy if Y is continuous Note that if Y is a discrete random variable taking on one of the values y 1 , . . . , y n , then, by deﬁning the events F i , i = 1 , . . . , n by F i = { Y = y i } , this equation reduces to the familiar equation P ( E ) = n X i =1 P ( E | F i ) P ( F i ) ©A. K. Khandani, E&CE307—Fall 2024 126 7.4.4 Conditional Variance We can deﬁne the conditional variance of X given that Y = y by Var( X | Y ) ≡ E [( X − E [ X | Y ]) 2 | Y ] and after simpliﬁcation Var( X | Y ) = E [ X 2 | Y ] − ( E [ X | Y ]) 2 There is a very useful relationship between Var( X ) , the unconditional variance of X , and Var( X | Y ) , the uncon- ditional variance of X given Y . E [Var( X | Y )] = E [ E [ X 2 | Y ]] − E [( E [ X | Y ]) 2 ] = E [ X 2 ] − E [( E [ X | Y ]) 2 ] Also, as E [ X ] = E [ E [ X | Y ]] , we have Var( E [ X | Y ]) = E [( E [ X | Y ]) 2 ] − ( E [ X ]) 2 By adding these two equations we obtain the following proposition. Proposition: Var( X ) = E [Var( X | Y )] + Var( E [ X | Y ]) ©A. K. Khandani, E&CE307—Fall 2024 127 7.5 Conditional Expectation and Prediction Sometimes a situation arises where the value of a random variable X is observed and then, based on the observed value, an attempt is made to predict the value of a second random variable Y . We would like to choose a function g so that g ( X ) tends to be close to Y . One possible criterion for closeness is to choose g so as to minimize E [( Y − g ( X )) 2 ] . The best possible predictor of Y is g ( X ) = E [ Y | X ] . Proposition: E [( Y − g ( X )) 2 ] ≥ E [( Y − E [ Y | X ]) 2 ] Linear Predictor: Sometimes the joint probability distribution function of X and Y is not completely known. If we know the means and variances of X and Y and correlation between them we can at least determine the best linear predictor of Y with respect to X . To obtain the best linear prediction of Y in respect to X , we need to choose a and b so as to minimize E [( Y − ( a + bX )) 2 ] . Minimizing this equation over a and b , yields b =  E [ XY ] − E [ X ] E [ Y ] E [ X 2 ] − ( E [ X ]) 2 = Cov( X, Y ) σ 2 X = ρ σ Y σ X a = E [ Y ] − bE [ X ] = E [ Y ] − ρσ Y E [ X ] σ X ©A. K. Khandani, E&CE307—Fall 2024 128 7.6 Moment Generating Function The moment generating function M ( t ) of a random variable X is deﬁned for all real values of t by M ( t ) = E [ e tX ] =        X x e tx p ( x ) if X is discrete with mass function p ( x ) Z ∞ −∞ e tx f ( x ) dx if X is continuous with density f ( x ) We call M ( t ) the moment generating function because all of the moments of X can be obtained by successively diﬀerentiating M ( t ) and then evaluating the result at t = 0 . For example, M ′ (0) = E [ X ] and M ′′ (0) = E [ X 2 ] In general the n th derivative of M ( t ) is given by M n (0) = E [ X n ] n ≥ 1 The moment generating function of some random variables are as follows. ©A. K. Khandani, E&CE307—Fall 2024 129 Moment Generating Functions of Some Random Variables If X is a binomial random variable with parameters n and p , then M ( t ) = ( pe t  + 1 − p ) n If X is a Poisson random variable with parameter λ , then M ( t ) = exp { λ ( e t  − 1) } If X is an exponential random variable with parameter λ , then M ( t ) = λ λ − t for t < λ If X is a standard normal random variable with parameters 0 and 1 , then M ( t ) = e t 2 / 2 If X is a normal random variable with parameters µ and σ 2 , then M ( t ) = exp ( σ 2 t 2 2 + µt ) ©A. K. Khandani, E&CE307—Fall 2024 130 Moment Generating function of the Sum of Independent Random Variables: An important property of moment generating function is the moment generating function of the sum of independent random variables equals to the product of the individual moment generating functions. If X and Y are independent, then M X + Y ( t ) = M X ( t ) M Y ( t ) 7.6.1 Joint Moment Generating Functions It is also possible to deﬁne the joint moment generating function of two or more ransom variables. M ( t 1 , . . . , t n ) = E [ e t 1 X 1 + ··· + t n X n ] The individual moment generating functions can be obtained from M ( t 1 , . . . , t n ) by letting all but one of the t j be 0. M X i ( t ) = M (0 , . . . , 0 , t, 0 , . . . , 0) where the t is in the i th place. It can be proved that M ( t 1 , . . . , t n ) uniquely determines the joint distribution of X 1 , . . . , X n . Joint Moment Generating function of Independent Random Variables: If the n random variables are independent, then M ( t 1 , . . . , t n ) = M X 1 ( t 1 ) · · · M X n ( t n ) On the other hand, if this equation is satisﬁed, then the n random variables are independent. ©A. K. Khandani, E&CE307—Fall 2024 131 7.7 Some Solved Problems 1. Let X and Y be two independent standard normal random variables (i.e. N (0 , 1) ).Find: (i) E ( | X | ) (ii) E ( X + Y ) (iii) V ( X − Y ) (iv) E ( √ X 2  + Y  2 ) . Solution: (i) E ( | X | ) = 2 Z ∞ 0 x e − x 2 / 2 √ 2 π dx = r 2 π where an elementary property of the normal integral has been used. (ii) E ( X + Y ) = E ( X ) + E ( Y ) = 0 (iii) Since E ( X − Y ) = 0 then V ( X − Y ) = Z ∞ −∞ ( x − y ) 2 e − ( x 2 + y 2 ) / 2 2 π dxdy = 1 + 1 = 2 (iv) E ( √ X 2  + Y  2 ) = Z ∞ −∞ Z ∞ −∞ ( x 2  + y 2 ) 1 / 2 e − ( x 2 + y 2 ) / 2 2 π dxdy = Z ∞ 0 Z 2 π 0 r 2 e − r 2 / 2 2 π drdθ = r π 2  . 2. Suppose that X and Y have the joint probability density function f XY =  (2 m + 6) x m y 0 < y < x < 1 0 otherwise . Show that this is a probability density function and ﬁnd E ( Y | X ) and V ( Y | X ) . Solution: The marginal denstiy of X is f X ( x ) = Z x 0 (2 m + 6) x m ydy = ( m + 3) x m +2  , 0 ≤ x ≤ 1 and notice that Z 1 0 ( m + 3) x m +2 dx = 1 and hence is a pdf. The conditional pdf is f Y | X ( y | x ) = 2( m + 3) x m y ( m + 3) x m +2 = 2 y x 2 , 0 ≤ y ≤ x and E ( Y | X ) = Z x 0 y ·  2 y x 2 dy = 2 3 x , E ( Y  2 | X ) = Z x 0 y 2  ·  2 y x 2 dy = 1 2 x 2 and V ( Y | X ) = E ( Y  2 | X ) − E 2 ( Y | X ) = x 2 / 18 . 3. Suppose X and Y have the joint density function e − y  for 0 < x < y < ∞ . Find the least mean squares line of Y on X and E ( Y | X ) . Solution: f X ( x ) = Z ∞ x e − y dy = e − x  , 0 ≤ x < ∞ ©A. K. Khandani, E&CE307—Fall 2024 132 and f Y | X ( y | x ) = e − ( y − x )  , x < y < ∞ and E ( Y | X ) = Z ∞ x ye − ( y − x ) dy = x + 1 Any time the conditional expectation is linear it is also the least squares line. However, for completeness, we compute the least squares line as well. E ( X ) = Z ∞ 0 xe − x dx = 1 E ( X 2 ) = 2 V ( X ) = 1 E ( Y ) = Z ∞ 0 y · ye − y dy = 2 E ( Y  2 ) = 6 V ( Y ) = 2 and E ( XY ) = Z ∞ 0 Z y 0 xdx  ye − y dy = 3 and thus ρ XY = 3 − 1 · 2 √ 1 √ 2 = 1 √ 2 and thus the least mean squares line is x + 1 , as before. 4. If X is a binomial random variable with parameters n and p and Y is a binomial random variable with parameters m and p , X and Y independent, and Z = X + Y , ﬁnd E ( X | Z ) . Solution: P ( X = j | Z = X + Y = l ) =  P ( X = j and Y = l − j ) P ( X + Y = l ) =  P ( X = j ) P ( Y = l − j ) P ( X + Y = l ) =  n j ! m l − j !  n + m l ! which is a hypergeometric distribution. The mean of this distribution is in the text (page 74) and is E ( X | Z ) = nl/ ( n + m ) . 5. Let ( X, Y ) be a bivariate random variable such that E ( X ) = 0 , E ( Y ) = 3 , V ( X ) = 3 , V ( Y ) = 5 and ρ XY = 1 / 2 . Find the mean and variance of the random variable Z = 2 X − 3 Y . Solution: In general it is easy to show (do it!) that V ( aX + bY ) = a 2 V ( X ) + b 2 V ( Y ) + 2 abρ XY σ X σ Y . Using this formula it follows that µ Z = 2 µ X − 3 µ Y = − 9 and V ( Z ) = 57 − 6 √ 15 . 6. It is known that the vertical and horizontal errors of a computer plotter, in standardized units, form a bivariate normal random variable with joint pdf f XY ( x, y ) = 3 4 π √ 8 exp {− 9 16 ( x 2 − xy 3  − 4 x 3 + y 2 4  − 2 y 3 + 4 3 ) } ©A. K. Khandani, E&CE307—Fall 2024 133 (i) Find E ( Y | X ) . (ii) For what value of the constant a will the random variables X and Z = X + aY be independent? (iii) Find the probability that | X − Y | < 1 . (Note: Sums of normal random variables are normal.) Solution: By comparing the form of the given bivariate normal density with that of the standard one, we can set up equations to be solved for the ﬁve parameters. The solutions are: µ X = 2 / 3 , µ Y = 4 / 3 , σ X = 1 , σ Y = 2 , ρ XY = 1 / 3 and these may be veriﬁed by substituting into the general form. (i) E ( Y | X ) =  2 3 X + 8 9 . (ii) The value of a such that E ( XZ ) = E ( X ) E ( Z ) is sought. By direct computation: E ( XZ ) = E ( X ) E ( Z ) = E ( X ( X + aY )) = E ( X )( E ( X ) + aE ( Y )) or V ( X ) + aρ XY σ X σ Y = 0 and from the parameters found, this implies that a = − 3 / 2 . (iii) From the parameters already found, let Z = X − Y and note that µ Z = − 2 / 3 and V ( Z ) = V ( X − Y ) = 11 / 3 Thus P ( | X − Y | < 1) = P ( | Z | < 1) = Φ( 5 √ 33 ) − Φ( − 1 √ 33 ) . 7. If ( X, Y ) is a bivariate normal random variable with pdf f XY ( x, y ) = 1 π √ 3 exp {− 2 3 ( x 2 − xy + y 2 ) } (i) Find the probability that P ( Y > 3 | X = 2) and compare this to P ( Y > 3) . (ii) Find E ( Y | X ) . Solution: (i) As with the previous problem, the ﬁve parameters of the bivariate density are found as µ X = µ Y = 0 σ X = σ Y = 1 ρ XY = 1 / 2 . Consequently the conditional density function f Y | X ( y | x ) ∼ N (  1 2 x, 3 4 . Then P ( Y > 3 | X = 2) = Z ∞ 3 e − ( y − 1) 2 / (3 / 2) √ 2 π p 3 / 4  dy = 1 − Φ( 4 √ 3 ) . Since Y ∼ N (0 , 1) , P ( Y > 3) = Z ∞ 3 e − y 2 / 2 √ 2 π dy = 1 − Φ(3) . (ii) E ( Y | X ) = X/ 2 . ©A. K. Khandani, E&CE307—Fall 2024 134 8. The time in minutes taken by person A to complete a certain task is assumed to be a random variable with an exponential pdf with parameter α 1 . The time taken by person B is independent of that for person A and has an exponential pdf with parameter α 2 i.e. f X ( x ) = α 1 e − α 1 x  , x ≥ 0 , f Y ( y ) = α 2 e − α 2 y  , y ≥ 0 (i) What is the probability it takes A longer to complete the task than B? (ii) If α 1 = α 2 what is the probability the tasks are ﬁnished within two minutes of each other? Solution: (i) Let X be the time taken by person A and Y that taken by person B. P ( X > Y ) = Z ∞ 0 Z x 0 α 2 e − α 2 y dy  α 1 e − α 1 x dx which can be computed to be P ( X > Y ) = α 2 α 1 + α 2 . (ii) It is desired to compute P ( | X − Y | < 2) . Let α 1 = α 2 = α . Consider P ( Y < X − 2) = Z ∞ 2 Z x − 2 0 αe − α dy  αe − αx dx which is readily evaluated to e − 2 α / 2 . The required probability is then P ( | X − Y | < 2) = 1 − 2( e − 2 α / 2) = 1 − e − 2 α .