{
    "1": "Combinatorial Analysis 1.1 Introduction This chapter deal with nding eective method for counting the number of way that thing can occur . In fact , many problem in probability theory can be solved simply by counting the number of dierent way that a certain event can occur . The mathematical theory of counting is formally known a combinatorial analysis . 1.2 The Basic Principle of Counting Number of way to perform two procedure in succession If we are going to perform two procedure in succession and if the rst procedure can be performed in n 1 way , and if , for each of these way , the second procedure can be performed in n 2 way , then there are n 1 n 2 way in which the two procedure can be performed successively . Number of way to perform several procedure in succession Similarly , if we are performing r proce- dures successively , the i th procedure capable of being performed in n i way regardless of the way in which the rst ( i 1 ) procedure were performed , then the r procedure can be performed in n 1 n 2 n r dierent way . 1.3 Permutations Number of way to order n distinct element Based on the reasoning mentioned earlier , if we have n dis- tinct element , there would be n ( n 1 ) ( n 2 ) 2 1 way of ordering them . We denote this quantity by n and in word by n factorial . A. K. Khandani , ECE307Fall 2024 2 Number of way to order n element ( which may not be distinct ) The number of distinct ordering of n object , n 1 of which are type 1 , n 2 of which are of type 2 , . . . , and n r of which are of type r , is equal to n n 1 , n 2 , . . . , n r = n n 1 n 2 n r , n 1 + n 2 + . . . n r = n Number of way to select r element from n element ( order is important ) The total number of way of ordering r element , chosen from n distinct element , is equal to n ( n 1 ) ( n 2 ) ( n r + 1 ) This quantity can be also expressed a n / ( n r ) . This is denoted by P n r , the number of permutation of n thing taken r at a time . 1.4 Combinations Number of way to select r element from n distinct element ( order is not important ) It is possi- ble to choose , without regard of order , r element from n distinct element in n r ( n r ) dierent way . This is an important quantity in combinatorics and will be denoted by n r . It is also called a Binomial Coecient and is sometimes written a C n r . A. K. Khandani , ECE307Fall 2024 3 Note that , n 0 = n n = 1 and n i = 0 , for i < 0 , and for i > n A useful combinatorial identity is , n r = n 1 r 1 + n 1 r , 1 r n This identity may be proved by the following combinatorial argument . Consider a group of n object and x attention on some particular one of these object ( call it object 1 ) . Now , there are n 1 r 1 group of size r that contain object 1 . ( since each such group is formed by selecting r 1 object from the n 1 object which are those remaining after excluding object 1 ) . Also , there are n 1 r group of size r that do not contain object 1 ( since each such group is formed by selecting r object from the n 1 object which are those remaining after excluding object 1 ) . Note that there is a total of n r group of size r , and a given group among these either contains or doe not contain object 1 . This mean that the total number of possible way to select r object out of n is equal to the number of way to select r object from n when object 1 is included ( total of n 1 r 1 group ) plus the number of way to select r object from n when object 1 is not included ( total of n 1 r group ) . This result in the above identity . We also have , n r = n n r A. K. Khandani , ECE307Fall 2024 4 Binomial Theorem is a follows ( X + Y ) n = n X k =0 n k X k Y n k Combinatorial Proof of the Binomial Theorem Consider the product ( X 1 + Y 1 ) ( X 2 + Y 2 ) . . . ( X n + Y n ) Its expansion consists of the sum of 2 n term , each term being the product of n factor . Furthermore , each of the 2 n term in the sum will contain a a factor either X i or Y i for each i = 1 , 2 , ... , n . For example , ( X 1 + Y 1 ) ( X 2 + Y 2 ) = X 1 X 2 + X 1 Y 2 + Y 1 X 2 + Y 1 Y 2 Now , how many of the 2 n term in the sum will have a factor k of the X i s and ( n k ) of the Y i s As each term consisting of k of the X i s and ( n k ) of the Y i s corresponds to a choice of a group of k from the n value X 1 , X 2 , ... , X n , there are n k such term . Thus , letting X i = X , Y i = Y , i = 1 , ... , n , we get the desired result . 1.5 Multinomial Coecients Multinomial Theorem is a follows ( x 1 + x 2 + + x r ) n = X n n 1 , n 2 . . . , n r x n 1 1 x n 2 2 x n r r where n 1 + n 2 + + n r = n , and n n 1 , n 2 . . . , n r = n n 1 n 2 n r and the summation is over all possible set of integer ( n 1 , n 2 , . . . , n r ) such that ( i ) 0 n i n , i = 1 , 2 , . . . , r and ( ii ) n 1 + n 2 + + n r = n . A. K. Khandani , ECE307Fall 2024 5 1.6 The Number of Integer Solutions of Equations The Number of Positive Integer Solutions of Equations There are n 1 r 1 distinct positive integer-valued vector ( x 1 , x 2 , . . . , x r ) satisfying x 1 + x 2 + + x r = n x i > 0 , i = 1 , . . . , r Proof Refer to problem 7 . The Number of Nonnegative Integer Solutions of Equations There are n + r 1 r 1 distinct nonnega- tive integer-valued vector ( x 1 , x 2 , . . . , x r ) satisfying x 1 + x 2 + + x r = n x i 0 , i = 1 , . . . , r Proof Refer to problem 8 . A. K. Khandani , ECE307Fall 2024 6 1.7 Some Solved Problems 1 . Twenty book are to be arranged on a shelf eleven on travel , ve on cooking , and four on gardening . The book in each category are to be grouped together . How many arrangement are possible Solution We have 11 arrangement for the travel , 5 arrangement for the cooking , and 4 arrangement for the gardening book . We can also permute the three dierent class of book in 3 way . Thus total = ( 11 ) ( 5 ) ( 4 ) ( 3 ) 2 . A sandwich in a vending machine cost 0.85 . In how many way can a customer put in two quarter , three dime , and a nickel Solution We know that the number of way to arrange n object , n 1 of one kind , n 2 of a second kind , . . . , and n r of an r th kind , r X i =1 n i = n , is n n 1 n 2 n r Viewed in this context , an admissible coin sequence is a permutation of n = n 1 + n 2 + n 3 = 6 object , where n 1 = number of quarter = 2 n 2 = number of dime = 3 n 3 = number of nickel = 1 It follows that the number of dierent way the coin can be put into the machine is 60 n n 1 n 2 n 3 = 6 231 = 60 3 . Twelve people belong to a club . How many way can they pick a president , vice-president , secretary , and treasurer Solution We think of lling the oces one at a time . There are 12 people we can pick for president . Having made the rst choice , there are always 11 possibility for vice-president , 10 for secretary , and 9 for treasurer . The Multiplication rule state that , if m experiment are performed in order and that , no matter what the outcome of experiment 1 , ... , k 1 are , experiment k ha n k possible outcome , then the total number of outcome is n 1 n 2 n m . So by the multiplication rule , the answer is 12 P 11 V 10 S 9 T 4 . A committee of three consisting of two men and one woman is to be chosen from six men and three woman . How many dierent committee can be chosen Solution The two men can be chosen in C 6 2 way . The one woman can be chosen in C 3 1 way . Thus the total number of dierent possible committee is ( C 6 2 ) ( C 3 1 ) = 6 2 3 1 = 6 42 3 2 = 45 5 . In a class of 70 student , how many way can the student be grouped so that there are 12 student in each of the rst ve group and 10 student in the last one A. K. Khandani , ECE307Fall 2024 7 Solution There are C 70 12 choice for the rst group . Having chosen 12 for the rst group , there remain C 58 12 choice for the second group and so on . By the multiplication principle , the total is total = 70 12 . 58 12 . 46 12 . 34 12 . 22 12 . 10 10 = 70 5812 58 4612 46 3412 34 2212 22 1012 10 010 = 70 ( 12 ) 5 10 6 . A house ha 12 room . We want to paint 4 yellow , 3 purple , and 5 red . In how many way can this be done Solution To generate all the possibility , we can rst decide the order in which the room will be painted , which can be done in 12 way , then paint the rst 4 on the list yellow , the next 3 purple , and the last 5 red . One example is 9 Y 6 Y 11 Y 1 Y 8 P 2 P 10 P 5 R 3 R 7 R 12 R 4 R Now , the rst four choice can be rearranged in 4 way without aecting the outcome , the middle three in 3 way , and the last ve in 5 way . Invoking the multiplication rule , we see that in a list of the 12 possible permutation each possible painting thus appears 435 time . Hence , the number of possible painting is 12 435 = 12 11 10 9 8 7 6 1 2 3 1 2 3 4 = 27 , 720 Another way of getting the last answer is to rst pick 4 of 12 room to be painted yellow , which can be done in C 12 4 way , and then pick 3 of the remaining 8 room to be painted purple , which can be done in C 8 3 way . ( The 5 unchosen room will be painted red . ) This is the same answer since C 12 4 C 8 3 = 12 48 8 35 = 12 435 7 . How many way can we divide n indistinguishable ball into r distinguishable group in such a way that there is at least one ball per group Solution To solve this problem we imagine a line of n white ball and r 1 piece of cardboard to place between them to indicate the boundary of the group . An example with n = 14 and r = 6 would be ooo o oo oooo ooo o Since we must pick r 1 of the n 1 space to put our cardboard divider into , there are n 1 r 1 possibility . When the group can have size 0 , the last scheme break down and we need to look at thing in a dierent way . We imagine having n white ball ( w s ) and r 1 black ball ( b s ) that will indicate the boundary between group . An example with n = 9 , r = 6 would be w w b b w w w w b w b w w b which corresponds to group of size n 1 = 2 , n 2 = 0 , n 3 = 4 , n 4 = 1 , n 5 = 2 , and n 6 = 0 . Each possible division of n indistinguishable ball into r box corresponds to one arrangement of the n white ball and r 1 black ball , which is in turn the same a the number of way of getting n head and r 1 tail in n + r 1 toss , so there are n + r 1 r 1 outcome . A. K. Khandani , ECE307Fall 2024 8 8 . Show that there are n + r 1 n distinct nonnegative integer-valued vector with component ( x 1 , x 2 , . . . , x r ) satisfying x 1 + x 2 + . . . + x r = n Solution Consider a vector consisting of n one and ( r 1 ) zero . To each permutation of this vector we correspond a solution of the above equation , namely , the solution where x 1 equal the number of one to the left of the rst zero , x 2 equal the number of one between the rst and second zero , x 3 equal the number of one between the second and third zero , and so on until x r , which equal the number of one to the right of the last zero . For instance , if n = 6 , r = 4 , then the vector ( 1,1,0,0,1,1,1,0,1 ) corresponds to the solution x 1 = 2 , x 2 = 0 , x 3 = 3 , x 4 = 1 . It is easy to see that this correspondence between permutation of a vector of n one and ( r 1 ) zero and solution of the above equation is a one-to-one correspondence . The result now follows because there are ( n + r 1 ) n ( r 1 ) permutation of a vector of n one and ( r 1 ) zero . Try to relate this problem to the previous problem . 9 . Show that n 1 + n 3 + = n 0 + n 2 + for any n . Solution Consider the expansion of ( x y ) n ( x y ) n = n X k =0 n k x k ( y ) n k Let x = y = 1 . Then x y = 0 , and the previous equation reduces to 0 = n X k =0 n k ( 1 ) n k which can be written n 0 + n 2 + = n 1 + n 3 + 10 . Prove that n 1 + 2 n 2 + + n n n = n 2 n 1 Solution This time we begin with the expansion of ( 1 + x ) n ( 1 + x ) n = n X k =0 n k x k ( 1 ) n k Dierentiating both side of the previous equation with respect to x give n ( 1 + x ) n 1 = n X k =0 n k kx k 1 Now , let x = 1 . This simplies the left-hand side of the previous equation to n 2 n 1 , while the right-hand side reduces to n X k =0 k n k = n 1 + 2 n 2 + + n n n A. K. Khandani , ECE307Fall 2024 9",
    "2": "Axioms of Probability 2.1 Introduction In this chapter we introduce the concept of the probability of an event and then show how these probability can be computed in certain situation . This is based on dening the concept of the sample space . Consider an experiment whose outcome is not predictable with certainty . However , although the outcome of the experiment will not be known in advance , let u suppose that the set of all possible outcome is known . This set of all possible outcome of an experiment is known a the sample space of the experiment . 2.2 Sample Space and Events Experiment An experiment is a procedure which result in an outcome . In general , the outcome of an experiment depends on the condition under which the experiment is performed . Deterministic Experiment In a deterministic experiment , the observed result ( outcome ) is not subject to chance . This mean that if we repeat a deterministic experiment under exactly the same condition , we will get the same result . Random Experiment In a random experiment , the outcome is subject to chance . This mean that if the experiment is repeated , the outcome may be dierent from time to time . Statistical Regularity Suppose that a random experiment is repeated for a large number of time . We say that the experiment ha statistical regularity if the fraction of time that a particular event E is observed tends to some limit . Note In this course , we consider only experiment which are repeatable and the outcome of the experiment exhibit statistical regularity . Sample Space For any random experiment , we dene the sample space S to be the set of all possible outcome of the experiment . Elementary event An elementary event E is a single element of S corresponding to a particular outcome of the experiment . Event An event is any subset of S ( a collection of elementary event or outcome ) . A. K. Khandani , ECE307Fall 2024 10 Union of Some Events The union of event E 1 , E 2 , . . . E n is dened the event for which at least one of E i s occurs and denoted by n i =1 E i . Intersection of Some Events The intersection of event E 1 , E 2 , . . . E n is dened the event for which all of E i s occur and denoted by n i =1 E i . Complement of An Event Consider an even E . The complement of E ( denoted a E c ) is the event that E doe not occur . We have , EE c = , E E c = S . S E E c Figure 1 Complement of an event E . A. K. Khandani , ECE307Fall 2024 11 Partitioning of Sample Space The event F 1 , F 2 , . . . , F n form a partition of S if they are disjoint and their union is S . This mean that , ( i ) n i =1 E i = S ( ii ) E i E j = , i = j S F 2 F 3 F n . . . F 1 Figure 2 Partition of the sample space . If F 1 , F 2 , . . . , F n form a partition of the sample space , then , for any event E S , we have , E = n i =1 ( EF i ) , where ( EF i ) ( EF j ) = , for i = j ( 1 ) S F 2 F n F 1 S F 2 F n F 1 EF 1 . . . E F 3 EF n Figure 3 E = n i =1 ( EF i ) . A. K. Khandani , ECE307Fall 2024 12 Some Properties of Union and Intersection Commutative law E F = F E EF = FE Associative law ( E F ) G = E ( F G ) ( EF ) G = E ( FG ) Distributive law ( E F ) G = EG FG ( EF ) G = ( E G ) ( F G ) Note that , E = , ES = E , E = E , E S = S DeMorgans law are a follows n i =1 E i c = n i =1 E c i n i =1 E i c = n i =1 E c i Note that F and F c parition the space , i.e. , FF c = and F F c = S , a a result , for any given E we have , E = EF c EF , where ( EF c ) ( EF ) = This is illustrated in Fig . 4 . S E F EF E c F EF c Figure 4 The intersection of event E and F showing E = EF c EF , where ( EF c ) ( EF ) = . A. K. Khandani , ECE307Fall 2024 13 2.3 Axioms of Probability Relative Frequency Consider a random experiment with a sample space S and let E be a particular outcome of E . This mean that the event ( outcome ) E is an element of the set S , i.e. , E S . Assume that the experiment is repeated for n time and the total number of time that the event E ha occurred is equal to n ( E ) . The ratio n ( E ) /n is called the relative frequency of the event E . Relative frequency of an event ha the property that 0 n ( E ) /n 1 , where n ( E ) /n = 0 if E occurs in none of the n trial and n ( E ) /n = 1 if E occurs in all of the n trial . Probability The probability of the event E is dened a , P ( E ) = lim n n ( E ) n ( 2 ) The probability P ( E ) of an event E satises the following axiom ( i ) 0 P ( E ) 1 . ( ii ) P ( S ) = 1 . ( iii ) If E 1 , E 2 , . . . are disjoint event , i.e. , E i E j = when i = j , then P i =1 E i = X i =1 P ( E i ) As a result of above axiom P ( ) = 0 A. K. Khandani , ECE307Fall 2024 14 2.4 Some Simple Propositions The probability P ( E ) of an event E satises the following property ( i ) P ( E c ) = 1 P ( E ) . ( ii ) If E F , then P ( E ) P ( F ) . ( iii ) P ( E F ) = P ( E ) + P ( F ) P ( EF ) . ( iv ) P ( E 1 E 2 E n ) = n X i =1 P ( E i ) X i 1 < i 2 P ( E i 1 E i 2 ) + + ( 1 ) r +1 X i 1 < i 2 < i r P ( E i 1 E i 2 E i r ) + + ( 1 ) n +1 P ( E 1 E 2 E n ) where the sum ( 1 ) r +1 P i 1 < i 2 < i r P ( E i 1 E i 2 E i r ) is over n r term . 2.5 Sample Spaces Having Equally Likely Outcomes For a Sample Space having N equally likely outcome , P ( 1 ) = P ( 2 ) = = P ( N ) = 1 N Probability of any event E is P ( E ) = number of point in E number of point in S A. K. Khandani , ECE307Fall 2024 15 2.6 Some Solved Problems 1 . Let A , B and C be three arbitrary set in a sample space S . In term of these set nd set-theoretic expression for the following event in term of A , B and C ( i ) Only A occurs , ( ii ) Both A and B but not C occur ( iii ) All three event occur ( iv ) At least one event occurs ( v ) At least two event occur ( vi ) Exactly one event occurs ( vii ) Exactly two event occur ( viii ) No event occurs ( ix ) Not more than two of the event occur . Solution ( i ) AB c C c ( ii ) ABC c ( iii ) ABC ( iv ) A B C ( v ) ( AB ) ( AC ) ( BC ) ( vi ) ( AB c C c ) ( BA c C c ) ( CA c B c ) ( vii ) ( ABC c ) ( AB c C ) ( A c BC ) ( viii ) A c B c C c ( ix ) ( ABC ) c = A c B c C c 2 . A three digit number ( 000 to 999 ) is chosen at random . Find the probability that exactly one of the digit in the number is greater than 5 . Find the probability that at least one of the digit in the number is greater than 5 . Solution One of the digit must be 6 , 7 , 8 or 9 and the other two must be 0 to 5 , inclusive . There are three way to choose the digit that will be greater than 5 and 4 way to ll the position . There are 6 6 way of lling the remaining two position . The total number of point satisfying the criterion is 3 4 6 6 = 432 point and the probability is . 432 . 3 . A coin is tossed until for the rst time the same result appears twice in succession . To every possible outcome requiring n toss , attribute a probability of 1 / 2 n 1 . Describe the sample space of this experiment . Find the probability of the event ( i ) The experiment stop before the 6 toss . ( ii ) An even number of toss is required . Solution Sample space probability HH TT 1/4 HTT THH 1/8 HTHH THTT 1/16 ( i ) Probability experiment end before the sixth toss is 2 ( 1 / 4 + 1 / 8 + 1 / 16 + 1 / 32 ) = 15 / 16 . ( ii ) Probability experiment end on an even number of toss 2 ( 1 4 + 1 16 + ) = 2 1 / 4 1 1 / 4 = 2 / 3 4 . A box contains ve tag marked with the integer 1 to 5 . Three tag are drawn from the box without replacement . Describe the sample space and the following event ( i ) E 1 is the sum of the tag number is 12 ( ii ) E 2 corresponds to at least one tag of the tag being a 5 ( iii ) E 1 E 2 ( iv ) Repeat the question if the experiment is with replacement . Solution The sample space consists of 3-tuples of distinct integer from 1 to 5 and there are 5 4 3 such point . ( i ) E 1 is the set of 3-tuples whose integer add to 12 - there are three such point ( 3,4,5 ) and it permutation . ( ii ) E 2 is the set of point containing a 5 . For a 5 in the rst position , there are 4 3 way of lling the other two position . As there are 3 way of placing the 5 , the total number of point in E 2 is 3 4 3 = 36 . A. K. Khandani , ECE307Fall 2024 16 ( iii ) E 1 E 2 = E 1 . ( iv ) The sample space now contains 5 3 point . E 1 consists of the point ( 5,5,2 ) and it permutation ( 3 point ) , ( 5,4,3 ) and it permutation ( 6 point ) and ( 4,4,4 ) . E 2 consists of point of the form ( 5,5,5 ) , ( 5,5 , X ) where X < 5 ( 12 point ) and ( 5 , X , Y ) , both X and Y < 5 ( 48 point - count them ) . E 1 E 2 consists of the point adding to 12 with at least one 5 namely the point ( 5,5,2 ) and it permutation ( 3 point ) and ( 5,4,3 ) and it permutation ( 6 point ) . 5 . An experiment consists of choosing two people and determining which day of the week their birthday fall on in the current year . Describe a sample space for this experiment and describe the following event ( i ) E 1 the birthday fall on the same day ( ii ) E 2 at least one birthday is on a Friday . Solution Without loss of generality assume there is an order in the experiment i.e . the two people are questioned sequentially . The the sample space consists of the 49 point S = ( x , y ) , x , y = Sunday , Monday , , Saturday where the rst coordinate refers to the rst person and the second , the second . Then E 1 = ( x , x ) , E 1 = 7 and E 2 = ( Friday , y ) , ( x , Friday ) , E 2 = 13 . 6 . A die is tossed three time . Describe a suitable sample space for this experiment . Describe the following event and determine the number of point ( i ) A = exactly two six were obtained ( ii ) B = the sum of the face showing is 12 ( iii ) C = all face show an even number Describe also the event AB , AC , ABC . Solution The sample space might consist of the 6 3 point ( i , j , k ) , 1 i , j , k 6 where the i th coordinate report the outcome on the i th toss . ( i ) A = 2 six and contains point of the form ( 6 , 6 , 1 ) , ( 6 , 6 , 2 ) , ( 1 , 6 , 6 ) , ( 5 , 6 , 6 ) Clearly A = 15 . ( ii ) B = sum of face is 12 and contains point such a ( 1 , 6 , 5 ) , ( 1 , 5 , 6 ) , ( 2 , 6 , 4 ) , ( 2 , 4 , 6 ) , and contains , by direct counting , 25 point . ( iii ) C = all face even and contains 3 3 point such a ( 2 , 2 , 2 ) , ( 2 , 2 , 4 ) , . By direct verication , AB = , AC contains six point and ABC = . 7 . Ten people enter an elevator on the ground oor and get o , at random , at one of eight oors above ground . Describe a suitable sample space and nd the probability that exactly one person get oon the fth oor . Solution An appropriate sample space for this situation is that of dropping ten ball ( people ) into eight cell ( oors ) which consists of 8 10 point i.e . ten-tuples with each coordinate place containing an integer from 1 to 8 . Each point is equally likely and the probability that exactly one person get oon the fth oor is the number of point containing one 5 divided by 8 10 . The number of such point is 10 7 9 since the coordinate position containing the 5 can be one of 10 , and the other coordinate position can contain any of 7 integer ( anything but a 5 ) . A. K. Khandani , ECE307Fall 2024 17 8 . You are dealt two card from a standard deck of 52 card . What is the probability you will obtain a pair ( both card have the same number or picture on them ) What is the probability the two card will have the same suit ( It is assumed the rst card is NOT replaced into the deck before the second card is dealt . ) Solution There are 52 2 distinct pair of card possible ( no order ) . Within a given number ( or picture ) card there are 4 2 way of obtaining a pair . There are 13 such number and the total number of way of obtaining a pair is 13 4 2 . The probability is then P ( pair ) = 13 4 2 52 2 . By a similar reasoning the probability of obtaining a pair with the same suit is P ( pair with the same suit ) = 4 13 2 52 2 . 9 . Choose a positive integer at random and cube it . What is the probability the last two digit of the cube are both one Solution Consider the base 10 expansion of an integer x + y 10 + z 10 2 + ( 0 x , y , z 9 ) , and cube it to give x 3 + 3 x 2 y 10 + 3 xy 2 10 2 + The only digit of the original number is x and the only digit , when cubed give a 1 in the least signicant position is 1 . Thus x = 1 . Similarly for the second least signicant digit to be a 1 we must the least signicant digit of 3 x 2 be a 1 and the only value for which this happens , give x = 1 is y = 7 . Thus only cube of number ending in 71 give number ending in 11 . Thus the probability of the cube of a randomly chosen integer ending in 11 is approximately . 01 ( approximately because we could choose single digit number etc . 10 . Each of nine ball are placed with equal probability in one of three box . Find the probability that ( a ) there will be three ball in each box .. ( b ) There will be four ball in one box , three in another and two in the other . Solution ( a ) The sample space contains ordered 9 -tuples , each position lled with one of three integer - total number of point is 3 9 . The number of these point containing 3 one , 3 two and 3 three is 9 3 6 3 3 3 and the probability follows . ( b ) Similarly P ( 4 in one box , 3 in another and 2 in the other ) = 9 4 5 3 2 2 3 9 . A. K. Khandani , ECE307Fall 2024 18 11 . In a batch of manufactured unit , 2 of the unit have the wrong weight ( and perhaps also the wrong color ) , 5 have the wrong color ( and perhaps also the wrong weight ) , and 1 have both the wrong weight and the wrong color . A unit is taken at random from the batch . What is the probability that the unit is defective in at least one of the two respect Solution Let A be the event that the unit ha the wrong weight and B the event that the unit ha the wrong color . The model that we have chosen then show that P ( A ) = 0 . 02 P ( B ) = 0 . 05 P ( AB ) = 0 . 01 . Using the Addition Theorem for Two Events which state that P ( A B ) = P ( A ) + P ( B ) P ( AB ) we conclude that the desired probability is given by P ( A B ) = P ( A ) + P ( B ) P ( AB ) = 0 . 02 + 0 . 05 0 . 01 = 0 . 06 . 12 . Let u select one of the ve-digit number 00000 , 00001 , ... , 99999 at random . What is the probability that the number only contains the digit 0 and 1 Solution We let each possible number correspond to one outcome . The number of possible case is then 10 5 . The number of favorable case is seen to be 2 5 , for there are two favorable digit in each of the ve position . Using the First Denition of Probability which state that if the probability of the outcome in a given experiment are the same , then the probability of an event is the ratio of the number of favorable case to the number of possible case , we conclude that the desired probability is 2 5 / 10 5 =0.00032 . Now let u compute the probability that the digit in the random number are all dierent . The number of favorable case is now 10 9 8 7 6 = 30 , 240 ( any of the 10 digit can come rst , then 1 of 9 digit can come second , and so on ) . Hence the probability is 30 , 240 / 10 5 = 0 . 3024 . 13 . Consider the event of rolling two dice . Determine the probability that at least one 4 appears on the dice . Solution The sample space is a follows ( 1 , 1 ) ( 2 , 1 ) ( 3 , 1 ) ( 4 , 1 ) ( 5 , 1 ) ( 6 , 1 ) ( 1 , 2 ) ( 2 , 2 ) ( 3 , 2 ) ( 4 , 2 ) ( 5 , 2 ) ( 6 , 2 ) ( 1 , 3 ) ( 2 , 3 ) ( 3 , 3 ) ( 4 , 3 ) ( 5 , 3 ) ( 6 , 3 ) ( 1 , 4 ) ( 2 , 4 ) ( 3 , 4 ) ( 4 , 4 ) ( 5 , 4 ) ( 6 , 4 ) ( 1 , 5 ) ( 2 , 5 ) ( 3 , 5 ) ( 4 , 5 ) ( 5 , 5 ) ( 6 , 5 ) ( 1 , 6 ) ( 2 , 6 ) ( 3 , 6 ) ( 4 , 6 ) ( 5 , 6 ) ( 6 , 6 ) There 36 element in the sample space . The number of element of sample space having at least one four is equal to 11 . The corresponding probability is 11 / 36 . Note that we could also solve this problem using the expression for the probability of the union of two event . 14 . Consider the event of rolling two dice . To visualize the set of outcome , it is useful to use the following table ( 1 , 1 ) ( 2 , 1 ) ( 3 , 1 ) ( 4 , 1 ) ( 5 , 1 ) ( 6 , 1 ) ( 1 , 2 ) ( 2 , 2 ) ( 3 , 2 ) ( 4 , 2 ) ( 5 , 2 ) ( 6 , 2 ) ( 1 , 3 ) ( 2 , 3 ) ( 3 , 3 ) ( 4 , 3 ) ( 5 , 3 ) ( 6 , 3 ) ( 1 , 4 ) ( 2 , 4 ) ( 3 , 4 ) ( 4 , 4 ) ( 5 , 4 ) ( 6 , 4 ) ( 1 , 5 ) ( 2 , 5 ) ( 3 , 5 ) ( 4 , 5 ) ( 5 , 5 ) ( 6 , 5 ) ( 1 , 6 ) ( 2 , 6 ) ( 3 , 6 ) ( 4 , 6 ) ( 5 , 6 ) ( 6 , 6 ) Compute the probability of the following event A. K. Khandani , ECE307Fall 2024 19 A = sum = 7 , B = 8 < sum 11 and C = 10 < sum . Solution The sample space is a shown for the previous problem . The number of element of the sample space resulting in event A sum = 7 is equal to A = 6 where A show the cardinality ( number of element ) of the set A . This result in P ( A ) = 6 / 36 = 1 / 6 . For event B 8 < sum 11 , we have , B = 9 = P ( B ) = 9 / 36 = 1 / 4 and for event C 10 < sum , we have , C = 3 = P ( C ) = 3 / 36 = 1 / 12 . 15 . Given two event A and B , belonging to the same sample space , with P ( A ) = 0 . 4 and P ( B ) = 0 . 7 . What are the maximum and minimum possible value for P ( AB ) Solution The maximum value of P ( AB ) is equal to P ( A ) = 0 . 4 and happens if A B . The minimum value of P ( AB ) is 0 . 1 and happens if A B = S . 16 . A and B have a and b dollar , respectively . They repeatedly toss a fair coin . If a head appears , A get 1 dollar from B if a tail appears , B get 1 dollar from A . The game go on until one of the player is ruined . Determine the probability that A is ruined . Solution The solution is easier to nd if we consider a more general problem . Suppose that , at a given moment , A ha n dollar , and hence B ha a + b n dollar . Let P n be the probability that , given this scenario , A is eventually ruined . We notice that after one tossing , A will have either n + 1 , or n 1 dollar each with probability 1 / 2 . Dene the following event , W A ha currently n dollar , A win in the next tossing , and A is nally ruined . L A ha currently n dollar , A loss in the next tossing , and A is nally ruined . We have , P ( W ) = P n +1 / 2 P ( L ) = P n 1 / 2 P ( A is ruined ) = P ( W L ) = P ( W ) + P ( L ) Note that W and L are disjoint . This result in , P n = 1 2 P n +1 + 1 2 P n 1 . Both of the characteristic root of this recursive equation are equal to one . From this observation we conclude that P n = C 1 + C 2 n The constant C 1 and C 2 are determined by noting that P a + b = 0 ( for if A possesses all the money , he can not be ruined ) and P 0 = 1 ( for if A ha no money , he is ruined from the beginning ) . Using these condition , a simple calculation show that P n = 1 n a + b By taking n = a , we nd that the probability is b/ ( a + b ) that A is ruined . In the same way , it is found that the probability is a/ ( a + b ) that B is ruined . Suppose that A start the game with a = 10 dollar and B with b = 100 dollar . The probability of ruin for A is then 100 / 110 = 0 . 91 . Conclusion Do not gamble with people who are richer than you 17 . In a standard deck of 52 card there are 13 spade . Two card are drawn at random . Determine the probability that both are spade . A. K. Khandani , ECE307Fall 2024 20 Solution The number of favorable case is equal to the number of combination of 2 element from among 13 , and the number of possible case is equal to the number of combination of 2 element from among 52 . That is , we get P ( A ) = 13 2 , 52 2 = 1 / 17 18 . Suppose we roll 6 dice . What is the probability of A = We get exactly two 4 Solution One way that A can occur is x 1 4 2 x 3 4 4 x 5 x 6 where x stand for not a 4 . Since the six event die one show x , die two show 4 , . . . , die six show x are independent , the indicated pattern ha probability 5 6 1 6 5 6 1 6 5 6 5 6 = 1 6 2 5 6 4 Here we have been careful to say pattern rather than outcome since the given pattern corresponds to 5 4 outcome in the sample space of 6 6 possible outcome for 6 dice . Each pattern that result in A corresponds to a choice of 2 of the 6 trial on which a 4 will occur , so the number of pattern is C 6 2 . When we write out the probability of each pattern there will be two 1/6s and four 5/6s so each pattern ha the same probability and P ( A ) = 6 2 1 6 2 5 6 4 19 . In a bridge hand , what is the probability each player will be dealt exactly one ace Solution Unlike the computation required for poker hand , here we need to consider simultaneously four 13-card hand . To visualize the basic counting technique , think of the 52 card laid out in a row . Under 13 of the card imagine an N under 13 others , an S under 13 others , an E and under the remaining 13 , a W . That permutation of N s , S s , E s , and W s would determine the hand received by North , South , East , and West , respectively . There are 52 way to permute the 52 card and there are 13 way to permute each of the hand . Clearly , the total number of way to deal the four hand will equal to , 52 13131313 By a similar argument , the ace can be distributed , one to a player , in 4/1111 way , and for each of those distribution , the remaining 48 card can be dealt in 48/12121212 way . The Fundamental Principle state that if operation A can be performed in m dierent way and operation B in n dierent way , the sequence ( operation A , operation B ) can be performed in m n dierent way . Thus , by the Fundamental Principle , the probability of each player receiving exactly one ace is 4 ( 1 ) 4 48 ( 12 ) 4 52 ( 13 ) 4 = 4 ( 13 ) 4 48 52 or 0.105 . A. K. Khandani , ECE307Fall 2024 21 20 . In the game of bridge the entire deck of 52 card is dealt out to 4 player . What is the probability that one of the player receives 13 spade Solution There are 52 13 , 13 , 13 , 13 possible division of the card among the four distinct player . As there are 39 13 , 13 , 13 possible division of the card leading to a xed player having all 13 spade , it follows that the desired probability is given by 4 39 13 , 13 , 13 52 13 , 13 , 13 , 13 6 . 3 10 12 One could also solve this problem by taking into account that 13 card can be selected in 52 13 way and one of these selection corresponds to 13 spade , then the desired probability is equal to , 4 52 13 where the factor 4 account for the fact that there are four player each having the same probability of receiving the 13 spade . 21 . If 2 ball are randomly drawn from a bowl containing 6 white and 5 black ball , what is the probability that one of the drawn ball is white and the other is black Solution If we regard the order in which the ball are selected a being signicant , then the sample space consists of 11 10 = 110 point . Furthermore , there are 6 5 = 30 way in which the rst ball selected is white and the second black . Similarly , there are 5 6 = 30 way in which the rst ball is black and the second white . Hence , assuming that randomly drawn mean that each of the 110 point in the sample space are equally likely to occur , we see that the desired probability is 30 + 30 110 = 6 11 This problem could also have been solved by regarding the outcome of the experiment a the ( unordered ) set of drawn ball . From this point of view , there would be 11 2 =55 point in the sample space . Using this second representation of the experiment , we see that the desired probability is 6 1 5 1 11 2 = 6 11 which agrees with the earlier answer . 22 . How many integer between 100 and 999 have distinct digit , and how many of those are odd Solution Think of the integer a being an arrangement of a hundred digit , a ten digit , and a unit digit . The hundred digit can be lled in any of 9 way ( 0 are inadmissible ) , the ten place in any of 9 way A. K. Khandani , ECE307Fall 2024 22 ( anything but what appears in the hundred place ) , and the unit place in any of 8 way ( the rst two digit must not be repeated ) . Thus , the number of integer between 100 and 999 with distinct digit is 9 9 8 , or 648 . ( 9 ) 100 ( 9 ) 10 ( 8 ) 1 To compute the number of odd integer with distinct digit , we rst consider the unit place , where any of 5 integer can be positioned ( 1,3,5,7 , or 9 ) . Then , turning to the hundred place , we have 8 choice ( the 0 is inadmissible so is whatever appeared in the unit place ) . The same number of choice is available for the ten place . Multiplying these number together give 8 8 5 = 320 a the number of odd integer in the range 100-999 . 23 . In a deck of card there are 52 card consisting of 4 suit with 13 denomination in each . A poker deal contains 5 randomly selected card . A full house mean that the player receives 3 card of one denomination and 2 card of another denomination three-of-a-kind mean that he get 3 card of one denomination , 1 of another denomination and 1 of a third denomination . Determine the probability of a full house and the probability of a three-of-a-kind . Solution ( a ) Full House The number of possible poker deal is 52 5 . The favorable case are found a follows . The denomination for the 3-cards and the 2-cards can be selected in 13 12 way . There are 4 3 way of selecting 3 card from 4 card with the same denomination analogously , there are 4 2 way of taking out 2 card . Hence the number of favorable case is 13 12 4 3 4 2 , and the probability we want becomes 13 12 4 3 4 2 , 52 5 1 694 ( b ) Three-of-a-kind The denomination for the 3-cards , the 1-card and the 1-card can be chosen in 13 12 2 way . Hence we nd , in about the same way a in ( a ) , the probability 13 12 2 4 3 4 1 4 1 , 52 5 1 47 24 . Suppose we select two marble at random from a bag containing four white and six red marble . What is the probability that we select one of each color Solution There are C 10 2 outcome in the sample space . We can choose one white marble in C 4 1 way and one red marble in C 6 1 ways.Thus the desired probability is P ( one white , one red ) = 4 1 6 1 10 2 = 24 45 Alternatively , we may call the event one white , one red the event ( w , r ) ( r , w ) and since these are disjoint event , the probability is ( 4 / 10 ) ( 6 / 9 ) + ( 6 / 10 ) ( 4 / 9 ) = 24 / 45 A. K. Khandani , ECE307Fall 2024 23 25 . A random experiment consists of selecting at random an integer between 100 and 499 . What is the probability of the following event ( a ) the selected integer contains at least one 1 , and ( b ) the selected integer contains exactly two 2 Solution The number of possible outcome is 400 . ( a ) Let E be the desired event . It is more simple to compute P ( E ) rst . For E to happen the rst digit can be 2 , 3 , or 4 , and the other two digit can be any of 2 , 3 , . . . , 9 . Thus P ( E ) = 1 P ( E ) = 1 3 . 9 . 9 400 = . 6075 ( b ) For this event we can have the following case x 2 2 , 2 x 2 , 2 2 x . In the rst case x can be 1 , 3 , or 4 , and in the two latter case x can be any digit except 2.Thus P ( exactly two 2 ) = 3 + 9 + 9 400 = . 0525 26 . Suppose we pick 4 ball out of an urn with 12 red ball and 8 black ball . What is the probability of B = We get two ball of each color Solution There are 20 4 = 20 19 18 17 1 2 3 4 = 5 19 3 17 = 4 , 845 way of picking 4 ball out of the 20 . To count the number of outcome in B , we note that there are 12 2 way to choose the red ball and 8 2 way to choose the black ball , so the multiplication rule implies B = 12 2 8 2 = 12 11 1 2 8 7 1 2 = 6 11 4 7 = 1 , 848 It follows that P ( B ) = 1848/4845=0.3814 . 27 . In a game of bridge , each of the four player get 13 card . If North and South have 8 spade between them , what is the probability that East ha 3 spade and West ha 2 Solution We can imagine that rst North and South take their 26 card and then East draw his 13 card from the 26 that remain . Since there are 5 spade and 21 nonspades , the probability he receives 3 spade and 10 nonspades is 5 3 21 10 / 26 13 . To compute the last probability it is useful to observe that 5 3 21 10 26 13 = 5 32 21 1011 26 1313 = 13 310 13 211 26 521 = 13 3 13 2 26 5 To arrive at the answer on the right-hand side directly , think of 26 blank , the rst thirteen being Easts card , the second thirteen being Wests . We have to pick 5 blank for spade , which can be done in 26 5 way , while the number of way of giving East 3 spade and West 2 spade is 13 3 13 2 . After some cancellation the right-hand side is 13 2 11 13 6 26 5 2 23 22 = 22 , 308 65 , 708 = 0 . 3395 A. K. Khandani , ECE307Fall 2024 24 Multiplying the last answer by 2 , we see that with probability 0.6782 the ve outstanding spade will be divided 3-2 , that is , one opponent will have 3 and the other 2 . Similar computation show that the probability of 4-1 and 5-0 split are 0.2827 and 0.0391 . 28 . In an urn there are N slip of paper marked 1 , 2 , .. , N . Slips are drawn at random , one at a time , until the urn is empty . If slip no . i is obtained at the i th drawing , we say that a rencontre ha occurred . Find the probability of at least one rencontre . This problem is called the problem of rencontre or the matching problem . Solution If A i = rencontre at the i th drawing , we can write the required probability P a P = P ( N 1 A i ) The Addition Theorem for Three Events state that P ( A B C ) = P ( A ) + P ( B ) + P ( C ) P ( AB ) P ( AC ) P ( BC ) + P ( ABC ) By generalizing this theorem to N event , we have the general addition formula P ( N 1 A i ) = X i P ( A i ) X i < j P ( A i A j ) + X i < j < k P ( A i A j A k ) + . . . + ( 1 ) N 1 P ( N 1 A i ) In the rst sum there are N term , in the second sum N 2 term , and so on . Consider a general term P ( A i 1 A i 2 ... A i r ) , which express the probability of rencontre in drawing i 1 , i 2 , ... , i r . Let u compute the probability . The total number of possible case for the N drawing is N . Favorable case are where the slip i 1 , i 2 , ... , i r appear in the drawing with these number , while the remaining slip can appear in any order in the other drawing this give ( N r ) possibility . Hence we have P ( A i 1 A i 2 ... A i r ) = ( N r ) /N If this is inserted into the expression given before , we nd P ( N 1 A i ) = N ( N 1 ) N N 2 ( N 2 ) N + + ( 1 ) N 1 N N 1 N or , after a reduction , P = 1 1 2 + 1 3 + ( 1 ) N 1 1 N For large N this is approximately equal to 1 e 1 = 0.63 . The problem of rencontre wa rst discussed by Montmort at the beginning of the eighteenth century . 29 . A carnival operator want to set up a ring-toss game . Players will throw a ring of diameter d on to a grid of square , the side of each square being of length s ( see the following gure ) . If the ring land entirely inside a square , the player win a prize . To ensure a prot , the operator must keep the player chance of winning down to something less than one in ve . How small can the operator make the ratio d/s Solution First , it will be assumed that the player is required to stand far enough away so that no skill is involved and the ring is falling at random on the grid . We see that in order for the ring not to touch any side of the square , the ring center must be somewhere in the interior of a smaller square , each side of which A. K. Khandani , ECE307Fall 2024 25 s s d d/2 s s is a distance d/ 2 from one of the grid line . Since the area of a grid square is s 2 and the area of an interior square is ( s d ) 2 , the probability of a winning toss can be written a a simple ratio P ( ring touch no line ) = ( s d ) 2 s 2 But the operator requires that ( s d ) 2 s 2 0 . 20 Solving for d/s give d s 1 0 . 20 = 0 . 55 That is , if the diameter of the ring is at least 55 a long a the side of one of the square , the player will have no more than a 20 chance of winning . 30 . Two friend agree to meet sometime around 1230 . But neither of them is particularly punctual - or patient . What will actually happen is that each will arrive at random sometime in the interval from 1200 to 100 . If one arrives and the other is not there , the rst person will wait 15 min or until 100 , whichever come rst , and then leave . What is the probability the two will get together Solution To simplify notation , we can represent the time period from 1200 to 100 a the interval from 0 to 60 min . Then if x and y denote the two arrival time , the sample space is the 60 60 square shown in the gure below . Furthermore , the event M , the two friend meet , will occur if and only if x y 15 or , equivalently , if and only if 15 x y 15 . These inequality appear a the shaded region in the gure below . Notice that the area of the two triangle above and below M are each equal to 1 2 ( 45 ) ( 45 ) . It follows that A. K. Khandani , ECE307Fall 2024 26 M y x ( 45,60 ) ( 60,45 ) 60 0 60 ( 15,0 ) ( 0,15 ) x-y=-15 x-y=15 the two friend have a 44 chance of meeting P ( M ) = area of M area of S = ( 60 ) 2 2 1 2 ( 45 ) ( 45 ) ( 60 ) 2 0 . 44 A. K. Khandani , ECE307Fall 2024 27",
    "3": "Conditional Probability and Independence 3.1 Introduction In this chapter we introduce one of the most important concept in probability theory , that of conditional probabil- ity . The importance of this concept is twofold . In the rst place , we are often interested in calculating probability when some partial information concerning the result of the experiment is available in such a situation the desired probability are conditional . Second , even when no partial information is available , conditional probability can often be used to compute the desired probability more easily . 3.2 Conditional Probabilities Conditional Probability Consider two event E and F which are somehow interrelated ( are dependent on each other ) . The conditional probability of E given F is dened a , P ( E F ) = P ( EF ) P ( F ) , if P ( F ) = 0 . If P ( F ) = 0 , then the conditional probability is not dened . The multiplication rule P ( E 1 E 2 E 3 E n ) = P ( E 1 ) P ( E 2 E 1 ) P ( E 3 E 1 E 2 ) P ( E n E 1 E 2 . . . E n 1 ) A. K. Khandani , ECE307Fall 2024 28 3.3 Bayes formula For any two event E and F , P ( E ) = P ( EF ) + P ( EF c ) = P ( E F ) P ( F ) + P ( E F c ) P ( F c ) = P ( E F ) P ( F ) + P ( E F c ) 1 P ( F ) The Odds Ratio of an event A is dened by P ( A ) P ( A c ) = P ( A ) 1 P ( A ) Consider a hypothesis H that is true with probability of P ( H ) and suppose that new evidence E is introduced . Then the conditional probability , given the evidence E , that H is true and that H is not true are given by P ( H E ) = P ( E H ) P ( H ) P ( E ) P ( H c E ) = P ( E H c ) P ( H c ) P ( E ) Therefore , the new odds ratio after the evidence E ha been introduced a P ( H E ) P ( H c E ) = P ( H ) P ( H c ) P ( E H ) P ( E H c ) That is , the new value of odds ratio of H is it old value multiplied by the ratio of the conditional probability of the new evidence given that H is true to the conditional probability given that H is not true . A. K. Khandani , ECE307Fall 2024 29 Bayes formula Suppose that F 1 , F 2 , . . . , F n are mutually exclusive event such that n i =1 F i = S , then P ( F j E ) = P ( EF j ) P ( E ) = P ( E F j ) P ( F j ) n X i =1 P ( E F i ) P ( F i ) 3.4 Independent Events Two Independent Events Two event E and F are independent if P ( EF ) = P ( E ) P ( F ) then P ( E F ) = P ( E ) and P ( F E ) = P ( F ) . If E and F are independent , then so are E and F c , a well a E c and F and also E c and F c . A. K. Khandani , ECE307Fall 2024 30 Three Independent Events The three event E , F and G are independent if P ( EFG ) = P ( E ) P ( F ) P ( G ) P ( EF ) = P ( E ) P ( F ) P ( EG ) = P ( E ) P ( G ) P ( FG ) = P ( F ) P ( G ) Independent Events Similarly , the event E i , i = 1 , 2 , . . . , n , are called independent if and only if for any collection of r distinct index ( 1 , 2 , . . . , r ) chosen from the set 1 , 2 , . . . , n , we have , P ( E 1 E 2 E r ) = P ( E 1 ) P ( E 2 ) P ( E r ) 3.5 P ( F ) Is a Probability Conditional Probabilities satisfy all of the property of ordinary probability . That is ( a ) 0 P ( E F ) 1 . ( b ) P ( S F ) = 1 . ( c ) If E 1 , E 2 , . . . are disjoint event , then P i E i F = X i P ( E i F ) A. K. Khandani , ECE307Fall 2024 31 3.6 Some Solved Problems 1 . Consider a die with 1 painted on three side , 2 painted on two side , and 3 painted on one side . If we roll this die ten time what is the probability we get ve 1 , three 2 and two 3 Solution The answer is 10 532 1 2 5 1 3 3 1 6 2 If we have a group of n object to be divided into m group of size n 1 , ... , n m with n 1 + + n m = n this can be done in n n 1 n 2 n m way The rst factor , in the answer above , give the number of way to pick ve roll for 1 , three roll for 2 , and two roll for 3 . The second factor give the probability of any outcome with ve 1 , three 2 , and two 3 . Generalizing from this example , we see that if we have k possible outcome for our experiment with probability p 1 , ... , p k then the probability of getting exactly n i outcome of type i in n = n 1 + ... + n k trial is n n 1 n k p n 1 1 p n k k since the rst factor give the number of outcome and the second the probability of each one . We referred to this a the Multinomial Distribution . 2 . Suppose an urn contains r = 1 red chip and w = 1 white chip . One is drawn at random . If the chip selected is red , that chip along with k = 2 additional red chip are put back into the urn . If it is white , the chip is simply returned to the urn , then a second chip is drawn . What is the probability that both selection are red Solution If we let R 1 be the event red chip is selected on rst draw and R 2 be red chip is selected on second draw , it should be clear that P ( R 1 ) = 1 2 and P ( R 2 R 1 ) = 3 4 We also know that P ( AB ) = P ( A B ) P ( B ) Hence , substituting the probability we found into the above equation give P ( R 1 R 2 ) = P ( both chip are red ) = P ( R 1 ) P ( R 2 R 1 ) = 1 2 3 4 = 3 8 3 . At a party n men take otheir hat . The hat are then mixed up and each man randomly selects one . We say that a match occurs if a man selects his own hat . 1 . What is the probability of no match 2 . What is the probability of exactly k match Solution We can solve this problem using the corresponding formula for matching problem . However , in the following , we examine a dierent approach using conditional probability . Let E denote the event that no match occur , and to make explicit the dependence on n write P n = P ( E ) . We start by conditioning on whether or not the rst man selects his own hat-call these event M and M c . Then P n = P ( E ) = P ( E M ) P ( M ) + P ( E M c ) P ( M c ) ( 1 ) A. K. Khandani , ECE307Fall 2024 32 Clearly , P ( E M ) = 0 , and so P n = P ( E M c ) n 1 n Now , P ( E M c ) is the probability of no match when n 1 men select from a set of n 1 hat that doe not contain the hat of one of these men . This can happen in either of two mutually exclusive way . Either there are no match and the extra man doe not select the extra hat ( this being the hat of the man that chose rst ) , or there are no match and the extra man doe select the extra hat . The probability of the rst of these event is just P n 1 , which is seen by regarding the extra hat a belonging to the extra man . As the second event ha probability 1 / ( n 1 ) P n 2 , we have P ( E M c ) = P n 1 + 1 n 1 P n 2 and thus , from Equation ( 1 ) , P n = n 1 n P n 1 + 1 n P n 2 or equivalently P n P n 1 = 1 n ( P n 1 P n 2 ) ( 2 ) However , a P n is the probability of no match when n men select among their own hat , we have P 1 = 0 P 2 = 1 2 and so , from Equation ( 2 ) , P 3 P 2 = ( P 2 P 1 ) 3 = 1 3 or P 3 = 1 2 1 3 P 4 P 3 = ( P 3 P 2 ) 4 = 1 4 or P 4 = 1 2 1 3 + 1 4 and , in general , we see that P n = 1 2 1 3 + 1 4 + ( 1 ) n n To obtain the probability of exactly k match , we consider any xed group of k men . The probability that they , and only they , select their own hat is 1 n 1 n 1 1 n ( k 1 ) P n k = ( n k ) n P n k where P n k is the probability that the other n k men , selecting among their own hat , have no match . As there are n k choice of a set of k men , the desired probability of exactly k match is P n k k = 1 2 1 3 + + ( 1 ) n k ( n k ) k 4 . In a batch of 50 unit there are 5 defectives . A unit is selected at random , and thereafter one more from the remaining one . Find the probability that both are defective . Solution Let A be the event that the rst unit is defective and B the event that the second unit is defective . It is seen that P ( A ) = 5 / 50 . If A occurs , there remain 49 unit , 4 of which are defective . Hence we conclude that P ( A B ) = 4 / 49 , and using the following formula P ( AB ) = P ( A ) P ( B A ) = P ( B ) P ( A B ) A. K. Khandani , ECE307Fall 2024 33 we arrive at the probability we seek P ( AB ) = 5 50 4 49 = 2 245 Let u now draw a third unit , and let u evaluate the probability that the rst two unit are defective and that the third one is good . If C is the event that the third unit is good , and we employ the top formula twice to get the following formula for three event P ( ABC ) = P ( AB ) P ( C ( AB ) ) = P ( A ) P ( B A ) P ( C ( AB ) ) we arrive at our nal answer P ( ABC ) = 5 50 4 49 45 48 = 3 392 5 . In a factory , unit are manufactured by machine H 1 , H 2 , H 3 in the proportion 25 35 40 . The percentage 5 , 4 and 2 , respectively , of the manufactured unit are defective . The unit are mixed and sent to the customer . ( a ) Find the probability that a randomly chosen unit is defective . ( b ) Suppose that a customer discovers that a certain unit is defective . What is the probability that it ha been manufactured by machine H 1 Solution ( a ) The Total Probability Theorem state that if the event H 1 , H 2 , ... , H n are mutually exclusive , have positive probability , and together ll the probability space completely , any event A satises the formula P ( A ) = n X i =1 P ( H i ) P ( A H i ) Using this theorem , and taking H i = unit produced by machine H i and A=unit is defective , we nd P ( A ) = 0 . 25 0 . 05 + 0 . 35 0 . 04 + 0 . 40 0 . 02 = 0 . 0345 . ( b ) Bayes Theorem state that , under the same condition a the Total Probability Theorem ( see part ( a ) ) , P ( H i A ) = P ( H i ) P ( A H i ) n X j =1 P ( H j ) P ( A H j ) Hence Bayes theorem give the answer ( A = defective unit ) P ( H 1 A ) = 0 . 25 0 . 05 0 . 25 0 . 05 + 0 . 35 0 . 04 + 0 . 40 0 . 02 = 0 . 36 6 . In a certain country there are two nationality living together , the Bigs and the Smalls . Among the Bigs 80 are tall , and among the Smalls 1 . A visiting tourist encounter a person at random who turn out to be tall . Determine the probability that this person belongs to the Bigs . Solution Let H 1 = the person belongs to the Bigs , H 2 = the person belongs to the Smalls , A=the person is tall . Bayes theorem show that P ( H 1 A ) = P ( H 1 ) 0 . 80 P ( H 1 ) 0 . 80 + P ( H 2 ) 0 . 01 . The formulation of the problem is inadequate , since it is necessary to know the probability P ( H 1 ) and P ( H 2 ) , the proportion of Bigs and Smalls in the country . If the proportion are the same , so that P ( H 1 ) = P ( H 2 ) = 1 / 2 , the probability becomes 80 / 81 = 0 . 99 . But if the Bigs are so few that P ( H 1 ) = 0 . 001 and P ( H 2 ) = 0 . 999 , the probability is instead 0 . 001 0 . 80 / ( 0 . 001 0 . 80 + 0 . 999 0 . 01 ) = 80 / 1079 = 0 . 08 . A. K. Khandani , ECE307Fall 2024 34 7 . Al ip 3 coin and Betty ip 2 . Al win if the number of Heads he get is more than the number Betty get . What is the probability Al will win Solution Let A be the event that Al win , let B i be the event that Betty get i Heads , and let C j be the event that Al get j Heads . By considering the four outcome of ipping two coin it is easy to see that P ( B 0 ) = 1 / 4 P ( B 1 ) = 1 / 2 P ( B 2 ) = 1 / 4 while considering the eight outcome for three coin lead to P ( A B 0 ) = P ( C 1 C 2 C 3 ) = 7 / 8 P ( A B 1 ) = P ( C 2 C 3 ) = 4 / 8 P ( A B 2 ) = P ( C 3 ) = 1 / 8 Since AB i , i = 0 , 1 , 2 are disjoint and their union is A , we have P ( A ) = 2 X i =0 P ( AB i ) = 2 X i =0 P ( A B i ) P ( B i ) since P ( AB i ) = P ( A B i ) P ( B i ) by the denition of conditional probability . Plugging in the value , we obtain , P ( A ) = 1 4 7 8 + 2 4 4 8 + 1 4 1 8 = 7 + 8 + 1 32 = 1 2 8 . Suppose for simplicity that the number of child in a family is 1 , 2 , or 3 , with probability 1 / 3 each . Little Bobby ha no brother . What is the probability he is an only child Solution Let B 1 , B 2 , B 3 be the event that a family ha one , two , or three child , and let A be the event that a family ha only one boy . We want to compute P ( B 1 A ) . By the denition of conditional probability , we have , P ( B 1 A ) = P ( B 1 A ) /P ( A ) To evaluate the numerator we again use the denition of conditional probability P ( B 1 A ) = P ( B 1 ) P ( A B 1 ) = 1 3 1 2 = 1 6 Similarly , P ( B 2 A ) = P ( B 2 ) P ( A B 2 ) = 1 3 2 4 = 1 6 and P ( B 3 A ) = P ( B 3 ) P ( A B 3 ) = 1 3 3 8 = 1 8 Now P ( A ) = X i P ( B i A ) so P ( B 1 A ) = P ( B 1 A ) P ( A ) = 1 / 6 1 / 6 + 1 / 6 + 1 / 8 = 8 8 + 8 + 6 = 4 11 9 . A company buy tire from two supplier , 1 and 2 . Supplier 1 ha a record of delivering tire containing 10 defectives , whereas supplier 2 ha a defective rate of only 5 . Suppose 40 of the current supply came from supplier 1 . If a tire is taken from this supply and observed to be defective , nd the probability that it came from Supplier 1 . A. K. Khandani , ECE307Fall 2024 35 Solution Let B i denote the event that a tire come from supplier i , i = 1,2 , and note that B 1 and B 2 form a partition of the sample space for the experiment of selecting one tire . Let A denote the event that the selected tire is defective . Then , P ( B 1 A ) = P ( B 1 ) P ( A B 1 ) P ( B 1 ) P ( A B 1 ) + P ( B 2 ) P ( A B 2 ) = . 40 ( . 10 ) . 40 ( . 10 ) + ( . 60 ) ( . 05 ) = 0 . 04 . 04 + . 03 = 4 7 Supplier 1 ha a greater probability of being the party supplying the defective tire than doe Supplier 2 . 10 . Urn I contains 2 white and 4 red ball , whereas urn II contains 1 white and 1 red ball . A ball is randomly chosen from urn I and put in to urn II , and a ball is then randomly selected from urn II . 1 ) What is the probability that the ball selected from urn II is white 2 ) What is the conditional probability that the transferred ball wa white , given that a white ball is selected from urn II Solution Letting W be the event that the transferred ball wa white , and E the event that the ball selected from II wa white we obtain , 1 ) P ( E ) = P ( E W ) P ( W ) + P ( E W c ) P ( W c ) = 2 3 2 6 + 1 3 4 6 = 4 9 2 ) P ( W E ) = P ( WE ) P ( E ) = P ( E W ) P ( W ) P ( E ) = 1 2 11 . A laboratory blood test is 95 per cent eective in detecting a certain disease when it is , in fact , present . However , the test also yield a false positive result for 1 per cent of the healthy person tested . ( That is , if a healthy person is tested , then , with probability 0.01 , the test result will imply he ha the disease . ) If 0.5 per cent of the population actually ha the disease , what is the probability a person ha the disease given that his test result is positive Solution Let D be the event that the tested person ha the disease and E the event that his test result is positive . The desired probability P ( D E ) is obtained by P ( D E ) = P ( DE ) P ( E ) = P ( E D ) P ( D ) P ( E D ) P ( D ) + P ( E D c ) P ( D c ) = ( . 95 ) ( . 005 ) ( . 95 ) ( . 005 ) + ( . 01 ) ( . 995 ) = 95 294 . 323 A. K. Khandani , ECE307Fall 2024 36 Thus only 32 per cent of those person whose test result are positive actually have the disease . As you may be surprised at this result ( a it is expected this gure to be much higher , since the blood test seems to be a good one ) , it is probably worthwhile to present a second argument which , although less rigorous than the preceding one , is probably more revealing . Since 0.5 per cent of the population actually ha the disease , it follows that , on the average , 1 person out of every 200 tested will have it . The test will correctly conrm that this person ha the disease with probability 0.95 . Thus , on the average , out of every 200 person tested the test will correctly conrm that 0.95 person have the disease . On the other hand , however , out of the ( on the average ) 199 healthy people , the test will incorrectly state that ( 199 ) ( 0.01 ) of these people have the disease . Hence , for every 0.95 diseased person that the test correctly state are ill , there are ( on the average ) ( 199 ) ( 0.01 ) healthy person that the test incorrectly state are ill . Hence the proportion of time that the test result is correct when it state that a person is ill is 0 . 95 0 . 95 + ( 199 ) ( 0 . 01 ) = 95 294 0 . 323 12 . A purchaser of electrical component buy them in lot of size 10 . It is his policy to inspect 3 component randomly from a lot and to accept the lot only if all 3 are non-defective . If 30 per cent of the lot have 4 defective component and 70 per cent have only 1 , what proportion of lot doe the purchaser reject Solution Let A denote the event that the purchaser accepts a lot . Now , using the Total Probability Theorem P ( A ) = P ( A lot ha 4 defectives ) 3 10 + P ( A lot ha 1 defective ) 7 10 = 4 0 6 3 10 3 3 10 + 1 0 9 3 10 3 7 10 = 54 100 Hence 46 per cent of the lot are rejected . 13 . Suppose event A , B , and C are mutually independent . Form a composite event from A and B and call it E . Is E independent of C Solution Yes . 14 . Suppose A and B are independent event . Does it follow that A c and B c are also independent That is , doe P ( A B ) = P ( A ) P ( B ) guarantee that P ( A c B c ) = P ( A c ) P ( B c ) Solution The answer is yes , the proof being accomplished by equating two dierent expression for P ( A c B c ) . First , we know that P ( A c B c ) = P ( A c ) + P ( B c ) P ( A c B c ) But the union of two complement is also the complement of their intersection . Therefore , P ( A c B c ) = 1 P ( AB ) Combining the two equation above , we get 1 P ( AB ) = 1 P ( A ) + 1 P ( B ) P ( A c B c ) A. K. Khandani , ECE307Fall 2024 37 Since A and B are independent , P ( AB ) = P ( A ) P ( B ) , so P ( A c B c ) = 1 P ( A ) + 1 P ( B ) ( 1 P ( A ) P ( B ) ) = ( 1 P ( A ) ) ( 1 P ( B ) ) = P ( A c ) P ( B c ) the latter factorization implying that A c and B c are , themselves , independent . 15 . We throw a fair four-sided die twice . Let E be the event that the sum of the dice is 4 , and let F be the event that the rst die thrown ha value 2 . Are the event E and F independent Solution The sample space for this problem consists of 16 equally likely outcome a shown graphical in gure 5 . Also shown are the two event E and F .The event EF is a single outcome ( 2,2 ) with probability 1/16 . 1 2 3 4 1 2 3 4 Event G die 2 ha value 4 Event F die 1 ha value 2 Event E sum of dice is 4 die 2 die 1 Figure 5 A sample space for two throw of a four-sided die . On the other hand , p ( E ) = 3 / 16 and P ( F ) = 4 / 16 , thus 3 16 . 4 16 = 3 64 = 1 16 The event E and F are therefore not independent . 16 . Two coin are available , one unbiased and the other two-headed . Choose a coin at random and toss it once assume that the unbiased coin is chosen with probability 3/4 . Given that the result is head , nd the probability that the two-headed coin wa chosen . Solution The tree diagram shown below represents the experiment . We may take to consist of the four possible path through the tree , with each path assigned a probability equal to the product of the probability assigned to each branch . Notice that we are given the probability of the event B 1 = unbiased coin chosen and B 2 = two-headed coin chosen , a well a the conditional probability P ( A B i ) , where A = coin come up head . This is sucient to determine the probability of A. K. Khandani , ECE307Fall 2024 38 two-headed unbiased H T H T 3/4 1/4 1/2 1/2 1 0 all event . Now we can compute P ( B 2 A ) using the denition of conditional probability to obtain P ( B 2 A ) = P ( B 2 A ) P ( A ) = P two-headed coin chosen and coin come up head P coin come up head = ( 1 / 4 ) ( 1 ) ( 3 / 4 ) ( 1 / 2 ) + ( 1 / 4 ) ( 1 ) = 2 5 17 . Russian roulette is played with a revolver equipped with a rotatable magazine of six shot . The revolver is loaded with one shot . The rst duelist , A , rotates the magazine at random , point the revolver at his head and press the trigger . If , afterwards , he is still alive , he hand the revolver to the other duelist , B , who act in the same way a A . The player shoot alternately in this manner , until a shot go o . Determine the probability that A is killed . Solution Let T be the event that A is killed on the rst trial . We have P ( A is killed ) = P ( T ) + P ( T c ) P ( A is killed T c ) . But if A survives the rst trial , the role of A and B are interchanged and so P ( A is killed T c ) =P ( B is killed ) =1-P ( A is killed ) . Inserting this above , we nd P ( A is killed ) = 1 6 + 5 6 1 P ( A is killed ) Solving this equation we nd P ( A is killed ) =6/11 . 18 . A toy manufacturer buy ball bearing from three dierent supplier - 50 of his total order come from supplier 1 , 30 from supplier 2 , and the rest from supplier 3 . Past experience ha shown that the quality control standard of the three supplier are not all the same . Of the ball bearing produced by supplier 1 , 2 are defective , while supplier 2 and 3 produce defective bearing 3 and 4 of the time , respectively . What proportion of the ball bearing in the toy manufacturer inventory are defective Solution Let A i be the event bearing came from supplier i , i =1,2,3 . Let B be the event bearing in toy manufacturer inventory is defective . Then P ( A 1 ) = 0 . 5 , P ( A 2 ) = 0 . 3 , P ( A 3 ) = 0 . 2 and P ( B A 1 ) = 0 . 02 , P ( B A 2 ) = 0 . 03 , P ( B A 3 ) = 0 . 04 A. K. Khandani , ECE307Fall 2024 39 The Total Probability Theorem state that Let A 1 n i =1 be a set of event dened over S such that S = S n i =1 A i , A i A j = for i = j , and P ( A i ) > 0 for i =1,2 , ... , n . For any event B , we have , P ( B ) = n X i =1 P ( B A i ) P ( A i ) Hence , combining the probability according to the theorem give P ( B ) = ( 0 . 02 ) ( 0 . 5 ) + ( 0 . 03 ) ( 0 . 3 ) + ( 0 . 04 ) ( 0 . 2 ) = 0 . 027 meaning the manufacturer can expect 2.7 of his stock of ball bearing to be defective . 19 . A man ha n key on a chain , one of which open the door to his apartment . Having celebrated a little too much one evening , he return home , only to nd himself unable to distinguish one key from another . Resourceful , he work out a endishly clever plan He will choose a key at random and try it . If it fails to open the door , he will discard it and choose one of the remaining n 1 key at random , and so on . Clearly , the probability that he gain entrance with the rst key he selects is 1 /n . What is the probability the door open with the second key he try Solution It would be tempting here to answer 1/ ( n 1 ) , but in this case our intuition would be in error . Actually , 1/ ( n 1 ) is a right answer , but to a dierent question . To see why , let K i , i = 1 , 2 , . . . , n , denote the event i th key tried open door . Then P ( K 1 ) is certainly 1/ n , but the event second key tried open door can occur only if the rst key doe not open the door . That is , P ( K 2 ) = P ( K 2 K c 1 ) Since we also know that P ( AB ) = P ( A B ) P ( B ) ( 3 ) applying this equation to the right-hand side of the top equation , we see that the probability that the second key tried open the door is the same a the probability for the rst key , 1/ n P ( K 2 K c 1 ) = P ( K 2 K c 1 ) P ( K c 1 ) = 1 n 1 n 1 n = 1 n Thus , the ratio 1/ ( n 1 ) doe answer a conditional probability P ( K 2 K c 1 ) . 20 . Urn I contains two white chip ( w 1 , w 2 ) and one red chip ( r 1 ) urn II ha one white chip ( w 3 ) and two red chip ( r 2 , r 3 ) . One chip is drawn at random from urn I and transferred to urn II . Then one chip is drawn from urn II . Suppose a red chip is selected from urn II . What is the probability the chip transferred wa white Solution Let A 1 and A 2 denote the event red chip is transferred from urn I and white chip is transferred from urn I . Let B be the event red chip is drawn from urn II . What we are asking for is P ( A 2 B ) . We note that P ( A 1 ) = 1 3 , P ( A 2 ) = 2 3 , P ( B A 1 ) = 3 4 , and P ( B A 2 ) = 2 4 . Bayes Theorem state A. K. Khandani , ECE307Fall 2024 40 Let A i n i =1 be a set of n event , each with positive probability , that partition S in such a way that S n i =1 A i = S and A i A j = for i = j . For any event B ( also dened on S ) , where P ( B ) > 0 , P ( A j B ) = P ( B A j ) P ( A j ) n X i =1 P ( B A i ) P ( A i ) for any 1 j n . Therefore , we can substitute the probability we have found into Bayes formula to obtain P ( A 2 B ) = P ( B A 2 ) P ( A 2 ) P ( B A 1 ) P ( A 1 ) + P ( B A 2 ) P ( A 2 ) = 2 4 2 3 3 4 1 3 + 2 4 2 3 = 4 7 21 . Urn I contains ve red chip and four white chip urn II contains four red chip and ve white chip . Two chip are to be transferred from urn I to urn II . Then a single chip is to be drawn from urn II . What is the probability the chip drawn from the second urn will be white Solution Let W be the event white chip is drawn from urn II . Let A i , i =0 , 1 , 2 , denote the event i white chip are transferred from urn I to urn II . By the Total Probability Theorem , we obtain , P ( W ) = P ( W A 0 ) P ( A 0 ) + P ( W A 1 ) P ( A 1 ) + P ( W A 2 ) P ( A 2 ) Note that P ( W A i ) = ( 5 + i ) / 11 and that P ( A i ) is gotten directly from the hypergeometric distribution . Therefore , P ( W ) = 5 11 4 0 5 2 9 2 + 6 11 4 1 5 1 9 2 + 7 11 4 2 5 0 9 2 = 5 11 10 36 + 6 11 20 36 + 7 11 6 36 = 53 99 22 . The highway connecting two resort area at A and B are shown in gure below There is a direct route through the mountain and a more-circuitous route going through a third resort area at C in the foothill . Travel between A and B during the winter month is not always possible , the road sometimes being closed due to snow and ice . Suppose we let E 1 , E 2 , and E 3 denote the event that highway AB , AC , and CB are passable , respectively , and we know from past year that on a typical winter day , P ( E 1 ) = 2 5 , P ( E 3 E 2 ) = 4 5 P ( E 2 ) = 3 4 , P ( E 1 E 2 E 3 ) = 1 2 P ( E 3 ) = 2 3 A. K. Khandani , ECE307Fall 2024 41 E 1 E 3 C A B E 2 Figure 6 What is the probability that a traveller will be able to get from A to B Solution If E denotes the event that we can get from A to B , then E = E 1 ( E 2 E 3 ) It follows that P ( E ) = P ( E 1 ) + P ( E 2 E 3 ) P E 1 ( E 2 E 3 ) We know that , P ( E ) = P ( E 1 ) + P ( E 3 E 2 ) P ( E 2 ) P E 1 ( E 2 E 3 ) P ( E 2 E 3 ) = P ( E 1 ) + P ( E 3 E 2 ) P ( E 2 ) P E 1 ( E 2 E 3 ) P ( E 3 E 2 ) P ( E 2 ) = 2 5 + 4 5 3 4 1 2 4 5 3 4 = 0 . 7 23 . A crooked gambler ha nine dice in her coat pocket Three are fair and six are biased . The biased one are loaded in such a way that the probability of rolling a 6 is 1 2 . She take out one die at random and roll it twice . Let A be the event 6 appears on the rst roll and B be the event 6 appears on the second roll . Are A and B independent Solution Our intuition here would most probably answer yes - but , appearance not withstanding , this is not a typical dice problem . Repeated throw of a die do qualify a independent event if the probability associated with the dierent face are known . In this situation , though , those probability are not know and depend in a random way on which die the gambler draw from her pocket . To see formally what eect having two dierent dice ha on the relationship between A and B , we must appeal to the Total Probability Theorem . Let F and L denote the event fair die selected and loaded die selected , respectively . Then P ( AB ) = P ( 6 on rst roll 6 on second roll ) = P ( AB F ) P ( F ) + P ( AB L ) P ( L ) Conditional on either F or L , A and B are independent , so P ( AB ) = 1 6 1 6 3 9 + 1 2 1 2 6 9 = 19 108 Similarly , P ( A ) = P ( A F ) P ( F ) + P ( A L ) P ( L ) = 1 6 3 9 + 1 2 6 9 = 7 18 = P ( B ) A. K. Khandani , ECE307Fall 2024 42 But note that P ( AB ) = 19 108 = 57 324 = P ( A ) P ( B ) = 7 18 7 18 = 49 324 proving that A and B are not independent . 24 . Laplaces Rule of Succession . There are k +1 biased coin in a box . The i th coin will , when ipped , turn up head with probability i/k , i =0,1 , . . . , k . A coin is randomly selected from the box and is then repeatedly ipped . If the rst n ip all result in head , what is the conditional probability that the ( n +1 ) st ip will do likewise Solution Let E i denote the event that the i th coin is initially selected , i =0,1 , . . . , k let F n denote the event that the rst n ip all result in head and let F be the event that the ( n +1 ) st ip is a head . The desired probability , P ( F F n ) , is now obtained a follows , P ( F F n ) = k X i =0 P ( F F n E i ) P ( E i F n ) Now , given that the i th coin is selected , it is reasonable to assume that the outcome will be conditionally independent with each one resulting in a head with probability i/k . Hence P ( F F n E i ) = P ( F E i ) = i k Also , P ( E i F n ) = P ( E i F n ) P ( F n ) = P ( F n E i ) P ( E i ) k X j =0 P ( F n E j ) P ( E j ) = ( i/k ) n 1 / ( k + 1 ) k X j =0 ( j/k ) n 1 / ( k + 1 ) Hence P ( F F n ) = k X i =0 ( i/k ) n +1 k X j =0 ( j/k ) n But if k is large , we can use the integral approximation 1 k k X i =0 ( i/k ) n +1 Z 1 0 x n +1 dx = 1 n + 2 1 k k X j =0 ( j/k ) n Z 1 0 x n dx = 1 n + 1 and so , for k large , P ( F F n ) n + 1 n + 2 A. K. Khandani , ECE307Fall 2024 43 25 . A bag contains 3 white ball , 6 red ball and 3 black ball . Three ball are drawn at random , without replacement . What is the probability that the three ball are the same color Solution Let W be the event the three ball are all white , R the event the three ball are all red , B the event the three ball are all black and C the event the ball are of the same color . Then by the theorem on total probability , P ( C ) = P ( C W ) P ( W ) + P ( C R ) P ( R ) + P ( C B ) P ( B ) = 1 3 3 + 1 6 3 + 1 3 3 1 12 3 26 . From a standard deck of 52 card , one red one is taken . Thirteen card are then chosen and found to be of the same color . What is the probability they are black Solution Given that the 13 card are of the same color , they could be red ( R ) or black ( B ) . Denote by C the event that all 13 card are of the same color . It is required to nd P ( B C ) . By Bayes theorem P ( B C ) = P ( C B ) P ( B ) P ( C B ) P ( B ) + P ( C R ) P ( R ) = 26 13 / 51 13 26 13 / 51 13 + 25 13 / 51 13 . Since P ( C R ) = P ( C B ) = 1 , then P ( B C ) 1 1 + 25 13 / 26 13 = 1 1 + 1 2 = 2 3 . and hence P ( C ) = 1 / 10 . 27 . A fair die is tossed . If the face show i then a fair coin is tossed i time . What is the probability of obtaining at least 3 head on any given turn Solution Let A be the event of obtaining at least three head on any outcome of the experiment and let B i be the event of obtaining i on the toss of a die , i = 1 , 2 , , 6 . By the theorem on total probability , P ( A ) = 6 X i =1 P ( A B i ) P ( B i ) . Now P ( B i ) = 1 / 6 and P ( A B 1 ) = P ( A B 2 ) = 0 and P ( A B 3 ) = 1 8 P ( A B 4 ) = 4 3 + 4 4 1 2 4 P ( A B 5 ) = 5 3 + 5 4 + 5 5 1 2 5 P ( A B 6 ) = 6 3 + 6 4 + 6 5 + 6 6 1 2 6 and the result follows . A. K. Khandani , ECE307Fall 2024 44 28 . If A and B are independent event and the probability that both A and B occur is .16 while the probability that neither occur is .36 , determine P ( A ) and P ( B ) . Solution From the problem statement , P ( AB ) = P ( A ) P ( B ) = 0 . 16 and P ( A B ) c = 0 . 36 from which it follows that P ( A B ) = 0 . 64 = P ( A ) + P ( B ) 0 . 16 which give two equation in two unknown ( i ) P ( A ) + P ( B ) = 0 . 80 ( ii ) P ( A ) P ( B ) = 0 . 16 which are easily solved to give P ( A ) = 0 . 40 and P ( B ) = 0 . 40 . 29 . Of three coin , 2 are two-headed and the other is fair . From these three coin , one is chosen at random and tossed n time . If the result is all head , what is the probability the coin tossed is two-headed Solution Let A be the event the coin is fair ( i.e . two headed ) , B the event the coin is two-headed and C the event that n head are obtained in n toss of the coin . Then P ( C B ) = 1 , P ( C A ) = 1 / 2 n , P ( A ) = 1 / 3 and P ( B ) = 2 / 3 . By Bayes theorem P ( B C ) = P ( C B ) P ( B ) P ( C B ) P ( B ) + P ( C A ) P ( A ) = 1 2 3 1 2 3 + 1 2 n 1 3 = 2 n +1 2 n +1 + 1 . 30 . A fair coin is thrown n time . ( a ) Show that the conditional probability of a head on any specied trial , given a total of k head over the n trial , is k/n . ( b ) Suppose 0 r k and 0 < m < n are integer . Show that the conditional probability of r head over the rst m trial , given a total of k head over all n trial , is m r n m k r / n k . Solution ( a ) Let A i be the event that a head is obtained on the i th trial and B k the event of obtaining k head in n trial . Then P ( A i B k ) = n 1 k 1 1 2 n 1 1 2 and P ( B k ) = n k 1 2 n Thus P ( A i B k ) = n 1 k 1 1 2 n n k 1 2 n = k n . ( b ) Let C r be the event of obtaining r head in the rst m place . Then P ( B k C r ) is the probability of k head in n throw with r of the k among the rst m . Then P ( C r B k ) = m r n m k r 2 n / n k 2 n = m r n m k r n k . A. K. Khandani , ECE307Fall 2024 45 31 . Stores A , B and C have 50 , 75 and 100 employee of which 50 , 60 and 70 respectively , are woman . Resignations are equally likely among all employee , regardless of gender . One employee resigns and this is a woman . What is the probability she worked in store C Solution By straightforward application of Bayes theorem P ( C W ) = P ( W C ) P ( C ) P ( W C ) P ( C ) + P ( W A ) P ( A ) + P ( W B ) P ( B ) = 0 . 7 ( 100 / 225 ) 0 . 7 ( 100 / 225 ) + 0 . 5 ( 50 / 225 ) + 0 . 6 ( 75 / 225 ) = 70 70 + 25 + 45 = 1 2 . 32 . Two hunter shoot at a deer which is hit by exactly one bullet . If the rst hunter hit his target with probability 0.3 and the second with probability 0.6 , what is the probability the second hunter killed the deer ( The answer is not 2/3 . ) Solution Let H 2 be the event that hunter 2 shot the deer and H c 2 the event that hunter 2 did not shoot the deer . Let B be the event that only one hunter shot the deer . Then P ( B ) = 0 . 3 0 . 4 + 0 . 7 0 . 6 = 0 . 12 + 0 . 42 = 0 . 54 and the conditional probability P ( H 2 B ) = 0 . 42 0 . 12 + 0 . 42 = 42 54 = 7 9 . 33 . You are about to have an interview for the Harvard Law School . 60 of the interviewer are conservative and 40 are liberal . 50 of the conservative smoke cigar but only 25 of the liberal do . Your interviewer light up a cigar . What is the probability he is a liberal Solution Let L = liberal and C = conservative and S = smoker . Then by Bayes theorem P ( L S ) = P ( S L ) P ( L ) P ( S L ) P ( L ) + P ( S C ) P ( C ) = . 025 0 . 4 0 . 25 0 . 4 + 0 . 5 0 . 6 = 0 . 25 . 34 . One slot machine pay o1/2 of the time . while another pay o1/4 of the time . A player pick one of the machine and play it six time , winning three of the time . What is the probability he is playing the machine that pay o1/4 of the time Solution Let S 1 , S 2 be slot machine 1 and 2 respectively , and W 3 , 6 the event of winning 3 out of 6 with the machine being played . Then by Bayes theorem P ( S 2 W 3 , 6 ) = P ( W 3 , 6 S 2 ) P ( S 2 ) P ( W 3 , 6 S 2 ) P ( S 2 ) + P ( W 3 , 6 S 1 ) P ( S 1 ) = 6 3 ( 1 / 4 ) 3 ( 3 / 4 ) 3 ( 1 / 2 ) 6 3 ( 1 / 4 ) 3 ( 3 / 4 ) 3 ( 1 / 2 ) + 6 3 ( 1 / 2 ) 6 ( 1 / 2 ) = 27 91 . 35 . Consider a line AB divided into two part by a point C , where the length of segment AC is greater than or equal to the length of segment CB ( see the following gure ) . Suppose a point X is chosen at random along the segment AC and a point Y is chosen at random along the segment CB . Let x and y denote the A. K. Khandani , ECE307Fall 2024 46 Y y x A X C B b a distance of X and Y from A and B , respectively . What is the probability the three segment AX , XY , and Y B will t together to form a triangle Solution The key here is to recognize that three condition must be met if the segment are to form a triangle - each segment must be shorter than the sum of the other two 1 . x < ( a + b x y ) + y 2 . a + b x y < x + y 3 . y < x + ( a + b x y ) Intuitively , it seems clear that the probability of the segment forming a triangle will be greatest when C is the midpoint of AB As b get smaller , y tends to get smaller , and the likelihood of condition ( 1 ) and ( 2 ) both being true diminishes . To make the argument precise , we need to determine what proportion of the sample space S = ( x , y ) 0 x a , 0 y b is included in the ( x , y ) -values satisfying condition ( 1 ) , ( 2 ) , and ( 3 ) . Note that 1 . x < ( a + b x y ) + y x < a + b 2 2 . a + b x y < x + y x + y > a + b 2 3 . y < x + ( a + b x y ) y < a + b 2 The ( x , y ) -values satisfying all three of these inequality make up the interior of the triangle shown in the gure . Call that interior E . 0 ( ( a+b ) /2,0 ) y x a ( 0 , ( a+b ) /2 ) b ( ( a+b ) /2 , ( a+b ) /2 ) E S x+y= ( a+b ) /2 E U S A. K. Khandani , ECE307Fall 2024 47 It follows that the probability of the segment forming a triangle will equal the area of E S divided by the area of S P ( segment form triangle ) = 1 2 b 2 ab = b 2 a As expected , the probability is greatest when C is midway between A and B , and it decrease fairly rapidly a C approach B . 36 . Consider the following connection of switch A S 1 S 2 B S 4 S 3 Dene the event E i , i = 1 , 2 , 3 , 4 a Switch S i is closed . Assume that P ( E 1 ) = P ( E 2 ) = P ( E 3 ) = P ( E 4 ) = a . Dene the event E a the event that point A is connected to point B . Compute the probability of the event E . Solution Consider the connection of switch shown in the above gure . Dene the event E 1 Switch S 1 is closed . = E c 1 Switch S 1 is open . E 2 Switch S 2 is closed . = E c 2 Switch S 2 is open . E 3 Switch S 3 is closed . = E c 3 Switch S 3 is open . E 4 Switch S 4 is closed . = E c 4 Switch S 4 is open . Assume that P ( E 1 ) = P ( E 2 ) = P ( E 3 ) = P ( E 4 ) = a , and consequently , P ( E c 1 ) = P ( E c 2 ) = P ( E c 3 ) = P ( E c 4 ) = 1 a . Dene the event E a the event that point A is connected to point B . What is P ( E ) . First method E = ( E 1 E 2 ) E 3 E 4 P ( E ) = P ( E 1 E 2 ) E 3 P ( E 4 ) P ( E 1 E 2 ) E 3 = P ( E 1 E 2 ) + P ( E 3 ) P ( E 1 E 2 E 3 ) = a + a 2 a 3 P ( E ) = a ( a + a 2 a 3 ) = a 2 + a 3 a 4 Second method ( fallacy ) P E 4 E 3 ( E 1 E 2 ) = P E 4 ( E 3 E 1 ) ( E 3 E 2 ) P ( E 3 E 1 ) = P ( E 3 ) + P ( E 1 ) P ( E 3 E 1 ) = a + a a 2 = 2 a a 2 P ( E 3 E 2 ) = P ( E 3 ) + P ( E 2 ) P ( E 3 E 2 ) = a + a a 2 = 2 a a 2 A. K. Khandani , ECE307Fall 2024 48 P E 4 ( E 3 E 1 ) ( E 3 E 2 ) = P ( E 4 ) P ( E 3 E 1 ) P ( E 3 E 2 ) = a ( 2 a a 2 ) 2 It is seen that the two answer are dierent . Why Third method E c = ( E 1 E 2 ) E 3 E 4 c = E c 4 E 3 ( E 1 E 2 ) c = E c 4 E c 3 ( E c 1 E c 2 ) P ( E c ) = P E c 4 E c 3 ( E c 1 E c 2 ) = P ( E c 4 ) + P E c 3 ( E c 1 E c 2 ) P E c 4 E c 3 ( E c 1 E c 2 ) P ( E c 4 ) = 1 a P ( E c 1 E c 2 ) = ( 1 a ) + ( 1 a ) ( 1 a ) 2 = 1 a 2 ( this is indeed equal to P ( E 1 E 2 ) c ) . P E c 4 E c 3 ( E c 1 E c 2 ) = ( 1 a ) 2 ( 1 a 2 ) Substituting result in P ( E c ) = P ( E c 4 ) + P E c 3 ( E c 1 E c 2 ) P E c 4 E c 3 ( E c 1 E c 2 ) = ( 1 a ) + ( 1 a ) ( 1 a 2 ) ( 1 a ) 2 ( 1 a 2 ) = 1 ( a 2 + a 3 a 4 ) . Which is in agreement with the previous result . Fourth method Let 1 correspond to a closed switch and 0 correspond to an open switch . Using these notation , the sample space is represented a S 4 S 3 S 2 S 1 e 1 0 0 0 0 e 2 0 0 0 1 e 3 0 0 1 0 e 4 0 0 1 1 e 5 0 1 0 0 e 6 0 1 0 1 e 7 0 1 1 0 e 8 0 1 1 1 e 9 1 0 0 0 e 10 1 0 0 1 e 11 1 0 1 0 OK e 12 1 0 1 1 a 3 ( 1 a ) OK e 13 1 1 0 0 a 2 ( 1 a ) 2 OK e 14 1 1 0 1 a 3 ( 1 a ) OK e 15 1 1 1 0 a 3 ( 1 a ) OK e 16 1 1 1 1 a 4 The e 1 , e 2 , . . . , e 16 are the elementary event . P ( E ) = P ( e 12 e 13 e 14 e 15 e 16 ) . As the elementary event are disjoint , the probability add . As a result , P ( E ) = P ( e 12 ) + P ( e 13 ) + P ( e 14 ) + P ( e 15 ) + P ( e 16 ) = a 3 ( 1 a ) + a 2 ( 1 a ) 2 + a 3 ( 1 a ) + a 3 ( 1 a ) + a 4 = a 2 + a 3 a 4 . Which is in agreement with the previous result . A. K. Khandani , ECE307Fall 2024 49",
    "4": "Random Variables 4.1 Random Variables It is frequently the case when an experiment is performed that we are mainly interested in some function of the outcome a opposed to the actual outcome itself . For instance , in tossing dice we are often interested in the sum of the two dice and are not really concerned about the separate value of each die . These quantity of interest , or more formally , these real-valued function dened on the sample space , are known a random variable . Random Variable A random variable is a number which selects value in an unpredictable manner . In other word , a random variable is the result of a random experiment whose outcome is a number . A more formal denition of a random variable is a follows . Let S be the sample space associated with some experiment E . A random variable X is a function that assigns a real number X ( s ) to each elementary event s S . In this case , a the outcome of the experiment , i.e. , the specic s , is not predetermined , then the value of X ( s ) is not xed . This mean that the value of the random variable is determined by the specic outcome of the experiment . The domain of a random variable X is the sample space S and the range space R X is a subset of the real line , i.e. , R X . There are two type of random variable discrete and continuous . It is conventional to use capital letter such a X , Y , S , T , . . . , to denote a random variable and the corresponding lower case letter , x , y , s , t , . . . , to denote particular value taken by the random variable . A. K. Khandani , ECE307Fall 2024 50 4.2 Discrete Random Variables A random variable that can take on at most a countable number of possible value is said to be discrete . Probability mass function For a discrete random variable X , we dene the probability mass function p ( a ) of X by p ( a ) = P X = a Since X must take on one of the value x i , we have X i p ( x i ) = X i p x = x i = 1 Cumulative Distribution Function ( CDF ) For the random variable X , we dene the function F ( x ) by the equation , F ( x ) = P ( X x ) . And for any a F ( a ) = X all x a p ( x ) 4.3 Expected Value The expected value of the discrete random variable X , which we denote by E ( X ) , is given by E ( X ) = X x xp ( x ) A. K. Khandani , ECE307Fall 2024 51 4.4 Expectation of a Function of a Random Variable A function of a discrete random variable X , say Y = g ( X ) , is also a discrete random variable . We have , E ( Y ) = E g ( X ) = X i g ( x i ) p ( x i ) If a and b are constant , then E aX + b = aE X + b 4.5 Variance The variance of the random variable X , which we denote by Var ( X ) , is given by , Var ( X ) = E X 2 = E X 2 2 where = E X For any two constant a and b Var ( aX + b ) = a 2 Var ( X ) Standard deviation is dened a the square root of the variance . That is , SD ( X ) = q Var ( X ) A. K. Khandani , ECE307Fall 2024 52 4.6 The Bernoulli and Binomial Random Variables Bernoulli random variable The Bernoulli random variable corresponds to an event E which have two possible outcome , success ( X = 1 ) and failure ( X = 0 ) and it probability mass function is p ( 0 ) = P X = 0 = 1 p p ( 1 ) = P X = 1 = p for 0 p 1 . Binomial random variable We repeat the Bernoulli experiment n time . The n trial are independent . Such an independent repetition of the same experiment under identical condition is called a Bernoulli Trial . Let random variable X denotes the total number of time that E ha occurred ( success ) . We have , p ( i ) = n i p i ( 1 p ) n i , i = 0 , 1 , . . . , n The binomial coecient enters here to account for the total number of possible way that the i time of success can be located among the n time of performing the experiment . We let 0 = 1 by denition . A. K. Khandani , ECE307Fall 2024 53 4.6.1 Properties of Binomial Random Variables The mean of the distribution is E X = n X i =0 iP ( i ) = n X i =1 i n i p i ( 1 p ) n i = n X i =1 i n i ( n i ) p i ( 1 p ) n i = n X i =1 n ( n 1 ) ( i 1 ) ( n i ) p i ( 1 p ) n i = n n X i =1 ( n 1 ) ( i 1 ) ( n i ) p i ( 1 p ) n i substituting j = i 1 , we obtain , = n n 1 X j =0 ( n 1 ) j ( n 1 j ) p j +1 ( 1 p ) n 1 j = np n 1 X j =0 ( n 1 ) j ( n 1 j ) p j +1 ( 1 p ) n 1 j = np n 1 X j =0 n 1 j p j ( 1 p ) n 1 j = np ( p + 1 p ) n 1 = np ( 1 ) n 1 = np The variance of the distribution can be computed similarly . The result is Var ( X ) = np ( 1 p ) If X is a binomial random variable with parameter ( n , p ) , a k go from 0 to n , P X = k rst increase monotonically and then decrease monotonically , reaching it largest value when k is the largest integer less than or equal to ( n + 1 ) p . A. K. Khandani , ECE307Fall 2024 54 4.6.2 Computing the Binomial Distribution Function Suppose that X is binomial with parameter ( n , p ) . The key to computing it distribution function P X i = i X k =0 n k p k ( 1 p ) n k i = 0 , 1 , . . . , n is to utilize the following relationship between P X = k + 1 and P X = k , P X = k + 1 = p 1 p n k k + 1 P X = k 4.7 The Poisson Random Variable A random variable X , taking one of the value 0 , 1 , 2 , . . . is said to be a Poisson random variable with parameter if for some > 0 , p ( i ) = P X = i = e i i The Poisson random variable is used a an approximation of a binomial random variable with parameter ( n , p ) when n is large and p is small so that np is in moderate size . The parameter of the approximated Poisson random variable is = np . That is , P X = i = n ( n i ) i p i ( 1 p ) n i = n ( n i ) i n i 1 n n i = n ( n 1 ) ( n i + 1 ) n i i i ( 1 /n ) n ( 1 /n ) i Now , for n large and moderate , 1 n n e n ( n 1 ) ( n i + 1 ) n i 1 1 n i 1 A. K. Khandani , ECE307Fall 2024 55 Hence , for n large and moderate , P X = i = e i i The mean of the distribution is E X = X i =0 ie i i = X i =1 ie i 1 ( i 1 ) = e X j =0 j j by substituting j = i 1 , = since X j =0 j j = e The variance of the distribution can be computed similarly . The result is Var ( X ) = The expected value and variance of a Poisson random variable are both equal to it parameter . A. K. Khandani , ECE307Fall 2024 56 4.7.1 Computing the Poisson Distribution Function If X is Poisson with parameter , then P X = i + 1 P X = i = e i +1 / ( i + 1 ) e i / ( i ) = ( i + 1 ) Starting with P X = i + 1 = e , we can use the above equation to compute successively P X = 1 = P X = 0 P X = 2 = 2 P X = 1 ... P X = i + 1 = i + 1 P X = i 4.8 Other Discrete Probability Distributions Geometric Distribution Suppose we repeat an experiment , independently , until the event E ( success ) occurs . What is the probability that the rst success occurs on the n th try if , on any one try , we have P ( E ) = p P X = n = ( 1 p ) n 1 p , n = 1 , 2 , . . . This is referred to a the Geometric Distribution . We can show that , E X = 1 p , Var ( X ) = 1 p p 2 A. K. Khandani , ECE307Fall 2024 57 The negative binomial random variable Suppose we repeat an experiment . Let X be the number of trial to obtain the rst r success . We can show that , P ( X = n ) = n 1 r 1 p r ( 1 p ) n r , n = r , r + 1 , . . . This is called the negative binomial Distribution . The binomial coecient enters here to account for the total number of possible way that the r 1 time of success can be located among the n 1 time of performing the experiment . Note that the last experiment ha denitely resulted in a success , and consequently , we only consider the number of way that the rst r 1 success are located among the rst n 1 trial . We have , E X = r p and Var ( X ) = r ( 1 p ) p 2 The hypergeometric random variable Assume that N item are in an urn , numbered 1 , 2 , . . . , N . Suppose that item 1 , 2 , . . . , m are white and the remaining N m item are black . Assume further that n item from the N are chosen at random . The probability that , in a draw of n item , we obtain exactly i white item is equal to P ( X = i ) = m i N m n i N n , i = 0 , 1 , 2 , . . . , min ( n , m ) This is referred to a the Hypergeometric Distribution . The mean and variance of the Hypergeometric Distribution are equal to E X = nm N , Var ( X ) = N n N 1 np ( 1 p ) A. K. Khandani , ECE307Fall 2024 58 The Zeta ( or zipf ) distribution A random variable is said to have a zeta distribution if it probability mass function is given by P X = k = C k +1 for some value of > 0 and C = X k =1 1 k +1 1 The zeta distribution owes it name to the fact that the function ( s ) = 1 + 1 2 s + 1 3 s + 1 k s + is known in mathematical discipline a the Riemann zeta function ( after the German mathematician G. B. F. Riemann ) . 4.9 Properties of Cumulative Distribution Function Some Properties of the cumulative distribution function ( c.d.f . ) F are 1 . F is a nondecreasing function that is , if a < b , then F ( a ) F ( b ) . 2. lim b F ( b ) = 1 . 3. lim b F ( b ) = 0 . 4 . F is right continuous . That is , for any b and any decreasing sequence b n , n 1 , that converges to b , lim n F ( b n ) = F ( b ) . 5 . P a < X b = F ( b ) F ( a ) A. K. Khandani , ECE307Fall 2024 59 4.10 Some Solved Problems 1 . A large lot of item is known to contain a fraction defective . Let X denote the random variable for the number of item to be inspected to obtain the second defective item . Find the probability distribution and mean of X . Solution If the second defective item is drawn on the i th draw , then the rst defective can be in one of the rst ( i 1 ) position . The probability that X = i is then P ( X = i ) = ( i 1 ) ( 1 ) i 2 2 , i = 2 , 3 , . To see that this is a probability distribution , note the following formula , obtained by dierentiating the rst one two time X i =0 x i = 1 1 x , X i =1 ix i 1 = 1 ( 1 x ) 2 , X i =2 i ( i 1 ) x i 2 = 2 ( 1 x ) 3 It follows readily from these relationship that X j =2 ( i 1 ) 2 ( 1 ) i 2 = 1 and E ( X ) = X i =2 iP ( X = i ) = X i =2 i ( i 1 ) 2 ( 1 ) i 2 = 2 2 . Find the mean and variance of the Poisson random variable Y if it is three time a likely that Y = 4 than Y = 2 . Solution Since , by the problem statement , P ( Y = 4 ) = e 4 4 = 3 e 2 2 = 6 Then E ( Y ) = Var ( Y ) = = 6 ( property of the Poisson distribution ) . 3 . The discrete random variable U ha a geometric distribution of the form P ( U = j ) = ap j , j = 0 , 1 , 2 , If P ( U 4 ) = 1 / 256 , nd P ( U 2 ) . Solution Since the given distribution must sum to unity we have X j =0 ap j = 1 = a/ ( 1 p ) which implies that a = 1 p . Now P ( U 4 ) = X j =4 ap j = 1 / 256 = ap 4 1 p = 1 256 p = 1 4 Then P ( U 2 ) = ( 1 p ) p 2 / ( 1 p ) = 1 / 16 . 4 . Z is a discrete random variable with probability distribution P ( Z = j ) = aj 2 ( j 1 ) , j = 1 , 2 , For what value of a is this a distribution A. K. Khandani , ECE307Fall 2024 60 Solution Notice that for x < 1 , X j =0 x j = 1 / ( 1 x ) . Dierentiating both side of this relationship wrt x give X j =1 jx j 1 = 1 / ( 1 x ) 2 . Applying this to the problem give X j =1 aj 2 ( j 1 ) = a 1 1 2 2 = 1 which give 4 a = 1 or a = 1 / 4 . 5 . The number of trial , X , to obtain the rst defective from a large lot of item , ha a geometric distribution with variance 2 . What is the probability it take more than 4 draw to obtain the rst defective Solution The mean of a geometric distribution is 1 / and the variance is ( 1 ) / 2 . If ( 1 ) / 2 = 2 then = 1 / 2 . The probability X > 4 is then P ( X > 4 ) = X 5 i 1 ( 1 ) = 5 1 = 1 16 . 6 . The number of emission from a radioactive source , Z , in a one hour period , is known to be a Poisson random variable . If it is known that the probability there are 10 emission in the one hour period , is exactly the same a the probability there are 12 emission , what is the probability there are no emission in the one hour period Solution If P ( X = 10 ) = exp ( ) 10 / 10 = exp ( ) 12 / 12 then 2 = 132 . it follows that P ( X = 0 ) = e = e 132 . 7 . In Lotto 649 , a player chooses 6 distinct number from 1 to 49 . This set is compared to 6 other number chosen at random by the house and prize are awarded according to the number matched . ( a ) What is the probability that k of the number match , k = 3 , 4 , 5 , 6 . Calculate the answer to 8 signicant digit . ( b ) What is the probability that all six number chosen by the house are from 1 to 9 ( c ) What is the probability that 2 of the number chosen by the house are from 1 to 9 and one each is from 10 to 19 , 20 to 29 , 30 to 39 and 40 to 49 respectively Solution ( a ) Let X be the number of the player 6 number that match the house 6 number . Then X ha a hypergeometric distribution and P ( X = k ) = 6 k 43 6 k 49 6 which is evaluated a P ( X = 3 ) = . 017650404 P ( X = 4 ) = . 00096861972 P ( X = 5 ) = . 000018449899 P ( X = 6 ) = 7 . 1511238 10 8 A. K. Khandani , ECE307Fall 2024 61 ( b ) The probability all six number chosen by the house are from 1 to 9 is 9 6 49 6 = 6 . 00694403 10 6 . ( c ) The required probability is 9 2 10 1 4 49 6 = 0 . 025744046 . 8 . In the game of Keno , a player chooses 5 distinct number from 1 to 100 , inclusive . A machine then chooses 10 number , again from 1 to 100 . Let X be the number of the player number chosen by the machine . Find the probability distribution of X . Solution This is a simple hypergeometric distribution P ( X = k ) = 10 k 90 5 k 100 5 . 9 . Consider a random experiment in which the probability of certain outcome , say A , is equal to p . The experiment is performed for n time . Let X denote the total number of time that A ha happened . Compute the average and the standard deviation of X . Solution Consider an experiment having only two possible outcome , A and A , which are mutually exclu- sive . Let the probability be P ( A ) = p and P ( A ) = 1 p = q . The experiment is repeated n time and the probability of A occurring i time is P i = n i p i q n i where n i is the binomial coecient , n i = n i ( n i ) The binomial coecient enters here to account for the total number of possible way to combine n item taken i at a time , with all possible permutation permitted . We let 0 = 1 by denition . This is called a Binomial Random Variable . It is straightforward to show that the mean and the variance of a Binomial Random Variable are equal to , mean = np 2 = np ( 1 p ) = m ( 1 p ) A. K. Khandani , ECE307Fall 2024 62 To compute the mean , we have , mean = n X i =0 iP i = n X i =1 i n i p i q n i = n X i =1 i n i ( n i ) p i q n i = n X i =1 n ( n 1 ) ( i 1 ) ( n i ) p i q n i = n n X i =1 ( n 1 ) ( i 1 ) ( n i ) p i q n i substituting j = i 1 , we obtain , = n n 1 X j =0 ( n 1 ) j ( n 1 j ) p j +1 q n 1 j = np n 1 X j =0 ( n 1 ) j ( n 1 j ) p j q n 1 j = np n 1 X j =0 n 1 j p j q n 1 j = np ( p + q ) n 1 = np ( 1 ) n 1 = np The standard deviation can be computed similarly . 10 . A person repeatedly shoot at a target and stop a soon a he hit it . The probability of hitting the target is 2/3 each time . The shot are red independently , and hence may be regarded a a sequence of repeated trial . Determine the probability that the target is hit on the k th attempt . Solution The event in which we are interested happens if the target is missed k 1 time and then is hit the k th time . The probability of missing the target with the rst shot is 1 2 / 3 = 1 / 3 , and likewise in each of the following shot . Because of the independence , the probability we seek is ( 1 / 3 ) k 1 ( 2 / 3 ) . 11 . Russian roulette is played with a revolver equipped with a rotatable magazine of six shot . The revolver is loaded with one shot . The rst duelist , A , rotates the magazine at random , point the revolver at his head and press the trigger . If , afterwards , he is still alive , he hand the revolver to the other duelist , B , who act in the same way a A . The player shoot alternately in this manner , until a shot go o . Determine the probability that A is killed . Solution Let H i be the event that a shot go oat the i th trial . The event H i are mutually exclusive . The event H i occurs if there are i 1 failure and then one success . Hence , we get , P ( H i ) = 5 6 i 1 1 6 The probability we want is given by P = P ( H 1 H 3 H 5 ) = 1 6 1 + 5 6 2 + 5 6 4 + = 1 6 1 1 5 6 2 = 6 11 Hence the probability that B loses his life is 1 6 / 11 = 5 / 11 that is , the second player ha a somewhat greater chance of surviving , a might be expected . A. K. Khandani , ECE307Fall 2024 63 12 . In a food package there is a small prize which can be of N dierent type . All type are distributed into the package at random with the same probability . A person buy n package in a shop . What is the probability that he obtains a complete collection of present Solution Denote the event in question by H and consider it complement H , which is the event that at least one type is missing . We then have P ( H ) = P N 1 A i , where A i =type no . i is missing . We now utilize the same addition formula for the union of some event . It is evident that P ( A i 1 A i 2 ... A i r ) = 1 r N n for the probability that a given package doe not contain type no . i 1 , i 2 , ... , i r is 1 r/N . Hence we nd 1 P ( H ) = P ( H c ) = N 1 1 N n N 2 1 2 N n + + ( 1 ) N 1 N N 1 N N n 13 . Suppose we roll a die repeatedly until a 6 occurs , and let N be the number of time we roll the die . Compute the average value and the variance of N . Solution Using to denote not a 6 , we have P ( N = 1 ) = P ( 6 ) = 1 6 P ( N = 2 ) = P ( 6 ) = 5 6 1 6 P ( N = 3 ) = P ( 6 ) = 5 6 5 6 1 6 From the rst three term it is easy to see that for k 1 P ( N = k ) = P ( on k-1 roll then 6 ) = 5 6 k 1 1 6 Generalizing , we see that if we are waiting for an event of probability p , the number of trial needed , N , ha the following distribution , P ( N = k ) = ( 1 p ) k 1 p for k = 1 , 2 , . . . since N = k occurs exactly when we have k 1 failure followed by a success . This is called a Geometric Distribution and a we will see in the lecture , it ha , E ( N ) = 1 /p V ar ( N ) = ( 1 p ) /p 2 These value can be easily obtained using the second and the third formula given in your sheet of Some useful relationship . 14 . Suppose that an airplane engine will fail , when in ight , with probability 1 p independently from engine to engine . Suppose that the airplane will make a successful ight if at least 50 per cent of it engine remain operative . For what value of p is a 4-engine plane preferable to a 2-engine plane A. K. Khandani , ECE307Fall 2024 64 Solution As each engine is assumed to fail or function independently of what happens with the other engine , it follows that the number of engine remaining operative is a binomial random variable . Hence the probability that a 4-engine plane make a successful ight is 4 2 p 2 ( 1 p ) 2 + 4 3 p 3 ( 1 p ) + 4 4 p 4 ( 1 p ) 0 = 6 p 2 ( 1 p ) 2 + 4 p 3 ( 1 p ) + p 4 whereas the corresponding probability for a 2-engine plane is 2 1 p ( 1 p ) + 2 2 p 2 = 2 p ( 1 p ) + p 2 Hence the 4-engine plane is safer if 6 p ( 1 p ) 2 + 4 p 2 ( 1 p ) + p 3 2 p which simplies to 3 p 3 8 p 2 + 7 p 2 0 or ( p 1 ) 2 ( 3 p 2 ) 0 which is equivalent to 3 p 2 0 or p 2 3 Hence the 4-engine plane is safer when the engine success probability is at least a large a 2 3 , whereas the 2-engine plane is safer when this probability fall below 2 3 . 15 . Ten hunter are waiting for duck to y by . When a ock of duck y overhead , the hunter re at the same time , but each chooses his target at random , independently of the others . If each hunter independently hit his target with probability p , compute the expected number of duck that escape unhurt when a ock of size 10 y overhead . Solution Let X i equal 1 if the i th duck escape unhurt and 0 otherwise , i = 1 , 2 , . . . , 10 . The expected number of duck to escape can be expressed a E X 1 + + X 10 = E X 1 + + E X 10 To compute E X i = P ( X i = 1 ) , we note that each of the hunter will , independently , hit the i th duck with probability p/ 10 and so P ( X i = 1 ) = 1 p 10 10 Hence E X = 10 1 p 10 10 16 . A fair coin is to be tossed until a head come up for the rst time . What is the chance of that happening on an odd-numbered toss Solution Suppose we let P ( k ) denote the probability that the rst head appears on the k th toss . Since the coin wa presumed to be fair , P ( 1 ) = 1 2 . Furthermore , we would expect half of the coin that showed a tail on the rst toss to come up head on the second , so , intuitively , P ( 2 ) = 1 4 . In general , P ( k ) = 1 2 k , A. K. Khandani , ECE307Fall 2024 65 k = 1 , 2 , . . . . Let E be the event rst head appears on an odd-numbered toss . Then P ( E ) = P ( 1 ) + P ( 3 ) + P ( 5 ) + = X i =0 1 2 2 i +1 = 1 2 X i =0 1 4 i Recall the formula for the sum of a geometric series If 0 < x < 1 , X k =0 x k = 1 1 x Applying that result to P ( E ) give P ( E ) = 1 2 1 1 1 4 = 2 3 A similar computation would show that the probability of the rst head appearing on an even-numbered toss is 1 3 . 17 . Two gambler , A and B , each choose an integer from 1 to m ( inclusive ) at random . What is the probability that the two number they pick do not dier by more than n Solution It will be easier if we approach this problem via it complement . Let x and y denote the number selected by A and B , respectively . The complement ha two case , depending on whether x < y or x > y . Let u rst suppose that x < y . Then , for a given x , the value of y such that y x > n are y = x + n + 1 , y = x + n + 2 , . . . , and y = m , altogether , a range of m n x choice . Summing over x , we nd that the total number of ( x , y ) -pairs such that y x > n reduces to the sum of the rst m n 1 integer m n 1 X x =1 ( m n x ) = m n 1 X i =1 i = ( m n 1 ) ( m n ) 2 By symmetry , the same number of ( x , y ) -pairs satises the second case x > y and x y > n . Thus the total number of ( x , y ) -selections such that x y > n is ( m n 1 ) ( m n ) . The sample space S contains m 2 outcome , all equally likely by assumption . It follows that P ( x y n ) = 1 ( m n 1 ) ( m n ) m 2 18 . A secretary is upset about having to stuenvelopes . Handed a box of n letter and n envelope , he vent his frustration by putting the letter into the envelope at random . How many people , on the average , will receive the correct mail Solution If X denotes the number of envelope properly stued , what we are seeking is E ( X ) . Let X i denote a random variable equal to the number of correct letter put into the i th envelope , i = 1 , 2 , . . . , n . Then X i equal 0 or 1 , and f X i ( k ) = P ( X i = k ) = 1 n for k =1 n 1 n for k =0 A. K. Khandani , ECE307Fall 2024 66 But X = X 1 + X 2 + + X n and E ( X ) = E ( X 1 ) + E ( X 2 ) + + E ( X n ) . Furthermore , each of the X i s ha the same expected value , 1 /n E ( X i ) = 1 X k =0 k P ( X i = k ) = 0 n 1 n + 1 1 n = 1 n It follows that E ( X ) = n X i =1 E ( X i ) = n 1 n = 1 showing that , regardless of n , the expected number of properly stued envelope is 1 . 19 . The honor count in a ( 13-card ) bridge hand can vary from 0 to 37 according to the formula Honor count = 4 number of ace + 3 number of king + 2 number of queen + 1 number of jack What is the expected honor count of Norths hand Solution If X i , i =1,2,3,4 , denotes the honor count for North , South , East , and West , respectively , and if X denotes the analogous sum for the entire deck , we can write X = X 1 + X 2 + X 3 + X 4 But X = E ( X ) = 4 4 + 3 4 + 2 4 + 1 4 = 40 By symmetry , E ( X i ) = E ( X j ) , i = j , so it follows that 40 = 4 E ( X 1 ) , which implies that 10 is the expected honor count of Norths hand . 20 . Let X 1 , X 2 , . . . , X n denote a set of n independent observation made on a random variable X having pdf f X ( x ) . Let 2 = E ( X ) 2 denote the variance of X . The sample variance of the X i s , denoted by S 2 , is dened a S 2 = 1 n 1 n X i =1 ( X i X ) 2 where X = 1 n n X i =1 X i . Show that E ( S 2 ) = 2 . Solution We know that if X is a random variable having mean and E ( X 2 ) nite , then V ar ( X ) = E ( X 2 ) 2 Hence , we rewrite S 2 in a form that enables u to apply the above equation E ( S 2 ) = E 1 n 1 n X i =1 ( X i X ) 2 = E 1 n 1 n X i =1 ( X 2 i 2 X i X + X 2 ) = E 1 n 1 n X i =1 X 2 i 2 X n X i =1 X i + nX 2 = 1 n 1 n X i =1 E ( X 2 i ) 2 E X n X i =1 X i + nE ( X 2 ) A. K. Khandani , ECE307Fall 2024 67 But , we have E ( X 2 i ) = 2 + 2 and , E X n X i =1 X i = 1 n n X i =1 n X j =1 E ( X i X j ) = 1 n h ( n 2 n ) 2 + n ( 2 + 2 ) i = n 2 + 2 and , E ( X 2 ) = 1 n 2 n X i =1 n X j =1 E ( X i X j ) = 1 n 2 h ( n 2 n ) 2 + n ( 2 + 2 ) i = 1 n ( n 2 + 2 ) = 2 + 2 n Therefore E ( S 2 ) = 1 n 1 h n ( 2 + 2 ) 2 n 2 2 2 + n 2 + 2 i = 1 n 1 ( n 1 ) 2 = 2 21 . A chip is to be drawn at random from each of k urn , each holding n chip numbered 1 through n . What is the probability all k chip will bear the same number Solution If X 1 , X 2 , . . . , X k denote the number on the 1st , 2nd , ... , k th chip , respectively , we have , P ( X i = ) = 1 n , X i X 1 , . . . , X k , 1 , n We are looking for the probability that X 1 = X 2 = = X k . Considering the independence of the event , we obtain , P ( X 1 = X 2 = = X k = ) = 1 n k , 1 , n Adding up these value for dierent value of , we obtain , P ( X 1 = X 2 = = X k ) = 1 n k 1 22 . Mean of a Hypergeometric Random Variable If n ball are randomly selected from an urn containing N white and M black ball , nd the expected number of white ball selected . Solution Let X denote the number of white ball selected . It follows that P ( X = k ) = N k M n k N + M n Hence , assuming n N E X = n X k =0 k N k M n k N + M n However , we can obtain a simpler expression for E X by using the representation X = X 1 + + X n A. K. Khandani , ECE307Fall 2024 68 where X i = 1 if the i th ball selected is white 0 otherwise Since the i th ball selected is equally likely to be any of the N + M , we have E X i = N M + N and thus E X = E X 1 + + E X N = nN M + N 23 . Six ball are tossed independently into three box A , B , C . For each ball the probability of going into a specic box is 1/3 . Find the probability that box A will contain ( a ) exactly four ball , ( b ) at least two ball , ( c ) at least ve ball . Solution Here we have six Bernoulli trial , with success corresponding to a ball in box A , failure to a ball in box B or C . Recall , a sequence of n Bernoulli trial is a sequence of n independent observation each of which may result in exactly one of two possible situation , called success or failure . At each observation the probability of success is p and the probability of failure is q = 1 p . Thus , referring the our problem we have that n = 6 , p = 1/3 , q = 2/3 , and so the required probability are ( a ) p ( 4 ) = 6 4 1 3 4 2 3 2 ( b ) 1 p ( 0 ) p ( 1 ) = 1 2 3 6 6 1 1 3 2 3 5 ( c ) p ( 5 ) + p ( 6 ) = 6 5 1 3 5 2 3 + 1 3 6 24 . A bubble gum company is printing a special series of baseball card featuring r of the greatest base-stealers of the past decade . Each player appears on the same number of card , and the card are randomly distributed to retail outlet . If a collector buy n ( r ) packet of gum ( each containing one card ) , what is the probability she get a complete set of the special series Solution Let A i be the event the collector ha no card for player i , i = 1 , 2 , . . . , r , and dene A to be the union , A = r i =1 A i . Then P ( collector ha at least one card of each player ) = 1 P ( A ) To begin the derivation of P ( A ) , notice that for any value of k , k = 1 , 2 , . . . , r , P ( A 1 A 2 A k ) = 1 k r n Therefore , from the general Addition Law , P r i =1 A i = P ( A ) = r X i =1 1 1 r n X i < j 1 2 r n + X i < j < k 1 3 r n + ( 1 ) r +1 0 = r 1 1 1 r n r 2 1 2 r n + r 3 1 3 r n + ( 1 ) r +1 0 A. K. Khandani , ECE307Fall 2024 69 Or more concisely , P ( A ) = r X k =1 ( 1 ) k +1 r k 1 k r n 25 . An urn contains nine chip , ve red and four white . Three are drawn out at random without replacement . Let X denote the number of red chip in the sample . Find E ( X ) , the expected number of red chip selected . Solution We recognize X to be a Hypergeometric random variable , where P ( X = x ) = 5 x 4 3 x 9 3 , x = 0 , 1 , 2 , 3 Hence , E ( X ) = X all x x P ( X = x ) = 3 X i =0 x 5 x 4 3 x 9 3 = ( 0 ) 4 84 + ( 1 ) 30 84 + ( 2 ) 40 84 + ( 3 ) 10 84 = 5 3 26 . The following problem wa posed and solved in the eighteenth century by Daniel Bernoulli . Suppose that a jar contains 2 N card , two of them marked 1 , two marked 2 , two marked 3 , and so on . Draw out m card at random . What is the expected number of pair that still remain in the jar Solution Dene , i = 1 , 2 , . . . , N X i = 1 if the i th pair remains in the jar 0 otherwise Now , E X i = P ( X i = 1 ) = 2 N 2 m 2 N m = ( 2 N 2 ) m ( 2 N 2 m ) ( 2 N ) m ( 2 N m ) = ( 2 N m ) ( 2 N m 1 ) ( 2 N ) ( 2 N 1 ) Hence the desired result is E X 1 + X 2 + + X N = E X 1 + + E X N = ( 2 N m ) ( 2 N m 1 ) 2 ( 2 N 1 ) A. K. Khandani , ECE307Fall 2024 70 27 . Consider a jury trial in which it take 8 of the 12 juror to convict that is , in order for the defendant to be convicted , at least 8 of the juror must vote him guilty . If we assume that juror act independently and each make the right decision with probability , what is the probability that the jury render a correct decision Solution The problem , a stated , is incapable of solution , for there is not yet enough information . For instance , if the defendant is innocent , the probability of the jury rendering a correct decision is 12 X i =5 12 i i ( 1 ) 12 i whereas , if he is guilty , the probability of a correct decision is 12 X i =8 12 i i ( 1 ) 12 i Therefore , if represents the probability that the defendant is guilty , then , by conditioning on whether or not he is guilty , we obtain that the probability that the jury render a correct decision is 12 X i =8 12 i i ( 1 ) 12 i + ( 1 ) 12 X i =5 12 i i ( 1 ) 12 i 28 . A single unbiased die is tossed independently n time . Let R 1 be the number of 1 obtained , and R 2 the number of 2 . Find E ( R 1 R 2 ) . Solution The indicator of an event A is a random variable I A and is dened a follows . I A ( ) = 1 if A 0 if A If A i is the event that the i th toss result in a 1 , and B i the event that the i th toss result in a 2 , then R 1 = I A 1 + + I A n R 2 = I B 1 + + I B n Hence E ( R 1 R 2 ) = n X i , j =1 E ( I A i I B j ) Now if i = j , I A i and I B j are independent hence E ( I A i I B j ) = E ( I A i ) E ( I B j ) = P ( A i ) P ( B j ) = 1 36 If i = j , A i and B j are disjoint , since the i th toss can not simultaneously result in a 1 and a 2 . Thus I A i I B i = I A i B i = 0 . Thus E ( R 1 R 2 ) = n ( n 1 ) 36 since there are n ( n 1 ) ordered pair ( i , j ) of integer 1 , 2 , . . . , n such that i = j . 29 . A miner is trapped in a mine containing 3 door . The rst door lead to a tunnel that will take him to safety after 3 hour of travel . The second door lead to a tunnel that will return him to the mine after 5 hour of travel . The third door lead to a tunnel that will return him to the mine after 7 hour . If we assume that A. K. Khandani , ECE307Fall 2024 71 the miner is at all time equally likely to choose any one of the door , what is the expected length of time until he reach safety Solution Let X denote the amount of time ( in hour ) until the miner reach safety , and let Y denote the door he initially chooses . Now E X = E X Y = 1 P ( Y = 1 ) + E X Y = 2 P ( Y = 2 ) + E X Y = 3 P ( Y = 3 ) = 1 3 ( E X Y = 1 + E X Y = 2 + E X Y = 3 ) However , E X Y = 1 = 3 E X Y = 2 = 5 + E X E X Y = 3 = 7 + E X To understand why the above equation is correct , consider , for instance , E X Y = 2 and reason a follows . If the miner chooses the second door , he spends 5 hour in the tunnel and then return to his cell . But once he return to his cell the problem is a before thus his expected additional time until safety is just EX . Hence E X Y = 2 = 5 + E X . The argument behind the other equality in the above equation is similar . Hence E X = 1 3 ( 3 + 5 + E X + 7 + E X ) or E X = 15 A. K. Khandani , ECE307Fall 2024 72",
    "5": "Continuous Random Variables 5.1 Introduction In Chapter 4 we considered discrete random variable , that is , random variable whose set of possible value is either nite or countably innite . However , there also exist random variable whose set of possible value is uncountable . Two example would be the time that a train arrives at a specied stop and the lifetime of a transistor . Denition We say that X is a continuous random variable if there exists a nonnegative function f , dened for all real x ( , ) , having the property that for any set B of real number P X B = Z B f ( x ) dx The function is called the probability density function ( pdf ) of the random variable X . Since X must assume some value , f must satisfy 1 = P X ( , ) = Z f ( x ) dx Relation of CDF and pdf P X < a = P X a = F ( a ) = Z a f ( x ) dx A. K. Khandani , ECE307Fall 2024 73 5.2 Expectation and Variance of Continuous Random Variables Expected Value The expected value of the discrete random variable X , which we denote by E ( X ) , is given by E ( X ) = Z xf ( x ) dx Expectation of a Function of a Continuous Random Variable If X is a continuous random variable with probability density function f ( x ) , then for any real-valued function g , E ( g ( X ) ) = Z g ( x ) f ( x ) dx Lemma For a nonnegative random variable Y , E ( Y ) = Z 0 P Y > y dy Corollary If a and b are constant , then E aX + b = aE X + b A. K. Khandani , ECE307Fall 2024 74 5.3 The Uniform Random Variable Uniform Random Variable We sat that X is a uniform random variable on the interval ( , ) if it probability density function is given by f ( x ) = 1 if < x < 0 otherwise Since F ( a ) = Z a f ( x ) dx , F ( a ) = 0 a a if < a < 1 a For a uniformly distributed random variable X over ( , ) E X = + 2 and Var ( X ) = ( ) 2 12 A. K. Khandani , ECE307Fall 2024 75 5.4 Normal Random Variables Normal Random Variable We say that X is a normal random variable , or simply that X is normally distributed , with parameter and 2 if the density of X is given by f ( x ) = 1 2 e ( x ) 2 / 2 2 For a normally distributed random variable X with parameter and 2 , E X = and Var ( X ) = 2 If X ha the pdf N ( , 2 ) , then Y = aX + b ha the pdf f Y ( y ) = 1 2 exp ( 1 2 2 y b a 2 ) 1 a , < y < = 1 2 a exp ( y a b ) 2 2 a 2 2 , < y < which is the pdf of N ( a + b , a 2 2 ) . Note that a linear transformation of a normal random variable result in a normal random variable . A. K. Khandani , ECE307Fall 2024 76 Standard Normal Random Variable Standard normal random variable is a normal random variable with parameter ( 0 , 1 ) . That is , f Z ( z ) = 1 2 e z 2 / 2 It is traditional to denote the cumulative distribution function of a standard normal random variable by ( x ) . That is , ( x ) = 1 2 Z x e y 2 / 2 dy For any normal random variable X with parameter ( , 2 ) , Z = X is a normal random variable and it cumulative distribution function can be written a F X ( a ) = P X a = P X a = a 5.4.1 The Normal Approximation to the Binomial Distribution The DeMoivre-Laplace Limit Theorem If S n denotes the number of success that occur when n indepen- dent trial , each resulting in a success with probability p , are performed then , for any a < b , P ( a S n np p np ( 1 p ) b ) ( b ) ( a ) A. K. Khandani , ECE307Fall 2024 77 5.5 Exponential Random Variable Exponential Random Variable A continuous random variable whose probability density function is given , for some > 0 , by f ( x ) = e x if x 0 0 if x < 0 is said to be an exponential random variable ( or , more simply , is said to be exponentially distributed ) with parameter . The cumulative distribution F ( a ) of an exponential random variable is given by F ( a ) = P X a = Z a 0 e x dx = e x a 0 = 1 e a a 0 For an exponential random variable X with parameter , E X = 1 and Var ( X ) = 1 2 A. K. Khandani , ECE307Fall 2024 78 Memoryless Random Variable We say that a nonnegative random variable X is memoryless if P X > s + t X > t = P X > s for all s , t 0 Since the above equation is satised when X is exponentially distributed , it follows that exponentially distributed random variable are memoryless . 5.5.1 Hazard Rate Functions Hazard Rate Function Consider a positive continuous random variable X that we interpret a being the lifetime of some item , having distribution function F and density f . The hazard rate ( sometimes called the failure rate function ( t ) of F is dened by ( t ) = f ( t ) F ( t ) F ( t ) = 1 F To interpret ( t ) , suppose that the item ha survived for a time t and we desire the probability that it will not survive for an additional time dt . That is , consider P X ( t , t + dt ) X > t . Now P X ( t , t + dt ) X > t = P X ( t , t + dt ) , X > t P X > t = P X ( t , t + dt ) P X > t f ( t ) F ( t ) dt Thus , ( t ) represents the conditional probability intensity that a t -unit-old item will fail . For an exponentially distributed random variable , the hazard rate function is constant . A. K. Khandani , ECE307Fall 2024 79 5.6 Other Continuous Distributions 5.6.1 The Gamma Distribution The Gamma Distribution A continuous random variable is said to have a gamma distribution with parameter ( , ) , > 0 , and > 0 if it density function is given by f ( x ) = e x ( x ) 1 ( ) if x 0 0 if x < 0 where ( ) , called the gamma function , is dened a ( ) = Z 0 e y y 1 dy The integration by part of ( ) yield that ( ) = ( 1 ) ( 1 ) For integer value of n ( n ) = ( n 1 ) A. K. Khandani , ECE307Fall 2024 80 When is a positive integer , say = n , the gamma distribution with parameter ( , ) often arises a the distribution of the amount of time one ha to wait until a total of n event ha occurred , when the condition told for Poisson distribution are valid . Let T n denote the time at which the n th event occurs , and note T n is less then or equal to t if and only if the number of event that have occurred by time t is at least n . That is , when N ( t ) equal to the number of event in 0 , t , P T n t = P N ( t ) n = X j = n P N ( t ) = j = X j = n e t ( t ) j j where the nal identity follows because the number of event in 0 , t ha a Poisson distribution with parameter t . Dierentiation of the above yield that the density function of T n is a follows f ( t ) = X j = n e t j ( t ) j 1 j X j = n e t ( t ) j j = X j = n e t ( t ) j 1 ( j 1 ) X j = n e t ( t ) j j = e t ( t ) n 1 ( n 1 ) Note that when n = 1 , this distribution reduces to the exponential . For a gamma distributed random variable we have , E X = and Var ( X ) = 2 A. K. Khandani , ECE307Fall 2024 81 5.6.2 The Weibull Distribution The Weibull Distribution The Weibull distribution function ha the form F ( x ) = 0 x 1 exp ( x ) x > Dierentiation yield the density is f ( x ) = 0 x x 1 exp ( x ) x > 5.6.3 The Cauchy Distribution The Cauchy Distribution The Cauchy density function ha the form f ( x ) = 1 1 1 + ( x ) 2 A. K. Khandani , ECE307Fall 2024 82 5.6.4 The Beta Distribution The Beta Distribution The beta density function ha the form f ( x ) = 1 B ( a , b ) x a 1 ( 1 x ) b 1 0 < x < 1 0 otherwise where B ( a , b ) = Z 1 0 x a 1 ( a x ) b 1 dx The following relationship exists between B ( a , b ) and gamma function B ( a , b ) = ( a ) ( b ) ( a + b ) For a beta distributed random variable , E X = a a + b and Var ( X ) = ab ( a + b ) 2 ( a + b + 1 ) 5.7 The Distribution of a Function of a Random Variable Theorem Let X be a continuous random variable having probability density function f X . Suppose that g ( x ) is a strictly monotone ( increasing or decreasing ) , dierentiable ( and thus continuous ) function of x . Then the random variable Y dened Y = g ( X ) ha a probability density function given by f Y ( y ) = f X g 1 ( y ) d dy g 1 ( y ) if y = g ( x ) for some x 0 if y = g ( x ) for all x A. K. Khandani , ECE307Fall 2024 83 5.8 Some Solved Problems 1 . Consider the probability density f X ( x ) = a e b x , where X is a random variable whose allowable value range from x = to x = + . Find ( a ) the cumulative distribution function F X ( x ) , ( b ) the relationship between a and b , and ( c ) the probability that the outcome X lie between 1 and 2 . Solution ( a ) The cumulative distribution function is F ( x ) = P ( X x ) = Z x f ( x ) dx = Z x a e b x dx = a b e bx x 0 1 2 + a b ( 1 e bx ) x 0 ( b ) In order that f ( x ) be a probability density , it is necessary that Z + f ( x ) dx = Z + a e b x dx = 2 a b = 1 so that a b = 1 2 . ( c ) The probability that X lie in the range between 1 and 2 is P ( 1 X 2 ) = b 2 Z 2 1 e b x dx = 1 2 ( e b e 2 b ) 2 . A certain retailer for a petroleum product sell a random amount , X , each day . Suppose that X , measured in hundred of gallon , ha the probability density function f X ( x ) = ( 3 / 8 ) x 2 0 x 2 0 elsewhere . The retailer prot turn out to be 5 for each 100 gallon sold ( 5 cent per gallon ) if X 1 and 8 per 100 gallon if X > 1 . Find the retailer expected prot for any given day . Solution Let g ( X ) denote the retailer daily prot . Then , g ( X ) = 5 X , 0 x 1 8 X , 1 < x 2 We want to nd expected prot , and E g ( X ) = Z g ( x ) f ( x ) dx = Z 1 0 5 x 3 8 x 2 dx + Z 2 1 8 x 3 8 x 2 dx = 15 ( 8 ) ( 4 ) x 4 1 0 + 24 ( 8 ) ( 4 ) x 4 2 1 = 15 32 ( 1 ) + 24 32 ( 15 ) = ( 15 ) ( 25 ) 32 = 11 . 72 3 . Let X have the probability density function given by f X ( x ) = 2 x 0 < x < 1 0 elsewhere . A. K. Khandani , ECE307Fall 2024 84 ( a ) Find the density function of U = 3 X 1 . ( b ) Find the density function of U = 4 X + 3 . Solution We know that for a given pdf , say f X ( x ) , the probability that X is in a neighborhood of x around x is given by f X ( x ) x . In this problem , the value of U is uniquely determined by the value of X . Then , the probability that U is in a neighborhood of u around u , i.e. , f U ( u ) u , is equal to the probability that X is in a neighborhood of x around x = h 1 ( u ) where h ( x ) = 3 x 1 . This mean that f U ( u ) u = f X h 1 ( u ) x = f U ( u ) = f X h 1 ( u ) x u . Here , we have , x = h 1 ( u ) = u + 1 3 and x u = dx du = 1 3 Thus , f U ( u ) = f X h 1 ( u ) dx du = 2 xdx du = 2 u + 1 3 1 3 = 2 ( u + 1 ) 9 1 < u < 2 = 0 elsewhere The range of u ( i.e. , the range over which f U ( u ) is positive ) is simply the interval 0 < x < 1 transformed to the u -axis by the function u = 3 x 1 . This result in u 1 , 2 . For u = h ( x ) = 4 x + 3 , we have , x = h 1 ( u ) = 3 u 4 and dx du = 1 4 As the function h ( x ) is decreasing in x , we have x/ u = dx/du ( note that x and u are always positive but dx and du may be positive or negative ) and we can write , f U ( u ) = f X h 1 ( u ) dx du = 2 x dx du = 2 3 u 4 1 4 = 3 u 8 1 < u < 3 = 0 elsewhere The range of u is equal to , u 1 , 3 . 4 . Consider a random variable X with the following pdf f X ( x ) = 1 a x , x 1 /a A. K. Khandani , ECE307Fall 2024 85 ( a ) Find the constant a and compute the mean and the standard deviation of X . ( b ) The random variable X is applied to a full-wave rectier whose output-input gain characteristic is y = b x . Determine the mean and standard deviation of the output random variable . ( c ) The random variable X is applied to a half-wave rectier whose output-input gain characteristic is y = b x , x 0 and y = 0 , x < 0 . Determine the mean and standard deviation of the output random variable . Solution The pdf of X is shown in the following gure . 1 1 /a 1 /a Figure 7 Pdf related to above Problem . We should have , Z f X ( x ) dx = Z 1 /a 1 /a f X ( x ) = 1 This result in a = 1 . We also have E ( X ) = 0 and , X = q E ( X 2 ) = q 1 / 6 In part 2 and 3 , we want to compute the average value of a function of a random variable . Given a random variable X , the general formula for the average value of the function H ( X ) is , E H ( X ) = Z H ( X ) f X ( x ) dx For the case of full wave rectier , the function is equal to , H ( X ) = b X . This result in , E ( Y ) = E ( b X ) = b Z 0 1 ( x ) f X ( x ) dx + Z 1 0 xf X ( x ) dx = 2 b Z 1 0 x ( 1 x ) dx = b 3 and , similarly , E ( Y 2 ) = 2 b 2 Z 1 0 x 2 ( 1 x ) dx = b 2 6 This result in Y = p E ( Y 2 ) E ( Y ) 2 = p b 2 / 18 . For the case of half wave rectier , we note that the output is in part continuous and in part discrete ( note that all the negative value of X are mapped to zero ) . This mean that the output of the half-rectier is with probability 1 / 2 equal to zero , while for value greater than zero , it obeys the pdf of X . This result in , E ( Y ) = ( 0 1 / 2 ) + Z 1 0 bx ( 1 x ) dx = b 6 A. K. Khandani , ECE307Fall 2024 86 and E ( Y 2 ) = ( 0 2 1 / 2 ) + Z 1 0 b 2 x 2 ( 1 x ) dx = b 2 12 This result in Y = p E ( Y 2 ) E ( Y ) 2 = p b 2 / 18 . 5 . The random variable X of the life length of certain kind of battery ( in hundred of hour ) is equal to f ( x ) = 1 2 e x/ 2 x > 0 0 otherwise ( i ) Find the probability that the life of a given battery is less than 200 or greater than 400 hour . ( ii ) Find the probability that a battery of this type last for 300 hour if we know that it ha already been in use for 200 hour . Solution ( i ) Let A denote the event that X is less than 2 and B the event that X is greater than 4 . Then , because A and B are mutually exclusive , P ( A B ) = P ( A ) + P ( B ) = Z 2 0 1 2 e x/ 2 dx + Z 4 1 2 e x/ 2 dx = ( 1 e 1 ) + e 2 = 1 0 . 368 + 0 . 135 = 0 . 767 ( ii ) We are interested in P ( X > 3 X > 2 ) and , by the denition of conditional probability , P ( X > 3 X > 2 ) = P ( X > 3 ) P ( X > 2 ) because the intersection of the event ( X > 3 ) and ( X > 2 ) is the event ( X > 3 ) . Now P ( X > 3 ) P ( X > 2 ) = Z 3 1 2 e x/ 2 dx Z 2 1 2 e x/ 2 dx = e 3 / 2 e 1 = e 1 / 2 = 0 . 606 6 . The failure of a circuit board interrupt work by a computer system until a new board is delivered . Delivery time X is uniformly distributed over the interval 1 to 5 day . The cost C of this failure and interruption consists of a xed cost c 0 for the new part and a cost that increase proportional to X 2 , i.e , C = c 0 + c 1 X 2 ( a ) Find the probability that the delivery time take 2 or more day . ( b ) Find the expected cost of a single failure , in term of c 0 and c 1 . Solution ( a ) The delivery time X is distributed uniformly from 1 to 5 day , which give f ( x ) = 1 4 1 x 5 0 elsewhere Thus , P ( X 2 ) = Z 5 2 1 4 dx = 1 4 ( 5 2 ) = 3 4 A. K. Khandani , ECE307Fall 2024 87 ( b ) We know that E ( C ) = c 0 + c 1 E ( X 2 ) so it remains to nd E ( X 2 ) . This could be found directly from the denition or by using the variance and the fact that E ( X 2 ) = V ( X ) + 2 Using the latter approach , E ( X 2 ) = ( b a ) 2 12 + a + b 2 2 = ( 5 1 ) 2 12 + 1 + 5 2 2 = 31 3 Thus , E ( C ) = c 0 + c 1 31 3 7 . Let X denote the life time ( in hundred of hour ) of a certain type of electronic component . These component frequently fail immediately upon insertion into the system . It ha been observed that the probability of immediate failure is 1/4 . If a component doe not fail immediately , the life-length distribution ha the exponential density f ( x ) = e x x > 0 0 elsewhere Find the distribution function for X and evaluate P ( X > 10 ) . Solution There is only one discrete point , X = 0 , and this point ha probability 1/4 . It follows that X is a mixture of two random variable , X 1 and X 2 , where X 1 ha a probability of one at the point zero and X 2 ha the given exponential density . That is , F 1 ( x ) = 0 if x < 0 1 if x 0 and F 2 ( x ) = Z x 0 e y dy = 1 e x x > 0 Now , F ( x ) = 1 4 F 1 ( x ) + 3 4 F 2 ( x ) Hence , P ( X > 10 ) = 1 P ( X 10 ) = 1 F ( 10 ) = 1 1 4 + 3 4 ( 1 e 10 ) = 3 4 1 ( 1 e 10 ) = 3 4 e 10 A. K. Khandani , ECE307Fall 2024 88 8 . Suppose X ha density function f ( x ) for 1 x 1 , 0 otherwise . Find the density function of ( a ) Y = X , ( b ) Z = X 2 . Solution ( a ) For y > 0 , P ( Y y ) = P ( y X y ) = F ( y ) F ( y ) . Dierentiating the cdf , we conclude that the pdf of Y is f ( y ) + f ( y ) for 0 < y < 1 and 0 elsewhere . You may also try to use the following relationship f Y ( y ) = X x i f X ( x i ) dy dx ( b ) For z > 0 , P ( Z z ) = P ( z X z ) = F ( z ) F ( z ) . Hence the pdf of Z is f ( z ) + f ( z ) ( 1 2 z 1 / 2 ) , for z > 0 and 0 otherwise . 9 . Suppose X is uniform on ( 0,1 ) . Find the density function of Y = X n . Solution Suppose X ha density f X and P ( a < X < b ) = 1 . Let Y = r ( X ) . Suppose r ( a , b ) ( , ) is continuous and strictly increasing , and let s ( , ) ( a , b ) be the inverse of r . Then , we know that Y ha density f Y ( y ) = f X s ( y ) s ( y ) for y ( , ) Therefore , since X ha density function f X ( x ) = 1 for 0 < x < 1 , and r ( x ) = x n ha inverse s ( x ) = x 1 /n , the theorem give the density function f X ( y 1 n ) 1 n y 1 n 1 = 1 n y 1 n 1 for 0 < y < 1 . 10 . Suppose X is uniform on ( 0 , / 2 ) and Y = sin X . Find the density function of Y . The answer is called the arcsine law because the distribution function contains the arcsine function . Solution The density function of X is f X ( x ) = 2 / for 0 < x < / 2 and r ( x ) = sin x ha the inverse s ( x ) = sin 1 x . Using the same result a in the previous problem , we obtain the density function f X s ( y ) s ( y ) = 2 1 p 1 y 2 for 0 < y < 1 . 11 . Suppose X ha density function 3 x 4 for x 1 . ( a ) Find a function g so that g ( X ) is uniform on ( 0,1 ) . ( b ) Find a function h so that if U is uniform on ( 0,1 ) , h ( U ) ha density function 3 x 4 for x 1 . Solution Suppose X ha a continuous distribution . Then Y = F X ( X ) is uniform on ( 0,1 ) . ( a ) P ( X x ) = Z x 1 3 y 4 dy = 1 x 3 for x > 1 and 0 elsewhere . The above statement tell that Y = g ( X ) = 1 X 3 is uniform on ( 0,1 ) . Suppose U ha a uniform distribution on ( 0,1 ) . Then Y = F 1 ( U ) ha distribution function F . ( b ) F ( x ) ha inverse F 1 ( x ) = ( 1 x ) 1 3 . The above statement say that F 1 ( U ) = ( 1 U ) 1 3 ha the given density function . 12 . A Gaussian distributed random variable X with zero mean and the unit variance is applied to a full-wave rectier whose output-input gain characteristic is y = x /a , a > 0 . Determine the pdf of the output random variable Y . The mapping is one-to-one for x < 0 and one-to-one for x > 0 . In both case , y > 0 . Using the equation f Y ( y ) = f X ( x ) dy/dx A. K. Khandani , ECE307Fall 2024 89 we have for x > 0 f Y ( y ) = a 1 2 e x 2 / 2 x = ay = a 2 e a 2 y 2 / 2 y > 0 and for x < 0 f Y ( y ) = a 1 2 e x 2 / 2 x = ay = a 2 e a 2 y 2 / 2 y > 0 Adding these two result , we obtain , f Y ( y ) = a r 2 e a 2 y 2 / 2 u ( y ) . We could equivalently use the following relationship f Y ( y ) = X k f X ( x k ) dg/dx k x k = g 1 ( y ) where g ( x ) = x /a . 13 . The random variable X of the previous problem is applied to the half-wave rectier whose output-input characteristic is y = ( x/a ) u ( x ) . Determine the pdf of the output . Solution For x > 0 , it can easily be shown that f y ( y ) = a 2 e a 2 y 2 / 2 , y > 0 For x < 0 , however all point of the input are mapped into zero in the output . To conserve probability we must add a contribution of R 0 f X ( x ) dx = 1 / 2 at the point y = 0 so that f y ( y ) = a 2 e a 2 y 2 / 2 u ( y ) + 1 2 ( y ) 14 . Suppose X ha density x 2 for x 1 and Y = X 2 . Find the pdf of Y . Solution Noting P ( Y x ) = P ( X x 1 2 ) = 1 F ( x 1 2 ) for x 1 lead to the density function of Y by dierentiating d dx P ( Y x ) = F ( x 1 2 ) ( 1 2 x 3 / 2 ) = 1 2 x 1 / 2 , forx 1 15 . The actual weight of a bag of sugar is assumed to be a normal random variable with mean 202 gram and standard deviation of 3 gram . If a bag weighs less than 199 gram or more than 205 gram it is rejected . ( i ) What is the probability that a bag will be rejected ( ii ) Given that a bag wa rejected , what is the probability it weighs less than 195 gram ( iii ) If the standard deviation of the lling process is changed to , but the mean remains at 202 gram , what is the largest value that can have so that the probability a bag is rejected is less than .01 Give numerical answer . Solution ( i ) P ( bag rejected ) = 1 P ( 199 < X < 205 ) = 1 P ( 199 202 3 < X 202 3 < 205 202 3 ) = 1 ( ( 1 ) ( 1 ) ) = 0 . 84134 0 . 15866 = 0 . 31732 . ( ii ) Let A = bag rejected , B = X < 195 , then P ( B A ) = P ( AB ) /P ( A ) = P ( X < 195 ) /P ( A ) = ( 195 202 3 ) / 0 . 31732 = ( 7 / 3 ) / 0 . 31732 = 0 . 00982 / 0 . 31732 . = 0 . 0309 . A. K. Khandani , ECE307Fall 2024 90 ( iii ) P ( bag rejected ) = 1 ( 3 / ) ( 3 / ) = 2 ( 1 ( 3 / ) = . 01 or ( 3 / ) = . 995 and 3 / = 2 . 575 and = 1 . 165 . 16 . The error in a linear measurement , is assumed to be a normal random variable with mean 0 and variance 2 , in mm 2 . ( i ) What is the largest value of allowable if P ( X < 2 ) is to be at least 0.90 ( ii ) If = 2 evaluate P ( X > 4 X < 5 ) . Solution ( i ) P ( X < 2 ) = ( 2 / ) ( 2 / ) = 2 ( 2 / ) = 0 . 9 or ( 2 / ) = 0 . 95 and 2 / = 1 . 65 and = 1 . 212 . ( ii ) P ( X > 4 X < 5 ) = P ( X > 4 and X < 5 ) /P ( X < 5 ) = ( 5 / 2 ) ( 4 / 2 ) ( 5 / 2 ) ( 5 / 2 ) = 0 . 0167 . 17 . The projection of a point chosen at random on the circumference of a circle of radius a onto a xed diameter , ha the cdf F X ( x ) = 1 x a 1 2 + 1 arcsin x a a x a 0 otherwise ( i ) Determine the probability that X will be on the interval ( a/ 2 , a/ 2 ) ( ii ) Find the probability density function of X ( iii ) Find the mode and the median of the distribution . Solution ( i ) P ( a 2 < X < a 2 ) = F X ( a 2 ) F X ( a 2 ) = 2 arcsin ( 1 2 ) = 1 3 . The density function is given by f X ( x ) d dx F X ( x ) = d dx ( 1 2 + 1 arcsin ( x a ) ) = 1 ( a 2 x 2 ) 1 / 2 , a x a and zero otherwise . The mode of the distribution is the point at which the pdf achieves it maximum value . This distribution ha no mode . The median is the point at which F X ( x ) = 1 / 2 , which is x = 0 . 18 . The signal strength in volt at the input to an antenna , is a random variable with cdf F ( x ) = 1 e x 2 /a , x 0 , a > 0 . ( i ) Find the probability density function , mean and variance of X . ( ii ) If ten independent sample of the signal strength are taken , what is the probability that exactly 7 of them will be greater than 2 volt A. K. Khandani , ECE307Fall 2024 91 Solution ( i ) The probability density function is f X ( x ) = d dx F X ( x ) = 2 x a e x 2 /a , x 0 E ( X ) = 2 a Z 0 x 2 e x 2 /a dx = a 2 E ( X 2 ) = 2 a Z 0 x 3 e x 2 /a dx = a E ( X ) = a ( 1 4 ) ( ii ) The probability that one sample is greater than 2 is p = exp ( 4 /a ) = 1 F X ( 2 ) and so P ( 7 out of 10 greater than 2 ) = 10 7 p 7 ( 1 p ) 3 . 19 . The random variable X is normal with mean 81 and variance 16 while Y is normal with mean 85 and variance 4 . Which random variable is more likely to be less than 88 Solution P ( X 88 ) = P ( X 81 4 7 4 ) = ( 7 4 ) P ( Y 88 ) = P ( Y 85 2 3 2 ) = ( 3 2 ) and hence X is more likely to be less than 88 . 20 . The lifetime of an electronic component is a random variable which ha an exponential pdf . If 50 of the component fail before 2,000 hour , what is the average lifetime of the component Solution The pdf is of the form exp ( x ) and P ( X 2000 ) = 1 exp ( 2 , 000 ) = 0 . 5 Thus = ln ( 0 . 5 ) / 2 , 000 and the average lifetime of the component is 1 / = 2 , 000 / ln ( 2 ) . 21 . The probability density function of a random variable X is f X ( x ) = ax 2 e kx , k > 0 , 0 x < . Find ( i ) the coecient a in term of k . ( ii ) the cdf of X . ( iii ) The probability that 0 X 1 /k . Solution ( i ) From the formula for the gamma function it is easily shown that Z 0 ax 2 e kx dx = 2 a k 3 . and a = k 3 / 2 . ( ii ) The cdf is F X ( x ) = Z x 0 k 3 2 y 2 e ky dy = 1 k 2 x 2 + 2 k + 2 2 e kx , x 0 . ( iii ) P ( 0 X 1 k ) = F X ( 1 k ) = 1 5 2 e 1 . A. K. Khandani , ECE307Fall 2024 92 22 . A machine shop ha a large number of drilling machine . The number of breakdown follows a Poisson probability law with a mean rate of 3 per hour . One service station operates to repair the machine , with a service time that ha an exponential distribution with a mean of .25 hour . ( i ) What is the probability that , at any given instant , the service station is idle ( ii ) What is the expected queue length ( iii ) What is the smallest service rate allowable to achieve an expected queue length less than 2 ( iv ) What is the smallest number of server required to achieve an average queue length of less than 1 if each server ha an exponential distribution with a mean of .25 hour Solution ( i ) For the M/M/1 queue , = / = 3 / 4 and P 0 = ( 1 ) = 1 / 4 and the probability the machine will have to wait for service is 1 P 0 = 3 / 4 . ( ii ) E ( N q ) = 2 1 = ( 3 / 4 ) 2 1 ( 3 / 4 ) = 9 4 . ( iii ) To reduce E ( N q ) to 2 we require 2 / ( 1 ) = 2 or 2 = 2 ( 1 ) or = 1 3 Thus = 3 / = 3 1 or is at least 3 / ( 3 1 ) . ( iv ) In an M/M/2 queue E ( N q ) = P 0 3 ( 2 ) 2 = P o 3 4 3 4 5 2 = P o 27 100 and thus P 0 = 1 + + 2 5 / 4 1 = 5 11 and thus E ( N q ) = 5 11 27 100 < 1 and two server will suce . 23 . ( a ) In an M/M/1 ( single server ) queue , customer arrive at the rate of = 15 per hour . What is the minimum server rate to ensure that ( i ) the server is idle at least 10 of the time ( ii ) the expected value of the queue length is not to exceed 10 ( iii ) the probability there at least 20 people in the queue is at most 0.50 ( b ) If n arrival occurred in the time interval ( 0 , t ) , nd the probability density function , g ( s ) , of the time to the rst arrival . Solution a ) ( i ) For P 0 0 . 10 1 0 . 10 / 0 . 90 and thus / 0 . 90 = 16 . 67 . ( ii ) E ( N q ) 2 / ( 1 ) 10 2 / ( 1 ) and the solution to this quadratic equation is = 5 35 or 5 + 35 and 15 / ( 5 + 35 ) = 16 . 375 . A. K. Khandani , ECE307Fall 2024 93 ( iii ) For there to be at least 20 in the queue there are at least 21 in the system . P ( n 21 ) = X n =21 ( 1 ) n = 21 . 5 exp ( ln ( 0 . 5 ) / 21 ) 15 / exp ( ln ( 0 . 5 ) / 21 ) b ) Let the required probability density function be g ( s ) . Then g ( s ) d = d h ( ( t s ) ) n 1 e ( t s ) / ( n 1 ) i e s / h ( t ) n e t /n i = n ( t s ) n 1 ds/t n = n ( 1 s/t ) n 1 ds/t g ( s ) = n ( 1 s/t ) n 1 /t , 0 s t A. K. Khandani , ECE307Fall 2024 94",
    "6": "Jointly Distributed Random Variables 6.1 Joint Distribution Functions So far , we have only concerned ourselves with probability distribution for single random variable . In this chapter , we deal with probability statement concerning two or more random variable . In order to deal with such probability , we dene , for any two random variable X and Y , the joint cumulative probability distribution function of X and Y by F ( a , b ) = P X a , Y b < a , b < The distribution of X can be obtained from the joint distribution of X and Y a follows F X ( a ) = P X a = P X a , Y < = F ( a , ) and similarly F Y ( b ) = F ( , b ) The distribution function F X and F Y are sometimes referred to a the marginal distribution of X and Y . In the case that both X and Y are both discrete random variable , it is convenient to dene the joint probability mass function of X and Y by p ( x , y ) = P X = x , Y = y The probability mass function of X can be obtained from p ( x , y ) by p X ( x ) = P X = x = X y p ( x , y ) and similarly p Y ( y ) = P Y = y = X x p ( x , y ) A. K. Khandani , ECE307Fall 2024 95 Jointly Continuous Random Variables We say that X and Y are jointly continuous if there exists a function f ( x , y ) dened for all real x and y , having the property that for every set C of pair of real number ( that is , C is a set in the two dimensional plane ) P ( X , Y ) C = ZZ ( x , y ) C f ( x , y ) dx dy The function f ( x , y ) is called the joint probability density function of X and Y . If A and B are any set of real number , then by dening C = x , y ) x A , y B , we see that P X A , Y B = Z B Z A f ( x , y ) dx dy Because F ( a , b ) = P X ( , a , Y ( , b = Z b Z a f ( x , y ) dx dy after dierentiating f ( a , b ) = 2 a b F ( a , b ) wherever the partial derivation are dened . A. K. Khandani , ECE307Fall 2024 96 Another interpretation of the joint density function is a follows P a < X < a + da , b < Y < b + db = Z b + db b Z a + da a f ( x , y ) dxdy f ( a , b ) dadb when da and db are small and f ( x , y ) is continuous at a , b . Hence f ( a , b ) is a measure of how likely it is that the random vector ( X , Y ) will be near ( a , b ) . Similar to discrete case we have f X ( x ) = Z f ( x , y ) dy and f Y ( y ) = Z f ( x , y ) dx We can also dene joint probability distribution for n random variable in exactly the same manner a we did for n = 2 . The joint cumulative probability distribution function F ( a 1 , a 2 , . . . , a n ) of the n random variable X 1 , X 2 , . . . , X n is dened by F ( a 1 , a 2 , . . . , a n ) = P X 1 a 1 , X 2 a 2 , . . . , X n a n Further , the n random variable are said to be jointly continuous if there exists a function f ( x 1 , x 2 , . . . , x n ) , called the joint probability density function , such that for any set C in n -space P ( X 1 , X 2 , . . . , X n ) C = ZZ Z ( x 1 , x 2 , ... , x n ) C f ( x 1 , . . . , x n ) dx 1 dx 2 dx n A. K. Khandani , ECE307Fall 2024 97 6.2 Independent Random Variables The random variable X and Y are said to be independent if for any two set of real number A and B , P X A , Y B = P X A P Y B In term of the joint distribution function F of X and Y , we have that X and Y are independent if F ( a , b ) = F X ( a ) F Y ( b ) for all a , b When X and Y are discrete random variable , the condition of independence is equivalent to p ( x , y ) = p X ( x ) p Y ( y ) for all x , y and for continuous case , f ( x , y ) = f X ( x ) f Y ( y ) for all x , y Proposition The continuous ( discrete ) random variable X and Y are independent if and only if their joint probability density ( mass ) function can be expressed a f X , Y ( x , y ) = f X ( x ) f Y ( y ) < x < , < y < Remark For set of random variable X 1 , . . . , X n we can show that thesis random variable are independent by showing that X 2 is independent of X 1 X 3 is independent of X 1 , X 2 X 4 is independent of X 1 , X 2 , X 3 . . . . . . X n is independent of X 1 , . . . , X n 1 A. K. Khandani , ECE307Fall 2024 98 6.3 Sums of Independent Random Variables Suppose that X and Y are independent , continuous random variable having probability distribution function f x and f y . The cumulative distribution function of X + Y is obtained a follows F X + Y ( a ) = P X + Y a = ZZ x + y a f X ( x ) f Y ( y ) dx dy = Z Z a y f X ( x ) f Y ( y ) dx dy = Z F X ( a y ) f Y ( y ) dy By dierentiating , we obtain that the probability density function f X + Y of X + Y is given by f X + Y ( a ) = d da Z F X ( a y ) f Y ( y ) dx dy = Z f X ( a y ) f Y ( y ) dy A. K. Khandani , ECE307Fall 2024 99 Proposition , Sum of Gamma Random Variables If X and Y are independent gamma random variable with respective parameter ( s , ) and ( t , ) , then X + Y is a gamma random variable with parameter ( s + t , ) . Proof f X + Y ( a ) = 1 ( s ) ( t ) Z a 0 e ( a y ) ( a y ) s 1 e y ( y ) t 1 dy = Ke a Z a o ( a y ) s 1 y t 1 dy = Ke a a s + t 1 Z 1 o ( 1 x ) s 1 x t 1 dx by letting x = y a = Ce a a s + t 1 where C is a constant that doe not depend on a . But a the above is a density function and thus must integrate to 1 , the value of C is determined , and we have f X + Y ( a ) = e a ( a ) s + t 1 ( s + t ) A. K. Khandani , ECE307Fall 2024 100 Sum of Square of Standard Normal Random Variables If Z 1 , Z 2 , . . . , Z n are independent standard nor- mal random variable , then Y n X i =1 Z 2 i is said to have the chi-squared ( sometimes seen a 2 ) distribution with n degree of freedom . When n = 1 , Y = Z 2 1 , we can see that it probability density function is given by f Z 2 ( y ) = 1 2 y f Z ( y ) + f Z ( y ) = 1 2 y 2 2 e y/ 2 = 1 2 e y/ 2 ( y/ 2 ) 1 / 2 1 Noting that ( 1 2 ) = , this is the gamma distribution with parameter ( 1 2 , 1 2 ) . From the above proposition we obtain that the 2 distribution with n degree of freedom is just the gamma distribution with parameter ( n 2 , 1 2 ) and hence ha the probability density function a f 2 ( y ) = 1 2 e y 2 / 2 ( y 2 ) n/ 2 1 ( n 2 ) y > 0 = e y 2 / 2 y n/ 2 1 2 n/ 2 ( n 2 ) y > 0 When n is an even integer , ( n 2 ) = ( n/ 2 ) 1 , and when n is odd , ( n 2 ) can be obtained from iterating the relationship ( t ) = ( t 1 ) ( t 1 ) and then using ( 1 2 ) = . A. K. Khandani , ECE307Fall 2024 101 Proposition , Sum of Normal Random Variables If X i , i = 1 , . . . , n are independent random variable that are normally distributed with respective parameter i , 2 i , i = 1 , . . . , n , then n X i =1 X i is normally distributed with parameter n X i =1 i and n X i =1 2 i . 6.4 Conditional Distributions Discrete Case For any two event E and F , the conditional probability of E given F is dened , provided that P ( F ) > 0 , by P ( E F ) = P ( EF ) P ( F ) Hence , If X and Y are discrete random variable , it is natural to dene the conditional probability mass function of X given by Y = y , by p X Y ( x y ) = P X = x Y = y = P X = x , Y = y P Y = y = p ( x , y ) p Y ( y ) for all value of y such that p Y ( y ) > 0 . Similarly , the conditional probability distribution function of X given that Y = y is dened , for all y such that p Y ( y ) > 0 , by F X Y ( x y ) = P X x Y = y = X a x p X Y ( a y ) If X and Y are independent , then p X Y ( x y ) = P X = x A. K. Khandani , ECE307Fall 2024 102 6.5 Conditional Distributions Continuous Case If X and Y have a joint density function f ( x , y ) , then the conditional probability density function of X , given that Y = y , is dened for all value of y such that f Y ( y ) > 0 , by f X Y ( x y ) = f ( x , y ) f Y ( y ) 6.6 Order Statistics Not covered in this course . 6.7 Joint Probability Distribution Functions of Random Variables Let X 1 and X 2 be jointly continuous random variable with joint probability density function f X 1 , X 2 . It is sometimes necessary to obtain the joint distribution of the random variable Y 1 and Y 2 , which arise a function of X 1 and X 2 . Specically , suppose that Y 1 = g 1 ( X 1 , X 2 ) and Y 2 = g 2 ( X 1 , X 2 ) . Assume that function g 1 and g 2 , satisfy the following condition 1 . The equation y 1 = g 1 ( x 1 , x 2 ) and y 2 = g 2 ( x 1 , x 2 ) can be uniquely solved for x 1 and x 2 in term of y 1 and y 2 with solution given by , x 1 = h 1 ( y 1 , y 2 ) and x 2 = h 2 ( y 1 , y 2 ) . 2 . The function g 1 and g 2 have the continuous partial derivative at all point ( x 1 , x 2 0 that are such that the following 2 2 determinant J ( x 1 , x 2 ) = g 1 x 1 g 1 x 2 g 2 x 1 g 2 x 2 = g 1 x 1 g 2 x 2 g 1 x 2 g 2 x 1 = 0 at all point ( x 1 , x 2 ) . Under these two condition it can be shown that the random variable Y 1 and Y 2 are jointly continuous with joint density function given by f Y 1 , Y 2 ( y 1 , y 2 ) = f X 1 , X 2 ( x 1 , x 2 ) J ( x 1 , x 2 ) 1 where x 1 = h 1 ( y 1 , y 2 ) and x 2 = h 2 ( y 1 , y 2 ) . A. K. Khandani , ECE307Fall 2024 103 When the joint density function of n random variable X 1 , X 2 , . . . , X n is given and we want to compute the joint density function of Y 1 , Y 2 , . . . , Y n , where Y 1 = g 1 ( X 1 , . . . , X n ) , Y 2 = g 2 ( X 1 , . . . , X n ) , . . . , Y n = g n ( X 1 , . . . , X n ) the approach is the same . Namely , we assume that the function g i have continuous partial derivative and that the Jacobian determinant J ( x 1 , . . . , x n ) = 0 at all point ( x 1 , . . . , x n ) , where J ( x 1 , . . . , x n ) = g 1 x 1 g 1 x 2 g 1 x n g 2 x 1 g 2 x 2 g 2 x n ... ... ... g n x 1 g n x 2 g n x n A. K. Khandani , ECE307Fall 2024 104 6.8 Some Solved Problems 1 . Let R 1 and R 2 be independent , each with density f ( x ) = e x , x 0 f ( x ) = 0 , x < 0 . Let R 3 =max ( R 1 , R 2 ) . Compute E ( R 3 ) . Solution E ( R 3 ) = E g ( R 1 , R 2 ) = Z Z g ( x , y ) f 12 ( x , y ) dx dy = Z 0 Z 0 max ( x , y ) e x e y dx dy Now max ( x , y ) = x if x y max ( x , y ) = y if x y . Thus E ( R 3 ) = Z Z x y xe x e y dx dy + Z Z y x ye x e y dx dy = Z x =0 xe x Z x y =0 e y dy dx + Z y =0 ye y Z y x =0 e x dx dy The two integral are equal , since one may be obtained from the other by interchanging x and y . Thus E ( R 3 ) = 2 Z 0 xe x Z x 0 e y dy dx = 2 Z 0 xe x ( 1 e x ) dx = 2 Z 0 xe x dx 2 Z 0 z 2 e z dz 2 = 3 2 ( 2 ) = 3 2 2 . We arrive at a bus stop at time t = 0 . Two bus A and B are in operation . The arrival time R 1 of bus A is uniformly distributed between 0 and t A minute , and the arrival time R 2 of bus B is uniformly distributed between 0 and t B minute , with t A t B . The arrival time are independent . Find the probability that bus A will arrive rst . Solution We are looking for the probability that R 1 < R 2 . Since R 1 and R 2 are independent , the conditional density of R 2 given R 1 is f ( x , y ) f 1 ( x ) = f 2 ( y ) = 1 t B , 0 y t B If bus A arrives at x , 0 x t A , it will be rst provided that bus B arrives between x and t B . This happens with probability ( t B x ) /t B . Thus P R 1 < R 2 R 1 = x = 1 x t B , 0 x t A Hence , P R 1 < R 2 = Z P R 1 < R 2 R 1 = x f 1 ( x ) dx = Z t A 0 1 x t B 1 t A dx = 1 t A 2 t B Alternatively , we may simply use the joint density P R 1 < R 2 = Z Z x < y f ( x , y ) dx dy = the shaded area in gure below , divided by total area t A t B = 1 t 2 A / 2 t A t B = 1 t A 2 t B a before . A. K. Khandani , ECE307Fall 2024 105 mup t A mup t B mup x = y mup x mup y 3 . Suppose that X and Y have joint density f ( x , y ) = ( 3 x 2 + 4 xy ) / 2 when 0 < x , y < 1 . Find the marginal density of X and the conditional density of Y given X = x . Solution f X ( x ) = Z 1 0 ( 3 x 2 + 4 xy ) / 2 dy = 3 2 x 2 + x , for 0 < x < 1 and f Y ( y X = x ) = 3 x 2 + 4 xy 2 / ( 3 2 x 2 + x ) = 3 x + 4 y 3 x + 2 , for 0 < y < 1 . 4 . Suppose the joint pdf for the random variable X and Y is given by f X , Y ( x , y ) = c , 0 x y 1 0 , otherwise . Find the following ( a ) The constant c , ( b ) The marginal pdfs , f X ( x ) and f Y ( y ) , and ( c ) The probability that X + Y < 1 . Solution ( a ) We know that the area under the pdf should be equal to one , and in this case , Z Z f X , Y ( x , y ) dx dy = Z 1 0 Z 1 x c dy dx = 1 Thus c/ 2 = 1 = c = 2 ( b ) f X ( x ) = Z f X , Y ( x , y ) dy so , f X ( x ) = Z 1 y = x 2 dy = 2 ( 1 x ) , 0 x 1 0 , otherwise Similarly , A. K. Khandani , ECE307Fall 2024 106 f Y ( y ) = Z f X , Y ( x , y ) dx = Z y x =0 2 dx = 2 y , 0 y 1 0 , otherwise ( c ) By noticing the following gure , it is clear that P ( X + Y < 1 ) = 1 2 mup1 mup0 mup y mup x + y = 1 mup1 mup x 5 . Consider a joint pdf for random variable X and Y dened a f X , Y ( x , y ) = cxy , 0 x , y 1 0 , otherwise . Dene the event A a Y > X . ( i ) Compute the contact c . ( ii ) Find the conditional pdf of X and Y , given A . ( ii ) Find the conditional pdf of Y given A . Solution We rst must nd the value for c in f X , Y ( x , y ) . We have , Z 1 0 Z 1 0 cxy dx dy = 1 And so , c Z 1 0 y ( x 2 2 1 0 ) dy = c Z 1 0 y 2 dy = 1 Which result in c = 4 . To nd f X , Y A ( x , y A ) , we rst nd p ( A ) . P ( A ) = P ( Y > X ) = 4 Z 1 0 Z y 0 xy dx dy = 1 2 The conditional density , given A , is A. K. Khandani , ECE307Fall 2024 107 f X , Y A ( x , y A ) = ( f X , Y ( x , y ) 1 / 2 , ( x , y ) A 0 x < y 1 0 , otherwise . = 8 xy , 0 x < y 1 0 , otherwise . The marginal pdf of Y given A , is obtained by integrating f X , Y A ( x , y A ) with respect to x. f Y A ( y A ) = Z y 0 8 xy dx = 8 y x 2 2 y 0 = 4 y 3 , 0 y 1 0 , otherwise . 6 . Let X and Y be independent and uniformly distributed over the interval ( 0 , a ) . The density function are given by f X ( x ) = 1 /a ( 0 x a ) f Y ( y ) = 1 /a ( 0 y a ) and zero otherwise . Find the density function of Z = X + Y . Solution We know that the PDF of Z is given by f Z ( z ) = Z f X ( x ) f Y ( z x ) dx Hence , the integrand is 1 /a 2 if 0 x a , 0 z x a and zero otherwise . Two case depending on the value of z ( a ) For 0 z a , we nd , f Z ( z ) = Z z 0 ( 1 /a ) 2 dx = z/a 2 ( b ) For a z 2 a , we nd , f Z ( z ) = Z a z a ( 1 /a ) 2 dx = ( 2 a z ) /a 2 7 . At a crossing there is a trac light showing alternately green and red light for a second . A car driver who arrives at random ha to wait for a time period Z . Find the distribution of Z . Solution If the driver arrives during a green period , his waiting time is zero . Since the green and the red light have the same duration , we have P ( Z = 0 ) = 1 / 2 . If the driver arrives during the rst a z second of a red period , his waiting time is greater than z . Hence for 0 < z < a we have P ( Z > z ) = a z 2 a and P ( Z z ) = 1 a z 2 a = 1 2 + z 2 a Thus , the cdf function is F Z ( z ) = 0 if z < 0 1 / 2 if z = 0 1 2 + z 2 a if 0 < z < a 1 if z a A. K. Khandani , ECE307Fall 2024 108 8 . Let ( X , Y , Z ) be a random point uniformly selected in the unit sphere . That is , their joint density is 3 / 4 when x 2 + y 2 + z 2 1 , and 0 otherwise . Find the marginal density of ( i ) ( X , Y ) and ( ii ) Z . Solution To nd the f XY ( x , y ) , we integrate f XY Z with respect to z in the range p 1 x 2 y 2 , p 1 x 2 y 2 . This result in , f X , Y ( x , y ) = 3 4 2 q 1 x 2 y 2 , for x 2 + y 2 1 . To compute f Z ( z ) , from the previous result we know that , f X , Z ( x , z ) = 3 4 2 p 1 x 2 z 2 , for x 2 + z 2 1 . We integrate f X , Z ( x , z ) with respect to x in the range , 1 z 2 , 1 z 2 . One way to do this is to apply the change of variable x 1 z 2 = co This result in , f Z ( z ) = 3 4 ( 1 z 2 ) . 9 . Suppose X 1 , . . . , X n are independent and all have the cdf F X ( x ) . Find the cdf of Y = max X 1 , . . . , X N , and Z = min X 1 , . . . , X N . Solution ( a ) F Y ( y ) = P ( X 1 y , , X n y ) = F X ( y ) n . ( b ) P ( Z > z ) = P ( X 1 > z , , X n > z ) = ( 1 F X ( z ) ) n , so F Z ( z ) = 1 ( 1 F X ( z ) ) n . 10 . Suppose X 1 , . . . , X 5 are independent and all have the same distribution which is continuous . Show that P ( X 3 < X 5 < X 1 < X 4 < X 2 ) = 1 / 5 . Solution Let X 1 , . . . , X 5 be independent with distribution f . We use the fact that f is continuous to conclude that with probability one all the X s are distinct . This mean that for a given set of X 1 , . . . , X 5 value , we can have 5 dierent permutation to rearrange the X value . Noting the independence of the X s , we conclude that these 5 dierent permutation all have the same probability . As these 5 dierent permutation are disjoint and cover all the possibility for the ordering of X s , we conclude that each happen with probability 1 / 5 . 11 . Suppose X and Y are independent , X is uniform on ( 0 , 1 ) and Y ha the cdf F ( y ) . Show that Z = X + Y ha the density function F ( z ) F ( z 1 ) . Solution Using the result of problem 4 with f X ( x ) = 1 , we obtain , f X + Y ( z ) = Z 1 0 f Y ( z x ) dx = F ( z ) F ( z 1 ) 12 . Fill in the rest of the joint distribution given in the following table knowing that ( i ) P ( Y = 2 X = 0 ) = 1 / 4 , and ( ii ) X and Y are independent . Y X = 0 3 6 1 2 . 1 . 05 ( i ) P ( Y = 2 X = 0 ) = 1 / 4 and P ( X = 0 , Y = 2 ) = 0 . 1 implies P ( X = 0 ) = 0 . 4 and P ( X = 0 , Y = 1 ) = 0 . 3 . Using the fact that X and Y are independent , we conclude now from P ( X = 0 , Y = 2 ) = P ( X = 0 ) P ( Y = A. K. Khandani , ECE307Fall 2024 109 2 ) = 0 . 1 that P ( Y = 2 ) = 0 . 25 , and P ( X = 3 , Y = 2 ) = P ( X = 3 ) P ( Y = 2 ) = 0 . 05 give that P ( X = 3 ) = 0 . 2 and hence , P ( X = 6 ) = 1 0 . 4 0 . 2 . Thus the entire distribution is given by , Y X = 0 3 6 1 . 3 . 15 . 3 2 . 1 . 05 . 1 13 . Suppose X and Y have joint density f ( x , y ) . Are X and Y independent if ( a ) f ( x , y ) = xe x ( 1+ y ) for x , y 0 ( b ) f ( x , y ) = 6 xy 2 when x , y 0 and x + y 1 ( c ) f ( x , y ) = 2 xy + x when 0 < x < 1 and 0 < y < 1 ( d ) f ( x , y ) = ( x + y ) 2 ( x y ) 2 when 0 < x < 1 and 0 < y < 1 In each case , f ( x , y ) = 0 otherwise . Solution ( a ) No . Since f X ( x ) = R 0 xe x ( 1+ y ) dy = e x for x > 0 . f Y ( y ) = Z 0 xe x ( 1+ y ) dx = xe x ( 1+ y ) 1 + y 0 + Z 0 e x ( 1+ y ) 1 + y dx = 1 ( 1 + y ) 2 for y > 0 , and f ( x , y ) = f X ( x ) f Y ( y ) . We could simply reach this conclusion by noticing that f X , Y ( x , y ) can not be factored a the product of two function , one only function of x and the other one only function of y . ( b ) No . Since ( x , y ) f ( x , y ) > 0 is not a rectangle . Note that f X , Y ( x , y ) can be factored a the product of two function , one only function of x and the other one only function of y . However , this is not sucient condition for independence . Theorem If f ( x , y ) can be written a g ( x ) h ( y ) then there is a constant c so that f X ( x ) = cg ( x ) and f Y ( y ) = h ( y ) /c . It follows that f ( x , y ) = f X ( x ) f Y ( y ) and hence X and Y are independent . Note that this requires the range of ( x , y ) to be a rectangle with one side corresponding to the range of x and the other side corresponding to the range of y . ( c ) Yes by the above theorem since f ( x , y ) = x ( 2 y + 1 ) . ( d ) Yes by the above theorem since f ( x , y ) = 2 x 2 y . 14 . Suppose X 1 , . . . , X n are independent and have distribution function F ( x ) . Find the joint density function of Y = max X 1 , . . . , X n and Z = min X 1 , . . . , X n . Find the joint density of Y and Z . Solution We have , P ( Y y , Z z ) = P ( z X i y for all i n ) = ( Z y z f ( x ) dx ) n = F ( y ) F ( z ) n . and , P ( Y y , Z z ) + P ( Y y , Z z ) = P ( Y y ) This is used to compute the joint distribution function F ( y , z ) a follows F ( y , z ) = P ( Y y , Z z ) = P ( Y y ) P ( Y y , Z z ) = F ( y ) n F ( y ) F ( z ) n The corresponding joint density function is equal to , f ( y , z ) = 2 F yz = n ( n 1 ) f ( y ) f ( z ) Z y z f ( x ) dx n 2 A. K. Khandani , ECE307Fall 2024 110 15 . Suppose X 1 , . . . , X n are independent and uniform on ( 0,1 ) . Let X ( k ) be the k th smallest of the X j . Show that the density of X ( k ) is given by n n 1 k 1 x k 1 ( 1 x ) n k Solution This formula is easy to understand There are n value of index j that we can pick such that the corresponding X j is the k th smallest of the X s. The rest of the formula then give the probability that exactly k 1 of the remaining n 1 variable are smaller than x . Suppose X 1 , . . . , X n are independent and have density function f . Let X ( 1 ) be the smallest of the X j , X ( 2 ) be the second smallest , and so on until X ( n ) is the largest . It can be shown that their joint density is given by f ( x 1 , . . . , x n ) = n f ( x 1 ) f ( x n ) if x 1 < x 2 < x n ( ) 0 otherwise Hence , the density function of X ( k ) is obtained by integrating the above joint density with respect to the other variable n Z x k 0 Z x k x 1 Z x k x k 2 Z 1 x k Z 1 x n 1 dx n dx k +1 dx k 1 dx 2 dx 1 = n 1 ( k 1 ) x k 1 k 1 ( n k ) ( 1 x k ) n k = n n 1 k 1 x k 1 k ( 1 x k ) n k To generalize the above result we show that if X 1 , . . . , X n are independent and have density f then the density of X ( k ) is given by nf ( x ) n 1 k 1 F ( x ) k 1 ( 1 F ( x ) ) n k The density function of X ( k ) is obtained by integrating the ( * ) equation n Z x k Z x k x k 2 Z x k Z x n 1 n i =1 f ( x i ) dx n dx k +1 dx k 1 dx 1 To evaluate this we prove by induction that Z b a Z b x k + i 1 i j =1 f ( x k + j ) dx k + i dx k +1 = 1 i ( F ( b ) F ( a ) ) i ( ) This is clear if i = 1 . If the formula is true for i then the integral for i + 1 is R b a f ( x k +1 ) 1 i ( F ( b ) F ( x k +1 ) ) i dx k +1 = ( F ( b ) F ( x k +1 ) ) i +1 ( i + 1 ) b a = 1 ( i +1 ) ( F ( b ) F ( a ) ) i +1 and we have proved ( * * ) . Using ( * * ) with a = , b = x k , i = k 1 and then a = x k , b = , i = n k we have g ( x k ) = n f ( x k ) 1 ( k 1 ) F ( x k ) k 1 1 ( n k ) ( 1 F ( x k ) ) n k = nf ( x k ) n 1 k 1 F ( x k ) k 1 ( 1 F ( x k ) ) n k A. K. Khandani , ECE307Fall 2024 111 16 . Suppose we take a die with 3 on three side , 2 on two side , and 1 on one side , roll it n time , and let X i be the number of time side i appeared . Find the conditional distribution P ( X 2 = k X 3 = m ) . Solution P ( X 3 = m ) = n m 3 6 m 1 3 6 n m P ( X 2 = k , X 3 = m ) = n ( n k m ) k m ( 3 / 6 ) m ( 2 / 6 ) k ( 1 / 6 ) n m k P ( X 2 = k X 3 = m ) = n m k ( 2 / 3 ) k ( 1 / 3 ) n m k that is , binomial ( n m , 2 / 3 ) . 17 . Suppose X 1 , . . . , X m are independent and have a geometric distribution with parameter p . Find P ( X 1 = k X 1 + X m = n ) . Solution One can show that the sum of i.i.d random variable with geometric distribution ha a Pascal distribution . Using this fact we have , P ( X 1 + + X m = n ) = n 1 m 1 p m ( 1 p ) n m , so P ( X 1 = k X 1 + + X m = n ) = P ( X 1 = k ) P ( X 2 + + X m = n k ) P ( X 1 + + X m = n ) = n 1 k m 2 / n 1 m 1 This is quite intuitive since among n 1 m 1 choice for the rst m 1 success there are n 1 k m 2 with the rst success occurring at the k th trial . 18 . Suppose X and Y have joint density f ( x , y ) = ( 1 / 2 ) e y when y 0 and y x y . Compute P ( X 1 Y = 3 ) . Solution f Y ( y ) = Z y y 1 2 e y dx = ye y for y 0 f X ( x Y = y ) = ( 1 / 2 ) e y /ye y = 1 / 2 y for y x y P ( X 1 Y = 3 ) = Z 1 3 1 / 6 dx = 2 / 3 19 . Jobs 1 and 2 must be completed before job 3 is begun . If the amount of time each task take is independent and uniform on ( 2,4 ) , nd the density function for the amount of time T it take to complete all three job . Solution Let T i be the time required for job i , S = max T 1 , T 2 , and T = S + T 3 be the total time . P ( S s ) = ( s 2 ) 2 / 4 when 2 s 4 so S ha density function ( s 2 ) / 2 , when 2 s 4 . Since S and T 3 are independent , the pdf of T is the convolution of pdfs for S and T 3 . So , we get f T ( t ) = ( ( u 4 ) 2 / 8 if 4 < u < 6 1 2 h 1 ( u 6 ) 2 4 i if 6 < u < 8 A. K. Khandani , ECE307Fall 2024 112 20 . Suppose X ha density function x/ 2 for 0 < x < 2 and 0 otherwise . Find the density function of Y = X ( 2 X ) . Solution P ( Y y ) = P ( X 2 2 X + 1 1 y ) = P ( X 1 p 1 y ) = F ( 1 + p 1 y ) F ( 1 p 1 y ) Dierentiating we see that the density function of Y is ( for 0 y 1 ) f ( 1 1 y ) + f ( 1 + 1 y ) 2 1 y = 1 / 2 p 1 y 21 . Suppose X 1 and X 2 have joint density f ( x 1 , x 2 ) = 1 for 0 < x 1 , x 2 < 1 0 otherwise Find the density of Y 1 = X 1 /X 2 and Y 2 = X 1 X 2 . Solution The map r ( x 1 , x 2 ) = ( x 1 /x 2 , x 1 x 2 ) ha the inverse s ( y 1 , y 2 ) = h ( y 1 y 2 ) 1 / 2 , ( y 2 /y 1 ) 1 / 2 i The element of the Jacobian are equal to D 11 = 1 2 y 1 / 2 1 y 1 / 2 2 , D 12 = 1 2 y 1 / 2 1 y 1 / 2 2 , D 21 = 1 2 y 3 / 2 1 y 1 / 2 2 , D 22 = 1 2 y 1 / 2 1 y 1 / 2 2 and the Jacobian is D = 1 2 y 1 1 . Thus f Y 1 , Y 2 ( y 1 , y 2 ) = 1 / 2 y 1 for 0 < y 1 y 2 < 1 , 0 < y 2 < y 1 22 . Let f XY ( x , y ) = 1 / if 0 x 2 + y 2 1 = 0 elsewhere Find the pdf of r 2 = x 2 + y 2 and = tan 1 y/x . Solve the same problem for the following joint pdf f XY ( x , y ) = Ae ( x 2 + y 2 ) if 0 x 2 + y 2 = 0 elsewhere We solve the second part of the problem which is more general . Consider , f X , Y ( x , y ) = f X ( x ) f Y ( y ) = 1 2 2 exp x 2 + y 2 2 2 ( 4 ) where 2 = 1 / 2 and A = 1 / . For the polar coordinate we have , R = X 2 + Y 2 = arctan ( Y/X ) ( 5 ) This result in , J ( x , y ) = x x 2 + y 2 y x 2 + y 2 y x 2 + y 2 x x 2 + y 2 ( 6 ) A. K. Khandani , ECE307Fall 2024 113 We have , det J ( x , y ) = 1 p x 2 + y 2 = 1 r ( 7 ) The set of equation , r = p x 2 + y 2 = arctan ( y/x ) ( 8 ) ha only one solution given by , x = r co y = r sin ( 9 ) Substituting these result , we obtain , f R , ( r , ) = rf X , Y ( r co , r sin ) = r 2 2 exp r 2 2 2 ( 10 ) The marginal pdfs for R , are equal to , f ( ) = Z 0 f R , ( r , ) dr = 1 2 Z 0 r 2 exp r 2 2 2 dr = 1 2 exp r 2 2 2 0 = 1 2 = ha a uniform distribution in 0 , 2 ( 11 ) and f R ( r ) = Z 2 0 f R , ( r , ) d = r 2 exp r 2 2 2 , r 0 ( 12 ) Then , f R ( r ) = r 2 exp r 2 2 2 , r 0 0 r < 0 ( 13 ) This is known a the Raleigh probability density function . We also note that f R , ( r , ) = f R ( r ) f ( ) . ( 14 ) This mean that R and are independent of each other . 23 . Let f XY ( x , y ) = e ( x + y ) , x , y 0 = 0 elsewhere Find the pdf of Z = X + Y . Solution Noticing that X and Y are independent , the pdf of Z is the convolution of pdfs for X and Y . This result f Z ( z ) = ze z , z 0 = 0 elsewhere A. K. Khandani , ECE307Fall 2024 114 24 . The discrete random variable X and Y are independent and each ha a geometric probability distribution P ( X = k ) = q k 1 p , P ( Y = k ) = q k 1 p , k = 1 , 2 , 3 , Find the conditional probability distribution P ( X = k X + Y = n ) . Solution P ( X + k X + Y = n ) = P ( X = k , Y = n k ) P ( X + Y = n ) and P ( X + Y = n ) = P n l =0 P ( X = l ) P ( Y = n l ) = P n l =0 q l 1 pq n l 1 p = p 2 P n l =0 q n 2 = p 2 q n 2 ( n + 1 ) and P ( X = k X + Y = n ) = q k 1 pq n k 1 p p 2 q n 2 ( n + 1 ) = 1 n + 1 , k = 0 , 1 , 2 , , n. 25 . Suppose a point ( X , Y ) is chosen at random in the unit circle x 2 + y 2 1 . Find the marginal density function of X . Solution The bivariate density function is uniform over the unit circle and hence ha height 1 / . The marginal density function for X is then f X ( x ) = Z 1 x 2 1 x 2 1 dy = 2 p 1 x 2 , 1 < x < 1 . 26 . Suppose X and Y have the joint density function f ( x , y ) . Are X and Y independent if ( i ) f ( x , y ) = xe x ( 1+ y ) , x , y > 0 . ( ii ) f ( x , y ) = 6 xy 2 when x , y 0 and x + y < 1 . ( iii ) f ( x , y ) = ( x + y ) 2 ( x y ) 2 when 0 x , y 1 . Solution ( i ) f X ( x ) = Z 0 xe x e xy dy = e x , x 0 and f Y ( y ) = Z 0 xe x ( 1+ y ) dx = 1 ( 1 + y ) 2 , y 0 and since f ( x , y ) = f X ( x ) f Y ( y ) X and Y are not independent . ( ii ) f X ( x ) = Z 1 x 0 6 xy 2 dy = 2 x ( 1 x ) 3 , 0 x 1 and f Y ( y ) = Z 1 y 0 6 xy 2 dx = 3 y 2 ( 1 y ) 2 , 0 y 1 and clearly X and Y are not independent . ( iii ) Notice that f ( x , y ) = 4 xy and so f X ( x ) = 2 x , 0 x 1 and f Y ( y ) = 2 y , 0 y 1 and X and Y are independent . A. K. Khandani , ECE307Fall 2024 115 27 . The bivariate random variable ( X , Y ) ha the joint pdf f ( x , y ) = 4 x ( 1 y ) 0 x , y 1 0 elsewhere ( i ) nd the marginal density function of X and Y and determine whether or not they are independent . ( ii ) Determine the probability that Y > X 2 . Solution ( i ) By straight forward computation the marginals are determined a f X ( x ) = Z 1 0 4 x ( 1 y ) dy = 2 x , 0 x 1 f Y ( y ) = Z 1 0 4 x ( 1 y ) dx = 2 ( 1 y ) , 0 y 1 and X and Y are independent . ( ii ) P ( Y > X 2 ) = Z 1 0 Z 1 x 2 4 x ( 1 y ) dy dx = 1 / 3 . 28 . ( X , Y ) is a bivariate random variable which is uniformly distributed over the triangle x , y 0 , x + y 1 . ( i ) Find the marginal pdf , mean and variance of X . ( ii ) Find the correlation coecient XY . Solution ( i ) Notice that the problem is symmetric in X and Y . The marginal of X is f X ( x ) = 2 ( 1 x ) , 0 x 1 . The mean and variance of X are , respectively E ( X ) = Z 1 0 x 2 ( 1 x ) dx = 1 / 3 , E ( X 2 ) = Z 1 0 x 2 2 ( 1 x ) dx = 1 / 6 and V ( X ) = 1 / 18 . ( ii ) To compute the correlation coecient of X and Y we need E ( XY ) = 2 Z 1 0 x Z 1 x 0 ydy dx = 1 / 12 and thus XY = 1 12 1 3 1 3 r 1 18 1 18 = 1 2 . 29 . Suppose that X and Y have the joint probability density function f XY = ( 2 m + 6 ) x m y 0 < y < x < 1 0 otherwise . Show that this is a probability density function and nd E ( Y X ) and V ( Y X ) . Solution The marginal density of X is f X ( x ) = Z x 0 ( 2 m + 6 ) x m ydy = ( m + 3 ) x m +2 , 0 x 1 and notice that Z 1 0 ( m + 3 ) x m +2 dx = 1 A. K. Khandani , ECE307Fall 2024 116 and hence is a pdf . The conditional pdf is f Y X ( y x ) = 2 ( m + 3 ) x m y ( m + 3 ) x m +2 = 2 y x 2 , 0 y x and E ( Y X ) = Z x 0 y 2 y x 2 dy = 2 3 x , E ( Y 2 X ) = Z x 0 y 2 2 y x 2 dy = 1 2 x 2 and V ( Y X ) = E ( Y 2 X ) E 2 ( Y X ) = x 2 / 18 . 30 . If X is a binomial random variable with parameter n and p and Y is a binomial random variable with parameter m and p , X and Y independent , and Z = X + Y , nd E ( X Z ) . Solution P ( X = j Z = X + Y = l ) = P ( X = j and Y = l j ) P ( X + Y = l ) = P ( X = j ) P ( Y = l j ) P ( X + Y = l ) = ( n j ) ( m l j ) ( n + m l ) which is a hypergeometric distribution . The mean of this distribution is in the text and is E ( X Z ) = nl/ ( n + m ) . 31 . If ( X , Y ) is a bivariate normal random variable with pdf f XY ( x , y ) = 1 3 exp 2 3 ( x 2 xy + y 2 ) ( i ) Find the probability that P ( Y > 3 X = 2 ) and compare this to P ( Y > 3 ) . ( ii ) Find E ( Y X ) . Solution ( i ) As with the previous problem , the ve parameter of the bivariate density are found a X = Y = 0 X = Y = 1 XY = 1 / 2 . Consequently the conditional density function f Y X ( y x ) N ( 1 2 x , 3 4 ) . Then P ( Y > 3 X = 2 ) = Z 3 e ( y 1 ) 2 / ( 3 / 2 ) 2 p 3 / 4 dy = 1 ( 4 3 ) . Since Y N ( 0 , 1 ) , P ( Y > 3 ) = Z 3 e y 2 / 2 2 dy = 1 ( 3 ) . ( ii ) E ( Y X ) = X/ 2 . 32 . The time in minute taken by person A to complete a certain task is assumed to be a random variable with an exponential pdf with parameter 1 . The time taken by person B is independent of that for person A and ha an exponential pdf with parameter 2 i.e . f X ( x ) = 1 e 1 x , x 0 , f Y ( y ) = 2 e 2 y , y 0 ( i ) What is the probability it take A longer to complete the task than B ( ii ) If 1 = 2 what is the probability the task are nished within two minute of each other A. K. Khandani , ECE307Fall 2024 117 Solution ( i ) Let X be the time taken by person A and Y that taken by person B. P ( X > Y ) = Z 0 Z x 0 2 e 2 y dy 1 e 1 x dx which can be computed to be P ( X > Y ) = 2 1 + 2 . ( ii ) It is desired to compute P ( X Y < 2 ) . Let 1 = 2 = . Consider P ( Y < X 2 ) = Z 2 Z x 2 0 e dy e x dx which is readily evaluated to e 2 / 2 . The required probability is then P ( X Y < 2 ) = 1 2 ( e 2 / 2 ) = 1 e 2 . 33 . Let X and Y be independent random variable , each with an exponential density function with parameter ( i.e . f X ( x ) = e x , x 0 ) . Find the density function of Z = X/ ( X + Y ) . Solution F Z ( z ) = P ( X X + Y z ) = P ( X z ( X + Y ) ) = P ( X z 1 z Y ) = Z 0 ( Z ( 1 z ) x/z e y dy ) e x dx = Z 0 e ( 1 z ) x/z e x dx = Z 0 e x/z dx = z and hence f Z ( z ) = 1 , 0 z 1 . 34 . Let X , Y and Z be independent and identically distributed random variable , each with an exponential pdf with parameter . Find the probability density function of W = X + Y + Z . Solution Let U + X + Y then f U ( u ) = Z u 0 f X ( u y ) f Y ( y ) dy = Z z 0 e ( z y ) e ( y ) dy = 2 ze z , 0 z < The process is repeated to nd the density of W f W ( w ) = Z w 0 e ( w v ) 2 ve ( v ) dv = 3 2 w 2 e w , w 0 35 . Let X and Y be independent random variable with density function f X ( x ) = e x , x 0 f Y ( y ) = n y n 1 ( n 1 ) e y , y 0 for n a positive integer . ( i ) Find the pdf of U = X + Y . ( ii ) Find the pdf of W = X/ ( X + Y ) . Solution We use the same technique a the previous two problem f U ( u ) = Z u 0 e ( u y ) n ( n 1 ) y n 1 e ( y ) dy = n +1 n u n e u . Notice that this implies that if Z = X 1 + X n , where the X i are i.i.d. , each with a density exp x , then the pdf of Z is f Z ( z ) = n z n 1 ( n 1 ) e z z 0 . A. K. Khandani , ECE307Fall 2024 118 36 . Let X and Y be independent random variable with the pdfs f X ( x ) = e x , x 0 , f Y ( y ) = e y , y 0 . Find the pdf of the random variable Z = ( X Y ) / ( X + Y ) and specify clearly it region of denition . Solution Notice that Z take value in the range 1 , 1 . F Z ( z ) = P ( X Y X + Y z ) = P ( ( X Y ) z ( X + Y ) ) = P ( ( 1 z 1+ z ) X Y ) = R 0 R 1 z 1+ z x e y e x dx = R 0 e 1 z 1+ z x e x dx = 1+ z 2 , 1 z 1 . Hence f Z ( z ) = 1 / 2 , 1 z 1 and zero elsewhere . 37 . The random variable X and Y have the joint probability density function f XY ( x , y ) = xe x ( 1+ y ) , 0 x , y < . Find the pdf of Z = XY . Solution F Z ( z ) = P ( Z z ) = P ( XY z ) = R 0 nR z/x 0 xe x ( 1+ y ) dy o dx R 0 xe x n 1 x e xy z/x 0 o dx = 1 e z and consequently f Z ( z ) = dF Z ( z ) /dz = e z , z 0 . 38 . Let Y be a random variable with pdf f X ( x ) = e x , x 0 . Find the pdf of the random variable Y = 1 e X , X 0 . Show that , in general , if X ha the pdf f X ( x ) and cdf F X ( x ) then Y = F X ( X ) is uniformly distributed on ( 0 , 1 ) . Solution Notice that Y take value in the range 0 , 1 and log ( 1 y ) > 0 . F Y ( y ) = P ( Y y ) = P ( 1 e X y ) = P ( X log ( 1 y ) ) = R log ( 1 y ) 0 e x dx = 1 e log ( 1 y ) = y and consequently f Y ( y ) = dF Y ( y ) /dy = 1 , 0 y 1 . In the general case , note that we are using the cdf a a montonic function , and that again Y take value in 0 , 1 F Y ( y ) = P ( Y y ) = P ( F X ( X ) y ) = P ( X F 1 X ( y ) ) = F X ( F 1 X ( y ) ) = y , 0 y 1 where F 1 X ( ) is the inverse function of F X ( ) . The result that Y is a uniformly distributed random variable on 0 , 1 follows . 39 . Let X , Y be independent normal random variable , each with zero mean and variance 2 . Show that Z = X + Y ha a normal density N ( 0 , 2 2 ) . A. K. Khandani , ECE307Fall 2024 119 Solution As in previous problem F Z ( z ) = Z 1 2 e ( z x ) 2 / ( 2 2 ) 1 2 e x 2 / ( 2 2 ) dx = Z 1 2 2 e 1 2 2 ( 2 x 2 2 zx + z 2 ) dx = 1 2 2 Z e 2 2 2 ( x z/ 2 ) 2 + z 2 / ( 4 2 ) dx = 1 2 2 e z 2 / 4 2 N ( 0 , 2 2 ) . A. K. Khandani , ECE307Fall 2024 120",
    "7": "Properties of Expectation 7.1 Introduction In this chapter we develop and exploit additional property of expected value . 7.2 Expectation of Sums of Random Variables Suppose that X and Y are random variable and g is a function of two variable . Then we have the following result . Proposition If X and Y have a joint probability mass function p ( x , y ) , then E g ( X , Y ) = X y X x g ( x , y ) p ( x , y ) If X and Y have a joint probability density function f ( x , y ) , then E g ( X , Y ) = Z Z g ( x , y ) f ( x , y ) dxdy For an important application of this proposition , suppose that E X and E Y are both nite and let g ( X , Y ) = X + Y . Then , in the continuous case , E X + Y = Z Z ( x + y ) f ( x , y ) dxdy = Z Z xf ( x , y ) dxdy + Z Z yf ( x , y ) dxdy = Z xf X ( x ) dx + Z yf Y ( y ) dy = E X + E Y The same result hold in general . Using this result and induction show that , if E X i is nite for all i = 1 , . . . , n , then E X 1 + + X n = E X 1 + + E X n A. K. Khandani , ECE307Fall 2024 121 7.3 Covariance , Variance of Sums , and Correlations Proposition If X and Y are independent , then for any function h and g , E g ( X ) h ( Y ) = E g ( X ) E h ( Y ) Denition The covariance between X and Y , denoted by Cov ( X , Y ) , is dened by Cov ( X , Y ) = E ( X E X ) ( Y E Y ) Just a the expected value and the variance of a single random variable give u information about this random variable , so doe the covariance between two random variable give u information about the relationship between the random variable . Upon Expanding the preceding denition , we see that Cov ( X , Y ) = E XY E X E Y Note that if X and Y are independent then using the preceding proposition , it follows that Cov ( X , Y ) = 0 . However , the converse is not true . Proposition ( i ) Cov ( X , Y ) = Cov ( Y , X ) ( ii ) Cov ( X , X ) = Var ( X ) ( iii ) Cov ( aX , Y ) = a Cov ( X , Y ) ( iv ) Cov n X i =1 X i , m X j =1 Y j = n X i =1 m X j =1 Cov ( X i , Y j ) A. K. Khandani , ECE307Fall 2024 122 It follows from part ( ii ) and ( iv ) of the preceding proposition , upon taking Y j = X j , j = 1 , . . . , n , that Var n X i =1 X i = Cov n X i =1 X i , n X i =1 X i = n X i =1 n X j =1 Cov ( X i , X j ) = n X i =1 Var ( X i ) + n X i =1 X j = i Cov ( X i , X j ) Since each pair of index , i , j , i = j appears twice in the double summation , the above is equivalent to the following Var n X i =1 X i = n X i =1 Var ( X i ) + 2 n X i =1 X j > i Cov ( X i , X j ) If X 1 , . . . , X n are pairwise independent , the preceding equation reduces to Var n X i =1 X i = n X i =1 Var ( X i ) A. K. Khandani , ECE307Fall 2024 123 Denition The correlation of two random variable X and Y , denoted by ( X , Y ) , is dened , a long a Var ( X ) Var ( Y ) is positive , by ( X , Y ) = Cov ( X , Y ) p Var ( X ) Var ( Y ) It can be shown that 1 ( X , Y ) 1 In fact , ( X , Y ) = 1 implies that Y = a + bX , where b = X / Y > 0 and ( X , Y ) = 1 implies that Y = a + bX , where b = X / Y < 0 . The reverse is also true , if Y = a + bX , then ( X , Y ) is either +1 or -1 , depending on the sign of b . The correlation coecient is a measure of the degree of linearity between X and Y . A value of ( X , Y ) near +1 or -1 indicates a high degree of linearity between X and Y , whereas a value near 0 indicates a lack of such linearity . A positive value of ( X , Y ) indicates that Y tends to increase when X doe , whereas a negative value indicates that Y tends to decrease when X increase . If ( X , Y ) = 0 , then X and Y are said to be uncorrelated . A. K. Khandani , ECE307Fall 2024 124 7.4 Conditional Expectation 7.4.1 denitions We saw that for two jointly discrete random variable X and Y , given that Y = y , the conditional probability mass function is dened by p X Y ( x y ) = P X = x Y = y = p ( x , y ) p Y ( y ) So , the conditional expectation of X , given that Y = y , for all value of y such that p Y ( y ) > 0 by E X Y = y = X x xp X = x Y = y = X x xp X Y ( x y ) Similarly for the case of continuous random variable , we have E X Y = y = Z xf X Y ( x y ) dx where f X Y ( x y ) = f ( x , y ) f Y ( y ) for all y such that f Y ( y ) > 0 . Similar to ordinary expectation we have E g ( X ) Y = y = X x g ( x ) p X Y ( x y ) in the discrete case Z g ( x ) f X Y ( x y ) dx in the continuous case and E n X i =1 X i Y = y = n X i =1 E X i Y = y A. K. Khandani , ECE307Fall 2024 125 7.4.2 Computing Expectations by Conditioning Let u denote by E X Y that function of a random variable Y whose value at Y = y is E X Y = y . Note that E X Y is itself a random variable . An extremely important property of conditioning property of conditional expectation is given by the following proposition . Proposition E X = E E X Y One way to understand this equation is to interpret it a follows To calculate E X , we may take a weighted average of the conditional expected value of X , given that Y = y , each of the term E X Y = y being weighted by the probability of the event on which it is conditioned . 7.4.3 Computing probability by Conditioning We can also use conditioning to nd probability . To see this , let E denote an arbitrary event and dene the indicator random variable X by X = 1 if E occurs 0 if E doe not occur It follows from the denition of X that E X = P ( E ) E X Y = y = P ( E Y = y ) for any random variable Y Therefore , P ( E ) = X y P ( E Y = y ) P ( Y = y ) if Y is discrete = Z P ( E Y = y ) f Y ( y ) dy if Y is continuous Note that if Y is a discrete random variable taking on one of the value y 1 , . . . , y n , then , by dening the event F i , i = 1 , . . . , n by F i = Y = y i , this equation reduces to the familiar equation P ( E ) = n X i =1 P ( E F i ) P ( F i ) A. K. Khandani , ECE307Fall 2024 126 7.4.4 Conditional Variance We can dene the conditional variance of X given that Y = y by Var ( X Y ) E ( X E X Y ) 2 Y and after simplication Var ( X Y ) = E X 2 Y ( E X Y ) 2 There is a very useful relationship between Var ( X ) , the unconditional variance of X , and Var ( X Y ) , the uncon- ditional variance of X given Y . E Var ( X Y ) = E E X 2 Y E ( E X Y ) 2 = E X 2 E ( E X Y ) 2 Also , a E X = E E X Y , we have Var ( E X Y ) = E ( E X Y ) 2 ( E X ) 2 By adding these two equation we obtain the following proposition . Proposition Var ( X ) = E Var ( X Y ) + Var ( E X Y ) A. K. Khandani , ECE307Fall 2024 127 7.5 Conditional Expectation and Prediction Sometimes a situation arises where the value of a random variable X is observed and then , based on the observed value , an attempt is made to predict the value of a second random variable Y . We would like to choose a function g so that g ( X ) tends to be close to Y . One possible criterion for closeness is to choose g so a to minimize E ( Y g ( X ) ) 2 . The best possible predictor of Y is g ( X ) = E Y X . Proposition E ( Y g ( X ) ) 2 E ( Y E Y X ) 2 Linear Predictor Sometimes the joint probability distribution function of X and Y is not completely known . If we know the mean and variance of X and Y and correlation between them we can at least determine the best linear predictor of Y with respect to X . To obtain the best linear prediction of Y in respect to X , we need to choose a and b so a to minimize E ( Y ( a + bX ) ) 2 . Minimizing this equation over a and b , yield b = E XY E X E Y E X 2 ( E X ) 2 = Cov ( X , Y ) 2 X = Y X a = E Y bE X = E Y Y E X X A. K. Khandani , ECE307Fall 2024 128 7.6 Moment Generating Function The moment generating function M ( t ) of a random variable X is dened for all real value of t by M ( t ) = E e tX = X x e tx p ( x ) if X is discrete with mass function p ( x ) Z e tx f ( x ) dx if X is continuous with density f ( x ) We call M ( t ) the moment generating function because all of the moment of X can be obtained by successively dierentiating M ( t ) and then evaluating the result at t = 0 . For example , M ( 0 ) = E X and M ( 0 ) = E X 2 In general the n th derivative of M ( t ) is given by M n ( 0 ) = E X n n 1 The moment generating function of some random variable are a follows . A. K. Khandani , ECE307Fall 2024 129 Moment Generating Functions of Some Random Variables If X is a binomial random variable with parameter n and p , then M ( t ) = ( pe t + 1 p ) n If X is a Poisson random variable with parameter , then M ( t ) = exp ( e t 1 ) If X is an exponential random variable with parameter , then M ( t ) = t for t < If X is a standard normal random variable with parameter 0 and 1 , then M ( t ) = e t 2 / 2 If X is a normal random variable with parameter and 2 , then M ( t ) = exp ( 2 t 2 2 + t ) A. K. Khandani , ECE307Fall 2024 130 Moment Generating function of the Sum of Independent Random Variables An important property of moment generating function is the moment generating function of the sum of independent random variable equal to the product of the individual moment generating function . If X and Y are independent , then M X + Y ( t ) = M X ( t ) M Y ( t ) 7.6.1 Joint Moment Generating Functions It is also possible to dene the joint moment generating function of two or more ransom variable . M ( t 1 , . . . , t n ) = E e t 1 X 1 + + t n X n The individual moment generating function can be obtained from M ( t 1 , . . . , t n ) by letting all but one of the t j be 0 . M X i ( t ) = M ( 0 , . . . , 0 , t , 0 , . . . , 0 ) where the t is in the i th place . It can be proved that M ( t 1 , . . . , t n ) uniquely determines the joint distribution of X 1 , . . . , X n . Joint Moment Generating function of Independent Random Variables If the n random variable are independent , then M ( t 1 , . . . , t n ) = M X 1 ( t 1 ) M X n ( t n ) On the other hand , if this equation is satised , then the n random variable are independent . A. K. Khandani , ECE307Fall 2024 131 7.7 Some Solved Problems 1 . Let X and Y be two independent standard normal random variable ( i.e . N ( 0 , 1 ) ) .Find ( i ) E ( X ) ( ii ) E ( X + Y ) ( iii ) V ( X Y ) ( iv ) E ( X 2 + Y 2 ) . Solution ( i ) E ( X ) = 2 Z 0 x e x 2 / 2 2 dx = r 2 where an elementary property of the normal integral ha been used . ( ii ) E ( X + Y ) = E ( X ) + E ( Y ) = 0 ( iii ) Since E ( X Y ) = 0 then V ( X Y ) = Z ( x y ) 2 e ( x 2 + y 2 ) / 2 2 dxdy = 1 + 1 = 2 ( iv ) E ( X 2 + Y 2 ) = Z Z ( x 2 + y 2 ) 1 / 2 e ( x 2 + y 2 ) / 2 2 dxdy = Z 0 Z 2 0 r 2 e r 2 / 2 2 drd = r 2 . 2 . Suppose that X and Y have the joint probability density function f XY = ( 2 m + 6 ) x m y 0 < y < x < 1 0 otherwise . Show that this is a probability density function and nd E ( Y X ) and V ( Y X ) . Solution The marginal denstiy of X is f X ( x ) = Z x 0 ( 2 m + 6 ) x m ydy = ( m + 3 ) x m +2 , 0 x 1 and notice that Z 1 0 ( m + 3 ) x m +2 dx = 1 and hence is a pdf . The conditional pdf is f Y X ( y x ) = 2 ( m + 3 ) x m y ( m + 3 ) x m +2 = 2 y x 2 , 0 y x and E ( Y X ) = Z x 0 y 2 y x 2 dy = 2 3 x , E ( Y 2 X ) = Z x 0 y 2 2 y x 2 dy = 1 2 x 2 and V ( Y X ) = E ( Y 2 X ) E 2 ( Y X ) = x 2 / 18 . 3 . Suppose X and Y have the joint density function e y for 0 < x < y < . Find the least mean square line of Y on X and E ( Y X ) . Solution f X ( x ) = Z x e y dy = e x , 0 x < A. K. Khandani , ECE307Fall 2024 132 and f Y X ( y x ) = e ( y x ) , x < y < and E ( Y X ) = Z x ye ( y x ) dy = x + 1 Any time the conditional expectation is linear it is also the least square line . However , for completeness , we compute the least square line a well . E ( X ) = Z 0 xe x dx = 1 E ( X 2 ) = 2 V ( X ) = 1 E ( Y ) = Z 0 y ye y dy = 2 E ( Y 2 ) = 6 V ( Y ) = 2 and E ( XY ) = Z 0 Z y 0 xdx ye y dy = 3 and thus XY = 3 1 2 1 2 = 1 2 and thus the least mean square line is x + 1 , a before . 4 . If X is a binomial random variable with parameter n and p and Y is a binomial random variable with parameter m and p , X and Y independent , and Z = X + Y , nd E ( X Z ) . Solution P ( X = j Z = X + Y = l ) = P ( X = j and Y = l j ) P ( X + Y = l ) = P ( X = j ) P ( Y = l j ) P ( X + Y = l ) = n j m l j n + m l which is a hypergeometric distribution . The mean of this distribution is in the text ( page 74 ) and is E ( X Z ) = nl/ ( n + m ) . 5 . Let ( X , Y ) be a bivariate random variable such that E ( X ) = 0 , E ( Y ) = 3 , V ( X ) = 3 , V ( Y ) = 5 and XY = 1 / 2 . Find the mean and variance of the random variable Z = 2 X 3 Y . Solution In general it is easy to show ( do it ) that V ( aX + bY ) = a 2 V ( X ) + b 2 V ( Y ) + 2 ab XY X Y . Using this formula it follows that Z = 2 X 3 Y = 9 and V ( Z ) = 57 6 15 . 6 . It is known that the vertical and horizontal error of a computer plotter , in standardized unit , form a bivariate normal random variable with joint pdf f XY ( x , y ) = 3 4 8 exp 9 16 ( x 2 xy 3 4 x 3 + y 2 4 2 y 3 + 4 3 ) A. K. Khandani , ECE307Fall 2024 133 ( i ) Find E ( Y X ) . ( ii ) For what value of the constant a will the random variable X and Z = X + aY be independent ( iii ) Find the probability that X Y < 1 . ( Note Sums of normal random variable are normal . ) Solution By comparing the form of the given bivariate normal density with that of the standard one , we can set up equation to be solved for the ve parameter . The solution are X = 2 / 3 , Y = 4 / 3 , X = 1 , Y = 2 , XY = 1 / 3 and these may be veried by substituting into the general form . ( i ) E ( Y X ) = 2 3 X + 8 9 . ( ii ) The value of a such that E ( XZ ) = E ( X ) E ( Z ) is sought . By direct computation E ( XZ ) = E ( X ) E ( Z ) = E ( X ( X + aY ) ) = E ( X ) ( E ( X ) + aE ( Y ) ) or V ( X ) + a XY X Y = 0 and from the parameter found , this implies that a = 3 / 2 . ( iii ) From the parameter already found , let Z = X Y and note that Z = 2 / 3 and V ( Z ) = V ( X Y ) = 11 / 3 Thus P ( X Y < 1 ) = P ( Z < 1 ) = ( 5 33 ) ( 1 33 ) . 7 . If ( X , Y ) is a bivariate normal random variable with pdf f XY ( x , y ) = 1 3 exp 2 3 ( x 2 xy + y 2 ) ( i ) Find the probability that P ( Y > 3 X = 2 ) and compare this to P ( Y > 3 ) . ( ii ) Find E ( Y X ) . Solution ( i ) As with the previous problem , the ve parameter of the bivariate density are found a X = Y = 0 X = Y = 1 XY = 1 / 2 . Consequently the conditional density function f Y X ( y x ) N ( 1 2 x , 3 4 . Then P ( Y > 3 X = 2 ) = Z 3 e ( y 1 ) 2 / ( 3 / 2 ) 2 p 3 / 4 dy = 1 ( 4 3 ) . Since Y N ( 0 , 1 ) , P ( Y > 3 ) = Z 3 e y 2 / 2 2 dy = 1 ( 3 ) . ( ii ) E ( Y X ) = X/ 2 . A. K. Khandani , ECE307Fall 2024 134 8 . The time in minute taken by person A to complete a certain task is assumed to be a random variable with an exponential pdf with parameter 1 . The time taken by person B is independent of that for person A and ha an exponential pdf with parameter 2 i.e . f X ( x ) = 1 e 1 x , x 0 , f Y ( y ) = 2 e 2 y , y 0 ( i ) What is the probability it take A longer to complete the task than B ( ii ) If 1 = 2 what is the probability the task are nished within two minute of each other Solution ( i ) Let X be the time taken by person A and Y that taken by person B. P ( X > Y ) = Z 0 Z x 0 2 e 2 y dy 1 e 1 x dx which can be computed to be P ( X > Y ) = 2 1 + 2 . ( ii ) It is desired to compute P ( X Y < 2 ) . Let 1 = 2 = . Consider P ( Y < X 2 ) = Z 2 Z x 2 0 e dy e x dx which is readily evaluated to e 2 / 2 . The required probability is then P ( X Y < 2 ) = 1 2 ( e 2 / 2 ) = 1 e 2 ."
}